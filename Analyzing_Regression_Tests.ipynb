{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Python 2.x / 3.x compatibility\n",
    "from __future__ import division, print_function\n",
    "\n",
    "#Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "#import csv\n",
    "import glob as gb\n",
    "\n",
    "#import pathlib\n",
    "\n",
    "import datetime\n",
    "import sqlite3\n",
    "\n",
    "from df2gspread import df2gspread as d2g\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (16, 9)\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regression_analysis\n",
    "# from imp import reload\n",
    "# reload(regression_analysis)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This is a RawNBConvert cell, it isn't run unless you switch it to \"Code\"\n",
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse compatibility Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compat_matrix = regression_analysis.parse_compatibility_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force a new version (don't want to be prompted each time I parse df_files)\n",
    "new_version = compat_matrix.iloc[0].copy()\n",
    "new_version['OpenStudio'] = \"2.4.2\"\n",
    "compat_matrix = compat_matrix.append(new_version).sort_values('OpenStudio', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compat_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compat_matrix['Has_Docker'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compat_matrix[compat_matrix['Has_Docker']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Number of OpenStudio versions within each E+ version\n",
    "compat_matrix.groupby('E+')['OpenStudio'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "compat_matrix.groupby('E+')['OpenStudio'].count().plot(kind='barh', ax=ax)\n",
    "ax.set_xlim(0, compat_matrix.groupby('E+')['OpenStudio'].count().max())\n",
    "ax.set_title('Number of OpenStudio version for each E+ version')\n",
    "ax.set_xlabel('Number of OpenStudio Versions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "# compat_matrix.to_csv('compat_matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix permissions and skin down the fuelcell OSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skinning it down is done in the model_tests.rb now\n",
    "help(regression_analysis.cleanup_bloated_osws)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!stat --format '%a' test/fuelcell.osm_2.2.0_out.osw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The permissions stuff is done in the launch docker shell scripts\n",
    "\n",
    "If you want to do it manually\n",
    "\n",
    "Need to do:\n",
    "    \n",
    "    sudo chown -R $USER * \n",
    "    sudo find . -type f -exec chmod 664 {} \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse out.osw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "reload(regression_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without custom tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = regression_analysis.find_info_osws(compat_matrix=compat_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With custom Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_files = regression_analysis.find_info_osws_with_tags(compat_matrix=compat_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the test status: Fail/Success/Blank"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from imp import reload\n",
    "reload(regression_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_colors(val):\n",
    "    s = 'background-color: {}'\n",
    "    if val == 'Fail':\n",
    "        return s.format('#F4C7C3')\n",
    "    elif val == '':\n",
    "        return s.format('#f2e2c1')\n",
    "    return ''\n",
    "\n",
    "def hover(hover_color=\"#ffff99\"):\n",
    "    return dict(selector=\"tr:hover\",\n",
    "                props=[(\"background-color\", \"%s\" % hover_color)])\n",
    "\n",
    "styles = [\n",
    "    hover(),\n",
    "    dict(selector=\"td\", props=[#(\"font-size\", \"150%\"),\n",
    "                               (\"text-align\", \"center\")]),\n",
    "    dict(selector=\"caption\", props=[(\"caption-side\", \"bottom\")])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataframe\n",
    "success = regression_analysis.success_sheet(df_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success[success[('8.8.0', '2.4.2')] == 'Fail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(success.style.applymap(background_colors).set_table_styles(styles)\n",
    "          .set_caption(\"Test Success\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for a few tests only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only those were some are missing or failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filt = success[(success == '').any(axis=1) |\n",
    "               (success == 'Fail').any(axis=1)].index.get_level_values(0).unique().tolist()\n",
    "\n",
    "(success.loc[filt].style\n",
    "          .applymap(background_colors)\n",
    "          .set_table_styles(styles)\n",
    "          .set_caption(\"Test Success\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Filter on a single containing string\n",
    "filt = success.index.get_level_values(0).str.contains('exterior_equi')\n",
    "\n",
    "# Filter on a pattern\n",
    "#filt = success.index.get_level_values(0).str.match(r'(exterior_equipment)|(meters)|(plant_op_schemes)|(avms_temp)')\n",
    "\n",
    "(success.loc[filt].style.applymap(background_colors).set_table_styles(styles)\n",
    "          .set_caption(\"Test Success\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torun = success[success[('8.8.0', '2.2.2')] != 'Success']\n",
    "s = \"openstudio model_tests.rb -n '/\"\n",
    "tests = []\n",
    "for i, (test, ext) in enumerate(torun.index.tolist()):\n",
    "    test_name = \"test_{}_{}\".format(test, 'osm')\n",
    "    #test_name = \"test_{}\".format(test)\n",
    "    #s += \" --name test_{}_{}\".format(test, ext)\n",
    "    if i < len(torun)-1:\n",
    "        s+='({})|'.format(test_name)\n",
    "    else:\n",
    "        s+='({})'.format(test_name)\n",
    "    tests.append(test_name)\n",
    "\n",
    "s += \"/'\"\n",
    "s"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torun = success[(success[('8.6.0', '2.0.4')] == 'Success') &\n",
    "                (success[('8.7.0', '2.2.1')] == '')]\n",
    "\n",
    "\n",
    "s = \"openstudio model_tests.rb -n '/\"\n",
    "tests = []\n",
    "for i, (test, ext) in enumerate(torun.index.tolist()):\n",
    "    test_name = \"test_{}_{}\".format(test, 'osm')\n",
    "    #test_name = \"test_{}\".format(test)\n",
    "    #s += \" --name test_{}_{}\".format(test, ext)\n",
    "    if i < len(torun)-1:\n",
    "        s+='({})|'.format(test_name)\n",
    "    else:\n",
    "        s+='({})'.format(test_name)\n",
    "    tests.append(test_name)\n",
    "\n",
    "s += \"/'\"\n",
    "s"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "for f in gb.glob('*_2.1.1.osm'):\n",
    "    outpath = '../model/simulationtests/'\n",
    "    dst_path = os.path.join(outpath, f.replace('.rb_2.1.1', '') )\n",
    "    print(dst_path)\n",
    "\n",
    "    copyfile(f, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_filt = success.loc[success[(success == '').sum(axis=1) >= 1].index.get_level_values(0).tolist()]\n",
    "filt = success_filt[success_filt[('8.8.0', '2.4.1')] == 'Fail'].index.get_level_values(0).tolist()\n",
    "\n",
    "(success.loc[filt].style.applymap(background_colors).set_table_styles(styles)\n",
    "        .set_caption(\"Test Success\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet = '/EffiBEM&NREL-Regression-Test_Status'\n",
    "wks_name = 'Test_Status'\n",
    "d2g.upload(success.T.reset_index().T.reset_index(),\n",
    "           gfile=spreadsheet, wks_name=wks_name,\n",
    "           row_names=False, col_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Missing tests: ruby versus osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_impl = regression_analysis.test_implemented_sheet(df_files=df_files, success=success,\n",
    "                                   only_for_mising_osm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_impl[~test_impl['osm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet = '/EffiBEM&NREL-Regression-Test_Status'\n",
    "wks_name = 'Tests_Implemented'\n",
    "d2g.upload(test_impl,\n",
    "           gfile=spreadsheet, wks_name=wks_name,\n",
    "           row_names=True, col_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ouput the total_site_energy (kBTU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_kbtu = df_files.applymap(regression_analysis.parse_total_site_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet = '/EffiBEM&NREL-Regression-Test_Status'\n",
    "wks_name = 'SiteKBTU'\n",
    "d2g.upload(site_kbtu.T.reset_index().T.reset_index().fillna(''),\n",
    "           gfile=spreadsheet, wks_name=wks_name,\n",
    "           # Skip first row\n",
    "           start_cell='A1',\n",
    "           row_names=False, col_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the rolling percent difference of total kBTU from one version to the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_kbtu_change = site_kbtu.pct_change(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_kbtu.loc[site_kbtu_change.index.get_level_values(0).str.contains('flat_plate')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site_kbtu_change.loc['pv_and_storage_facilityexcess']\n",
    "site_kbtu_change.loc[site_kbtu_change.index.get_level_values(0).str.contains('flat_plate')]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(site_kbtu_change[(site_kbtu_change.abs() > 0.005).any(axis=1)].abs().style\n",
    " .background_gradient(cmap=sns.light_palette(\"red\", as_cmap=True))\n",
    " .format(\"{:.2%}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap > 1% change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_analysis.heatmap_sitekbtu_pct_change(site_kbtu=site_kbtu,\n",
    "                            row_threshold=0.01, display_threshold=0.001, \n",
    "                            savefig=False, show_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap > 0.5% change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_analysis.heatmap_sitekbtu_pct_change(site_kbtu=site_kbtu,\n",
    "                            row_threshold=0.005, display_threshold=0.001, \n",
    "                            savefig=False, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet = '/EffiBEM&NREL-Regression-Test_Status'\n",
    "wks_name = 'SiteKBTU_Percent_Change'\n",
    "d2g.upload(site_kbtu.pct_change(axis=1).T.reset_index().T.reset_index().fillna(''),\n",
    "           gfile=spreadsheet, wks_name=wks_name,\n",
    "           row_names=False, col_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 40px; color:red;\">ANYTHING PAST THIS POINT NEEDS CLEANING</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference in end use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_5pct = (site_kbtu.pct_change(axis=1).abs() > 0.005).sum(axis=0).to_frame()\n",
    "over_5pct.columns = ['Count (ABS(pct_diff) > 0.5%)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_kbtu.pct_change(axis=1).abs().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_5pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_5pct = (site_kbtu.pct_change(axis=1).abs() > 0.005).sum(axis=1).to_frame()\n",
    "over_5pct.columns = ['Count (ABS(pct_diff) > 0.005)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_5pct.replace(0, np.nan).dropna().sort_values('Count (ABS(pct_diff) > 0.005)', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_1 = '2.1.0'\n",
    "version_2 = '2.2.1'\n",
    "\n",
    "\n",
    "all_diffs = {}\n",
    "failed = {}\n",
    "for index, row in  df_files.T.reset_index(level=0, drop=True).T.iterrows():\n",
    "    diff_ok = True\n",
    "    try:\n",
    "        cleaned_end_use_2 = regression_analysis.parse_end_use(row[version_2])\n",
    "        ok2 = True\n",
    "    except:\n",
    "        cleaned_end_use_2 = 'Failed'\n",
    "        diff_ok = False\n",
    "        ok2 = False\n",
    "    try:\n",
    "        cleaned_end_use_1 = regression_analysis.parse_end_use(row[version_1])\n",
    "        ok1 = True\n",
    "    except:\n",
    "        cleaned_end_use_1 = 'Failed'\n",
    "        diff_ok = False\n",
    "        ok1 = False\n",
    "    if diff_ok:\n",
    "        pct_diff = (cleaned_end_use_2 - cleaned_end_use_1) / cleaned_end_use_1\n",
    "        \n",
    "        all_diffs[index] = {version_1: cleaned_end_use_1,\n",
    "                            version_2: cleaned_end_use_2,\n",
    "                            'diff': pct_diff}\n",
    "    else:\n",
    "        failed[index] = {version_1: ok1,\n",
    "                         version_2: ok2}\n",
    "        \n",
    "df_failed = pd.DataFrame(failed).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the ones that changed: False means it fails, True means it worked\n",
    "df_failed[df_failed[version_1] != df_failed[version_2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_diffs = {}\n",
    "for test, d in all_diffs.items():\n",
    "    #dmax = \n",
    "    max_diffs[test] = {'Max': d['diff'].max().max(),\n",
    "                       'Min': d['diff'].min().min(),\n",
    "                       'Total Diff': (d[version_2][('Total', 'kBtu')].sum()\n",
    "                                      - d[version_1][('Total', 'kBtu')].sum()) / d[version_1][('Total', 'kBtu')].sum()}\n",
    "    \n",
    "    \n",
    "df_diffs = pd.DataFrame(max_diffs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diffs[~(df_diffs == 0).all(axis=1)].style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ('coolingtowers', 'rb')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "\n",
    "fmt = lambda x,pos: '{:.0%}'.format(x)\n",
    "\n",
    "sns.heatmap(all_diffs[test]['diff'].dropna(how='all', axis=0).dropna(how='all', axis=1).abs(),\n",
    "            ax=ax, cmap='YlOrRd',\n",
    "            vmin=0, vmax=1,\n",
    "            cbar_kws={'format': mpl.ticker.FuncFormatter(fmt)},\n",
    "            annot=all_diffs[test]['diff'].dropna(how='all', axis=0).dropna(how='all', axis=1), fmt='.1%')\n",
    "ax.set_title(\"Percent difference in End Use By Fuel for test '{}' between {} and {}\".format(test, version_2, version_1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find missing tests: Map tests to Cpp classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grep in ruby and osm tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/julien/Software/Others/OpenStudio-resources/model/simulationtests/')\n",
    "\n",
    "# Grep in ruby test for Model:: statements\n",
    "grep = !/bin/grep \"Model::\" *.rb\n",
    "objs = pd.DataFrame([x.split(':', maxsplit=1 ) for x in grep], columns=['file', 'grepped_line'])\n",
    "\n",
    "# Grep in ruby test for Model:: statements\n",
    "grep_lib = !/bin/grep \"Model::\" ./lib/*.rb\n",
    "objs_lib = pd.DataFrame(grep_lib, columns=['grepped_line'])\n",
    "objs_lib['file'] = 'lib/baseline_model.rb'\n",
    "\n",
    "# Find all Model namespace Classes by getting name from the cpp files\n",
    "os_classes = !ls /home/julien/Software/Others/OpenStudio/openstudiocore/src/model/*.cpp\n",
    "os_classes = [os.path.split(os.path.splitext(p)[0])[1] for p in os_classes]\n",
    "\n",
    "os.chdir('/home/julien/Software/Others/OpenStudio-resources')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_object_pat = re.compile(r'OpenStudio::Model::(.*?)\\.new')\n",
    "def parse_model_object(s):\n",
    "    m = model_object_pat.search(s)\n",
    "    if m:\n",
    "        return m.groups()[0]\n",
    "    else:\n",
    "        print('Cannot match {}'.format(s))\n",
    "        return None\n",
    "    \n",
    "objs['ModelObject'] = objs['grepped_line'].apply(parse_model_object)\n",
    "objs_lib['ModelObject'] = objs_lib['grepped_line'].apply(parse_model_object)\n",
    "\n",
    "# Concat both\n",
    "objs = pd.concat([objs, objs_lib])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(objs['ModelObject']) - set(os_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(os_classes) - set(objs['ModelObject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_os_classes = pd.DataFrame(index=os_classes)\n",
    "df_os_classes['In Ruby Test'] = False\n",
    "df_os_classes = df_os_classes.join(objs.groupby('ModelObject')['file'].apply(list))\n",
    "df_os_classes.loc[df_os_classes['file'].notnull(),\n",
    "                  'file'] = df_os_classes.loc[df_os_classes['file'].notnull(),\n",
    "                                              'file'].apply(np.unique)\n",
    "df_os_classes.loc[df_os_classes['file'].notnull(), 'In Ruby Test'] = True\n",
    "df_os_classes = df_os_classes.rename(columns={'file': 'files'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_os_classes['In Ruby Test'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_os_classes.to_csv('Mapping_ruby_test_to_cpp_classes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get comments dict from the google sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from df2gspread import gspread2df as g2d\n",
    "\n",
    "spreadsheet = '/EffiBEM&NREL-Regression-Test_Status'\n",
    "wks_name = 'Mapping_ruby_test_to_cpp_classes'\n",
    "\n",
    "df = g2d.download(spreadsheet, wks_name, col_names = True, row_names = True)\n",
    "#comments_dict = df['IsNormal'].to_dict()\n",
    "comments_dict = df.loc[df['IsNormal'] != '', 'IsNormal'].to_dict()\n",
    "comments_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(comments_dict)\n",
    "s = s[s.str.lower().str.contains('added')].str.split(':', expand=True)[1].str.strip().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tot_obj = 0\n",
    "n_tot_tests = 0\n",
    "for index, val in s.reset_index().groupby(1)['index'].apply(list).items():\n",
    "    if index == 'pv_and_storage_facilityexcess.rb':\n",
    "        test = 'pv_and_storage_facilityexcess.rb and pv_and_storage_demandleveling.rb'\n",
    "        n_tot_tests += 1\n",
    "    else:\n",
    "        test = index\n",
    "    n_tot_tests += 1\n",
    "    n_tot_obj += len(val)\n",
    "    print(\"**{}** ({})\".format(test, len(val)))\n",
    "    print()\n",
    "    for x in val:\n",
    "        print(\"* {}\".format(x))\n",
    "    print(\"\\n\")\n",
    "print(\"\\n**Total Added: {} objects in {} tests**\".format(n_tot_obj, n_tot_tests))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "comments_dict = {'AccessPolicyStore': 'TRUE',\n",
    " 'AirLoopHVACReturnPlenum': 'TRUE',\n",
    " 'AirLoopHVACSupplyPlenum': 'TRUE',\n",
    " 'AirLoopHVACZoneMixer': 'TRUE',\n",
    " 'AirLoopHVACZoneSplitter': 'TRUE',\n",
    " 'AirToAirComponent': 'TRUE',\n",
    " 'AvailabilityManager': 'Base Class',\n",
    " 'AvailabilityManagerDifferentialThermostat': 'Just Added: plantloop_avms_temp.rb',\n",
    " 'AvailabilityManagerHighTemperatureTurnOff': 'Just Added: plantloop_avms_temp.rb',\n",
    " 'AvailabilityManagerHighTemperatureTurnOn': 'Just Added: plantloop_avms_temp.rb',\n",
    " 'AvailabilityManagerLowTemperatureTurnOff': 'Just Added: plantloop_avms_temp.rb',\n",
    " 'AvailabilityManagerLowTemperatureTurnOn': 'Just Added: plantloop_avms_temp.rb',\n",
    " 'BoilerSteam': \"I don't think this works because that's the only steam object...\",\n",
    " 'Building': 'TRUE',\n",
    " 'BuildingStory': 'TRUE',\n",
    " 'CentralHeatPumpSystem': 'Just Added: centralheatpumpsystem.rb',\n",
    " 'CentralHeatPumpSystemModule': 'Just Added: centralheatpumpsystem.rb',\n",
    " 'Component': 'TRUE',\n",
    " 'ComponentData': 'TRUE',\n",
    " 'ComponentWatcher': 'TRUE',\n",
    " 'Connection': 'TRUE',\n",
    " 'ConnectorMixer': 'TRUE',\n",
    " 'ConstructionBase': 'TRUE',\n",
    " 'Curve': 'TRUE',\n",
    " 'CurveBicubic': 'TRUE',\n",
    " 'CurveDoubleExponentialDecay': 'TRUE',\n",
    " 'CurveExponentialDecay': 'TRUE',\n",
    " 'CurveExponentialSkewNormal': 'TRUE',\n",
    " 'CurveFanPressureRise': 'TRUE',\n",
    " 'CurveFunctionalPressureDrop': 'TRUE',\n",
    " 'CurveLinear': 'TRUE',\n",
    " 'CurveQuadraticLinear': 'TRUE',\n",
    " 'CurveQuartic': 'TRUE',\n",
    " 'CurveRectangularHyperbola1': 'TRUE',\n",
    " 'CurveRectangularHyperbola2': 'TRUE',\n",
    " 'CurveSigmoid': 'TRUE',\n",
    " 'CurveTriquadratic': 'TRUE',\n",
    " 'DefaultConstructionSet': 'TRUE',\n",
    " 'DefaultScheduleSet': 'TRUE',\n",
    " 'DefaultSubSurfaceConstructions': 'TRUE',\n",
    " 'DefaultSurfaceConstructions': 'TRUE',\n",
    " 'DesignDay': 'TRUE',\n",
    " 'DesignSpecificationZoneAirDistribution': 'Not in ruby API',\n",
    " 'ElectricLoadCenterInverterLookUpTable': 'Just Added: pv_and_storage_facilityexcess.rb',\n",
    " 'ElectricLoadCenterStorageConverter': 'Just Added: pv_and_storage_facilityexcess.rb',\n",
    " 'ElectricLoadCenterStorageSimple': 'Just Added: pv_and_storage_facilityexcess.rb',\n",
    " 'ElectricalStorage': 'TRUE',\n",
    " 'ExteriorFuelEquipment': 'Just Added: exterior_equipment.rb',\n",
    " 'ExteriorFuelEquipmentDefinition': 'Just Added: exterior_equipment.rb',\n",
    " 'ExteriorLoadDefinition': 'Base Class',\n",
    " 'ExteriorLoadInstance': 'Base Class',\n",
    " 'ExteriorWaterEquipment': 'Just Added: exterior_equipment.rb',\n",
    " 'ExteriorWaterEquipmentDefinition': 'Just Added: exterior_equipment.rb',\n",
    " 'Facility': 'TRUE',\n",
    " 'FileOperations': 'TRUE',\n",
    " 'FloorplanJSForwardTranslator': 'TRUE',\n",
    " 'Gas': 'TRUE',\n",
    " 'GasEquipment': 'Just Added: space_load_instances.rb',\n",
    " 'GasEquipmentDefinition': 'Just Added: space_load_instances.rb',\n",
    " 'Generator': 'TRUE',\n",
    " 'GeneratorMicroTurbine': 'Just Added: generator_microturbine.rb',\n",
    " 'GeneratorMicroTurbineHeatRecovery': 'Just Added: generator_microturbine.rb',\n",
    " 'GeneratorPhotovoltaic': 'Tested for in photovoltaics.rb',\n",
    " 'GenericModelObject': 'TRUE',\n",
    " 'Glazing': 'TRUE',\n",
    " 'HVACComponent': 'TRUE',\n",
    " 'HVACTemplates': 'This is used in most files through the lib/baseline_model.rb',\n",
    " 'HotWaterEquipment': 'Just Added: space_load_instances.rb',\n",
    " 'HotWaterEquipmentDefinition': 'Just Added: space_load_instances.rb',\n",
    " 'Inverter': 'TRUE',\n",
    " 'LayeredConstruction': 'TRUE',\n",
    " 'LifeCycleCost': 'It is tested for LifeCycleParameters.rb',\n",
    " 'LifeCycleCostParameters': 'It is tested for LifeCycleParameters.rb',\n",
    " 'LifeCycleCostUsePriceEscalation': 'It is tested for LifeCycleParameters.rb',\n",
    " 'Loop': 'TRUE',\n",
    " 'Luminaire': 'Not sure how to use it',\n",
    " 'LuminaireDefinition': 'Not sure how to use it',\n",
    " 'Material': 'TRUE',\n",
    " 'MeterCustom': 'Just Added: meters.rb',\n",
    " 'MeterCustomDecrement': 'Just Added: meters.rb',\n",
    " 'Mixer': 'TRUE',\n",
    " 'ModelExtensibleGroup': 'TRUE',\n",
    " 'ModelMerger': 'TRUE',\n",
    " 'ModelObject': 'TRUE',\n",
    " 'ModelObjectList': 'TRUE',\n",
    " 'Node': 'TRUE',\n",
    " 'OpaqueMaterial': 'True, base class',\n",
    " 'OtherEquipment': 'Just Added: space_load_instances.rb',\n",
    " 'OtherEquipmentDefinition': 'Just Added: space_load_instances.rb',\n",
    " 'OutputMeter': 'Just Added: meters.rb',\n",
    " 'ParentObject': 'TRUE',\n",
    " 'PhotovoltaicPerformance': 'Base class',\n",
    " 'PhotovoltaicPerformanceSimple': 'Just Added: pv_and_storage_facilityexcess.rb',\n",
    " 'PlanarSurface': 'TRUE',\n",
    " 'PlanarSurfaceGroup': 'TRUE',\n",
    " 'PlantEquipmentOperationOutdoorDewpoint': 'Just Added: plant_op_schemes_temp.rb',\n",
    " 'PlantEquipmentOperationOutdoorDewpointDifference': 'Just Added: plant_op_schemes_deltatemp.rb',\n",
    " 'PlantEquipmentOperationOutdoorDryBulb': 'Just Added: plant_op_schemes_temp.rb',\n",
    " 'PlantEquipmentOperationOutdoorDryBulbDifference': 'Just Added: plant_op_schemes_deltatemp.rb',\n",
    " 'PlantEquipmentOperationOutdoorRelativeHumidity': 'Just Added: plant_op_schemes_temp.rb',\n",
    " 'PlantEquipmentOperationOutdoorWetBulbDifference': 'Just Added: plant_op_schemes_deltatemp.rb',\n",
    " 'PlantEquipmentOperationRangeBasedScheme': 'Base Class',\n",
    " 'PlantEquipmentOperationScheme': 'Base Class',\n",
    " 'PortList': 'TRUE',\n",
    " 'Relationship': 'TRUE',\n",
    " 'RenderingColor': 'TRUE',\n",
    " 'ResourceObject': 'TRUE',\n",
    " 'RoofVegetation': 'Just Added: roof_vegetation.rb',\n",
    " 'Schedule': 'TRUE',\n",
    " 'ScheduleBase': 'Base Class',\n",
    " 'ScheduleTypeRegistry': 'TRUE',\n",
    " 'ScheduleWeek': 'TRUE',\n",
    " 'ScheduleYear': 'TRUE',\n",
    " 'SetpointManager': 'TRUE',\n",
    " 'Shade': 'TRUE',\n",
    " 'Site': 'TRUE',\n",
    " 'SizingPeriod': 'TRUE',\n",
    " 'SizingPlant': 'TRUE',\n",
    " 'SizingSystem': 'TRUE',\n",
    " 'SizingZone': 'TRUE',\n",
    " 'SkyTemperature': \"Forward translator does nothing, and it doesn't have setters nor getters: https://github.com/jmarrec/OpenStudio/blob/develop/openstudiocore/src/energyplus/ForwardTranslator/ForwardTranslateSkyTemperature.cpp#L42\",\n",
    " 'Space': 'TRUE',\n",
    " 'SpaceItem': 'TRUE',\n",
    " 'SpaceLoad': 'TRUE',\n",
    " 'SpaceLoadDefinition': 'TRUE',\n",
    " 'SpaceLoadInstance': 'TRUE',\n",
    " 'SpaceType': 'TRUE',\n",
    " 'Splitter': 'TRUE',\n",
    " 'SteamEquipment': 'Just Added: space_load_instances.rb',\n",
    " 'SteamEquipmentDefinition': 'Just Added: space_load_instances.rb',\n",
    " 'StraightComponent': 'Base Class',\n",
    " 'SubSurface': 'TRUE',\n",
    " 'Surface': 'TRUE',\n",
    " 'Thermostat': 'TRUE',\n",
    " 'ThreeJSForwardTranslator': 'TRUE',\n",
    " 'ThreeJSReverseTranslator': 'TRUE',\n",
    " 'UtilityCost_Charge_Block': 'Not Functional in API',\n",
    " 'UtilityCost_Charge_Simple': 'Not Functional in API',\n",
    " 'UtilityCost_Computation': 'Not Functional in API',\n",
    " 'UtilityCost_Qualify': 'Not Functional in API',\n",
    " 'UtilityCost_Ratchet': 'Not Functional in API',\n",
    " 'UtilityCost_Tariff': 'Not Functional in API',\n",
    " 'UtilityCost_Variable': 'Not Functional in API',\n",
    " 'Version': 'TRUE',\n",
    " 'WaterToAirComponent': 'TRUE',\n",
    " 'WaterToWaterComponent': 'TRUE',\n",
    " 'YearDescription': 'TRUE',\n",
    " 'ZoneHVACComponent': 'TRUE',\n",
    " 'ZoneHVACEquipmentList': 'TRUE'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comments.set_index('Test')['IsNormal'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge comments\n",
    "comments = pd.Series(comments_dict, name='IsNormal')\n",
    "df_os_classes = df_os_classes.join(comments)\n",
    "df_os_classes = df_os_classes[['In Ruby Test', 'IsNormal', 'files']]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filt1 = ~df_os_classes['In Ruby Test']\n",
    "filt2 = df_os_classes['IsNormal'].isnull()\n",
    "df_os_classes[filt1 & filt2] # .apply(lambda x: print(x.name), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find objects in the osm tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile a regex\n",
    "os_class_pattern = re.compile(r'OS:(.*?),')\n",
    "\n",
    "# Initialize a column of empty lists\n",
    "df_os_classes['osms'] = np.empty((len(df_os_classes), 0)).tolist()\n",
    "\n",
    "# Loop on all osms, and find OS objects\n",
    "for osm_path in gb.glob('*.osm'):\n",
    "    with open(osm_path) as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        m = os_class_pattern.match(line)\n",
    "        if m:\n",
    "            classname = m.groups()[0].replace(':','')\n",
    "            if classname in df.index:\n",
    "                df_os_classes.loc[classname, 'osms'].append(osm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_os_classes.loc[df_os_classes['osms'].apply(len) == 0, 'osms'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt1 = ~df_os_classes['In Ruby Test']\n",
    "filt2 = df_os_classes['IsNormal'].isnull()\n",
    "filt3 = df_os_classes['osms'].isnull()\n",
    "df_os_classes[filt1 & filt2 & filt3] # .apply(lambda x: print(x.name), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_os_classes.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet = '/EffiBEM&NREL-Regression-Test_Status'\n",
    "wks_name = 'Mapping_ruby_test_to_cpp_classes'\n",
    "d2g.upload(df_os_classes.fillna(''),\n",
    "           gfile=spreadsheet, wks_name=wks_name,\n",
    "           row_names=True, col_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OSCLI = '/home/julien/Software/Others/OS-build/Products/openstudio'\n",
    "OSCLI= '/usr/bin/openstudio-2.4.1'\n",
    "RUN_N_TIMES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the same in.OSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/julien/Software/Others/OpenStudio-resources/testruns/evaporative_cooling.osm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "    \n",
    "r = {}\n",
    "o = {}\n",
    "e = {}\n",
    "for i in range(0, RUN_N_TIMES):\n",
    "    process = subprocess.Popen([OSCLI, 'run', '-w', 'in.osw'], shell=False,\n",
    "                           stdout=subprocess.PIPE, \n",
    "                           stderr=subprocess.PIPE)\n",
    "\n",
    "    # wait for the process to terminate\n",
    "    out, err = process.communicate()\n",
    "    o[i] = out\n",
    "    e[i] = err\n",
    "    errcode = process.returncode\n",
    "    r[i] = regression_analysis.parse_total_site_energy('out.osw')\n",
    "    print(\"{} - {:,.0f}\".format(i, r[i]))\n",
    "    \n",
    "# Say to user\n",
    "!echo \"THIS IS DONE\" | espeak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.Series(r)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/julien/Software/Others/OpenStudio-resources')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the same test (measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/julien/Software/Others/OpenStudio-resources')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = over_5pct.replace(0, np.nan).dropna()['Count (ABS(pct_diff) > 0.005)'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"openstudio model_tests.rb -n '/\"\n",
    "tests = []\n",
    "for i, (test, ext) in enumerate(diffs[diffs > 1].index.tolist()):\n",
    "    print(i)\n",
    "    test_name = \"test_{}_{}\".format(test, ext)\n",
    "    #s += \" --name test_{}_{}\".format(test, ext)\n",
    "    if i < len(diffs[diffs > 1])-1:\n",
    "        s+='({})|'.format(test_name)\n",
    "    else:\n",
    "        s+='({})'.format(test_name)\n",
    "    tests.append(test_name)\n",
    "#print(\"$os_build/Products/openstudio model_tests.rb {}\".format(s))\n",
    "#print(\"ruby model_tests.rb {}\".format(s))\n",
    "s += \"/'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(diffs[diffs > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {}\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from shutil import copyfile\n",
    "\n",
    "#test_exts = diffs[diffs > 1].index.tolist()\n",
    "test_exts =  [\n",
    "    #('surface_properties', 'rb'),\n",
    "    #('fan_on_off', 'rb'),\n",
    "    #('exterior_equipment', 'rb'),\n",
    "    ('generator_microturbine', 'rb'),\n",
    "]\n",
    "for (test, ext) in test_exts:\n",
    "    print(test)\n",
    " \n",
    "    #base_path = '/home/julien/Software/Others/OpenStudio-resources/testruns/availability_managers.rb/'\n",
    "    base_path = '/home/julien/Software/Others/OpenStudio-resources/testruns/{}.{}/'.format(test, ext)\n",
    "\n",
    "\n",
    "    r = {}\n",
    "    o = {}\n",
    "    e = {}\n",
    "    for i in range(0, RUN_N_TIMES):\n",
    "        process = subprocess.Popen([OSCLI, 'model_tests.rb', '-n',\n",
    "                                    '/{}_{}/'.format(test, ext)], \n",
    "                                   shell=False,\n",
    "                                   stdout=subprocess.PIPE,\n",
    "                                   stderr=subprocess.PIPE)\n",
    "\n",
    "        # wait for the process to terminate\n",
    "        out, err = process.communicate()\n",
    "        o[i] = out\n",
    "        e[i] = err\n",
    "        errcode = process.returncode\n",
    "        \n",
    "        if errcode != 0:\n",
    "            print(\"Problem with {}.{}, run {}\".format(test, ext, i))\n",
    "        else:\n",
    "            r[i] = regression_analysis.parse_total_site_energy(os.path.join(base_path, 'out.osw'))\n",
    "            print(\"{} - run {} - {:,.0f}\".format(test, i, r[i]))\n",
    "\n",
    "\n",
    "            # cp the osm somewhere else\n",
    "            #src_path = os.path.join(base_path, 'in.osm')\n",
    "            #dst_path = os.path.join(base_path, '../{t}.{e}_{i}.osm'.format(t=test, e=ext, i=i))\n",
    "            #copyfile(src_path, dst_path)  \n",
    "            \n",
    "            #src_path = os.path.join(base_path, 'run/in.idf')\n",
    "            #dst_path = os.path.join(base_path, '../{t}.{e}_{i}.idf'.format(t=test, e=ext, i=i))\n",
    "            #copyfile(src_path, dst_path)\n",
    "    \n",
    "    out_dict[\"{}.{}\".format(test, ext)] = {'r':r, 'o':o, 'e':e}\n",
    "\n",
    "!echo \"THIS IS DONE\" | espeak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "for k, v in out_dict.items():\n",
    "    df = pd.DataFrame(v)\n",
    "    df['test'] = k\n",
    "    df['run'] = df.index\n",
    "    df.set_index(['test', 'run'], inplace=True)\n",
    "    all_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat(all_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['r'].unstack(0).pct_change().max().sort_values(ascending=False)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['r'].unstack(0)['surface_properties.rb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['r'].unstack(0)['fan_on_off.rb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_properties.rb, fan_on_off.rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " set([\".\".join([x[0], x[1]]) for x in diffs[diffs > 1].index]) - set(df_all['r'].unstack(0).pct_change().columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.Series(r)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*(result - result.iloc[0])/result.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = subprocess.Popen([OSCLI, 'model_tests.rb', '-n', '/availability/'], shell=True,\n",
    "                            stdout=subprocess.PIPE, \n",
    "                            stderr=subprocess.PIPE)\n",
    "out, err = process.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare With Custom Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_only_runs = ['Ubuntu_run1', 'Ubuntu_run2', 'Windows_run1', 'Windows_run2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = regression_analysis.find_info_osws_with_tags(compat_matrix=compat_matrix)\n",
    "subset_files = df_files[[x for x in df_files.columns\n",
    "                         if x[1] == '2.4.1' \n",
    "                         and x[2] in keep_only_runs\n",
    "                        ]]\n",
    "subset_success = regression_analysis.success_sheet(subset_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_success[subset_success['n_fail+missing']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First filter only tests that have some variations in site kBTU\n",
    "\n",
    "I check for tests where the min accross runs isn't equal to the max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_kbtu = df_files.applymap(regression_analysis.parse_total_site_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to our version of interest, drop rows with all nan\n",
    "site_kbtu_241 = site_kbtu['8.8.0']['2.4.1'].dropna(how='all')\n",
    "\n",
    "# Keep only the custom tagged ones\n",
    "site_kbtu_241 = site_kbtu_241[[x for x in site_kbtu_241.columns if x in keep_only_runs]]\n",
    "\n",
    "# Filter on rows where the min is not the max\n",
    "site_kbtu_241 = site_kbtu_241[site_kbtu_241.apply(lambda row: min(row) != max(row), axis=1)]\n",
    "\n",
    "# Make a multiindex \n",
    "site_kbtu_241.columns = pd.MultiIndex.from_tuples([x.split('_') for x in site_kbtu_241.columns],\n",
    "                                                 names=['Platform', 'Run'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these tests where we have variations, we can visualize the deviation each run Platform/run using a boxplox:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "site_kbtu_241.boxplot(ax=ax, grid=False)\n",
    "ax.set_title('Boxplot of tests that have variations, by platform and run')\n",
    "ax.set_ylabel('Total site kBTU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_kbtu_241.plot(kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check biggest differences by looking at CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, I calculate the coefficient of variation ($CV$) for each test = standard deviation ($\\sigma$) divided by mean ($\\mu$)\n",
    "\n",
    "$$CV = \\frac{\\sigma}{\\mu}$$\n",
    "\n",
    "I then use a set tolerance to filter out tests that have a CV that isn't above or equal to the tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient of variation: standard deviation divided by mean\n",
    "cv = site_kbtu_241.std(axis=1) / site_kbtu_241.mean(axis=1)\n",
    "cv.name = 'Coefficient of Variation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tol = 0.00001\n",
    "print(\"Setting CV Tolerance to {:.3%}\".format(cv_tol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "sns.set_palette('Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = cv[cv >= cv_tol].sort_values(ascending=True).plot(kind='barh', figsize=(16,9))\n",
    "vals = ax.get_xticks()\n",
    "ax.set_xticklabels(['{:3.2f}%'.format(x*100) for x in vals])\n",
    "ax.set_title('Coefficient of Variation for tests that are above cv_tol={:.3%}'.format(cv_tol))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the same tests, we can visualize the total site kBTU for each:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "site_kbtu_241.reindex(index=cv[cv >= cv_tol].index).plot(kind='bar', ax=ax)\n",
    "ax.set_title('Total site kBTU for tests that are above the CV tolerance')\n",
    "ax.set_ylabel('Total Site kBTU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Could Ruby test just be unstable regardless of platform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) First,  **the big differences are in the ruby tests mostly** (except 2.). I've mentionned already that I fixed a bunch of instabilities in the ruby tests, but there are some I couldn't fix yet: **could the ruby tests in question just be unstable regardless of platform?**\n",
    "\n",
    "I plot the entire heatmap (all OS version) of these tests which have a CV >= cv_tol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toplot = site_kbtu.reindex(index=cv[cv >= cv_tol].index)\n",
    "toplot = toplot[[x for x in toplot.columns if x[2] in ([''] + keep_only_runs)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_analysis.heatmap_sitekbtu_pct_change(site_kbtu=toplot,\n",
    "                            row_threshold=0.0000, display_threshold=0.0001, \n",
    "                            savefig=False, show_plot=True, figsize=(16,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following tests are unstable regardless of platform:\n",
    "    \n",
    "* airloop_and_zonehvac.rb\n",
    "* evaporative_cooling.rb\n",
    "* surface_properties.rb \n",
    "* unitary_system_performance_multispeed.rb (edited)\n",
    "\n",
    "The big unknown is **what the heck happened in Windows Run 1 for unitary_vav_bypass**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One OSM test produces different results on different platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "2) One very important exception to (1) above is the `evaporative_cooling.osm` test: **seems to be stable on both platform, but it doesn't have the same numbers on Ubuntu versus windows! Further investigation is warranted.**\n",
    "\n",
    "Note: You might say it's hard to tell if the OSM is stable on a given platform with two runs. I ran it 10 times on Ubuntu, and it is stable.\n",
    "\n",
    "    count    1.000000e+01\n",
    "    mean     7.632714e+06\n",
    "    std      9.817002e-10\n",
    "    min      7.632714e+06\n",
    "    25%      7.632714e+06\n",
    "    50%      7.632714e+06\n",
    "    75%      7.632714e+06\n",
    "    max      7.632714e+06\n",
    "    dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run N more times on a given machine to have more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"'/\"\n",
    "\n",
    "torun = (cv[cv >= cv_tol].index.tolist())\n",
    "\n",
    "for i, (test, ext) in enumerate(torun):\n",
    "    test_name = \"test_{}_{}\".format(test, ext)\n",
    "    #test_name = \"test_{}\".format(test)\n",
    "    #s += \" --name test_{}_{}\".format(test, ext)\n",
    "    if i < len(torun)-1:\n",
    "        s+='({})|'.format(test_name)\n",
    "    else:\n",
    "        s+='({})'.format(test_name)\n",
    "\n",
    "s += \"/'\"\n",
    "print(\"ruby model_tests.rb -n {}\".format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload with more runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = regression_analysis.find_info_osws_with_tags(compat_matrix=compat_matrix)\n",
    "subset_files = df_files[[x for x in df_files.columns\n",
    "                         if x[1] == '2.4.1' \n",
    "                         #and x[2] != 'Ubuntu_run2'\n",
    "                        ]]\n",
    "# Keep only those that I run more than twice\n",
    "subset_files = subset_files.loc[subset_files[('8.8.0', '2.4.1', 'Ubuntu_run3')].notnull()]\n",
    "\n",
    "# Parse site_kbtu\n",
    "site_kbtu = subset_files.applymap(regression_analysis.parse_total_site_energy)\n",
    "\n",
    "# Restrict to our version of interest, drop rows with all nan\n",
    "site_kbtu_241 = site_kbtu['8.8.0']['2.4.1'].dropna(how='all')\n",
    "\n",
    "# Keep only the custom tagged ones\n",
    "site_kbtu_241 = site_kbtu_241[[x for x in site_kbtu_241.columns if x != '']]\n",
    "\n",
    "# Make a multiindex \n",
    "site_kbtu_241.columns = pd.MultiIndex.from_tuples([x.split('_') for x in site_kbtu_241.columns],\n",
    "                                                 names=['Platform', 'Run'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_kbtu_241.groupby(level='Platform', axis=1).mean().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_from_pct_diff(toplot, display_threshold=0.001, \n",
    "                          title=None):\n",
    "    # Prepare two custom cmaps with one single color\n",
    "    grey_cmap = mpl.colors.ListedColormap('#f7f7f7')\n",
    "    green_cmap = mpl.colors.ListedColormap('#f0f7d9')\n",
    "\n",
    "\n",
    "    w = 16\n",
    "    h = w * toplot.shape[0] / (3 * toplot.shape[1])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(w, h))\n",
    "\n",
    "    # Reserve 1.5 inches at bottom for explanation\n",
    "    #fig.subplots_adjust(bottom=1.5/h)\n",
    "\n",
    "    # Same as: fmt = lambda x,pos: '{:.1%}'.format(x)\n",
    "    def fmt(x, pos): return '{:.1%}'.format(x)\n",
    "\n",
    "\n",
    "    # Plot with colors, for those that are above the display_threshold\n",
    "    sns.heatmap(toplot.abs(), mask=toplot.abs() <= display_threshold,\n",
    "                ax=ax, cmap='YlOrRd',  # cmap='Reds', 'RdYlGn_r'\n",
    "                vmin=0, vmax=0.5,\n",
    "                cbar_kws={'format': mpl.ticker.FuncFormatter(fmt)},\n",
    "                annot=toplot, fmt='.2%', linewidths=.5)\n",
    "\n",
    "    # Plot a second heatmap on top, only for those that are below\n",
    "    sns.heatmap(toplot, mask=((toplot.abs() > display_threshold) |\n",
    "                              (toplot.abs() == 0)),\n",
    "                cbar=False,\n",
    "                annot=True, fmt=\".4%\", annot_kws={\"style\": \"italic\"},\n",
    "                ax=ax, cmap=grey_cmap)\n",
    "\n",
    "    # Plot a third heatmap on top, only for those that are zero,\n",
    "    # no annot just green\n",
    "    sns.heatmap(toplot, mask=(toplot.abs() != 0),\n",
    "                cbar=False,  # linewidths=.5, linecolor='#cecccc',\n",
    "                annot=False,\n",
    "                ax=ax, cmap=green_cmap)\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % from the mean siteKBTU of the test\n",
    "toplot = ((site_kbtu_241.T - site_kbtu_241.T.mean())/(site_kbtu_241.T.mean())).T\n",
    "\n",
    "heatmap_from_pct_diff(toplot, title='Percentage difference from mean of test (both_versions)',\n",
    "                     display_threshold=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % from the mean siteKBTU of the test\n",
    "mean_ubuntu = site_kbtu_241['Ubuntu'].mean(axis=1)\n",
    "toplot = ((site_kbtu_241.T - mean_ubuntu)/(mean_ubuntu)).T\n",
    "\n",
    "heatmap_from_pct_diff(toplot, title='Percentage difference from mean of test for Ubuntu platform',\n",
    "                     display_threshold=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = gb.glob('./test/plant_op_schemes_*')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at out.osw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls test/*generator*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = regression_analysis.load_osw('test/generator_microturbine.rb_2.0.4_out.osw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename out.osw"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "os.mkdir('test/Windows')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "for f in gb.glob('test/*_2.4.1_out.osw'):\n",
    "    #outpath = '../model/simulationtests/'\n",
    "    #dst_path = os.path.join(outpath, f.replace('.rb_2.1.1', '') )\n",
    "    dst_path = f.replace('2.4.1_out', '2.4.1_out_Windows_run1').replace('test/', 'test/Windows/')\n",
    "    print(dst_path)\n",
    "    os.rename(f, dst_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for f in gb.glob('test/*2.4.1_out_Windows_run1.osw'):\n",
    "    dst_path  = f.replace('test/', 'test/Windows/')\n",
    "    os.rename(f, dst_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for f in gb.glob('test/Windows/*'):\n",
    "    os.rename(f, f.replace('run1', 'run2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all tagged runs into a 'Tagged' directory for zipping\n",
    "os.mkdir('test/Tagged')\n",
    "for f in gb.glob('test/*.osw'):\n",
    "    if 'out_' in f:\n",
    "        print(f)\n",
    "        dst_path  = f.replace('test/', 'test/Tagged/')\n",
    "        copyfile(f, dst_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip in one go...\n",
    "import zipfile\n",
    "import glob as gb\n",
    "with zipfile.ZipFile(\"Tagged.zip\", \"w\") as z:\n",
    "    for f in gb.glob('test/*.osw'):\n",
    "        if 'out_' in f:\n",
    "            z.write(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify it worked\n",
    "z = zipfile.ZipFile(\"Tagged.zip\")\n",
    "z.printdir()\n",
    "z.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "f = 'test/ems.rb_2.2.0_out.osw'\n",
    "#f = 'test/setpoint_managers.rb_2.4.1_out_Ubuntu_run1.osw'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "917px",
    "left": "0px",
    "right": "1859.42px",
    "top": "111px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
