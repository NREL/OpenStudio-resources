{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 2.x / 3.x compatibility\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "#import csv\n",
    "import glob as gb\n",
    "\n",
    "#import pathlib\n",
    "\n",
    "import datetime\n",
    "import sqlite3\n",
    "\n",
    "from df2gspread import df2gspread as d2g\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (16, 9)\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python import regression_analysis\n",
    "# from imp import reload\n",
    "# reload(regression_analysis)\n",
    "\n",
    "from python.regression_analysis import background_colors, getStyles\n",
    "styles = getStyles()\n",
    "\n",
    "# def background_colors(val):\n",
    "#     fmt = ''\n",
    "#     s = 'background-color: {}'\n",
    "#     if val == 'Fail':\n",
    "#         fmt = s.format('#F4C7C3')\n",
    "#     elif val == 'N/A':\n",
    "#         fmt = s.format('#EDEDED') +\"; color: #ADADAD;\"\n",
    "#     elif val == '':\n",
    "#         fmt = s.format('#f2e2c1')\n",
    "#     return fmt\n",
    "\n",
    "# def hover(hover_color=\"#ffff99\"):\n",
    "#     return dict(selector=\"tr:hover\",\n",
    "#                 props=[(\"background-color\", \"%s\" % hover_color)])\n",
    "\n",
    "# styles = [\n",
    "#     hover(),\n",
    "#     dict(selector=\"td\", props=[#(\"font-size\", \"150%\"),\n",
    "#                                (\"text-align\", \"center\")]),\n",
    "#     dict(selector=\"caption\", props=[(\"caption-side\", \"bottom\")])\n",
    "# ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This is a RawNBConvert cell, it isn't run unless you switch it to \"Code\"\n",
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "ROOT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse compatibility Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compat_matrix = regression_analysis.parse_compatibility_matrix()\n",
    "compat_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forcing a new version\n",
    "\n",
    "The compatibility matrix is parsed from its [online location](https://github.com/NREL/OpenStudio/wiki/OpenStudio-Version-Compatibility-Matrix) and is used for looking up E+ versions corresponding to your OpenStudio version.\n",
    "\n",
    "If you are working on a custom develop local build, you don't want to be prompted each time you parse df_files, so you can directly add it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you want to get info from your local build**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_info = regression_analysis.test_os_cli('/home/julien/Software/Others/OS-build-release/Products/openstudio')\n",
    "# version_info = regression_analysis.test_os_cli('openstudio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '.'.join([version_info['major'], version_info['minor'], version_info['patch']])\n",
    "if version_info['prerelease']:\n",
    "    version += \"-{}\".format(version_info['prerelease'])\n",
    "\n",
    "sha = version_info['buildmetadata']\n",
    "\n",
    "# Force a new version (don't want to be prompted each time I parse df_files)\n",
    "new_version = compat_matrix.iloc[0].copy()\n",
    "new_version['OpenStudio'] = version\n",
    "new_version['E+'] = \"24.1.0\"\n",
    "new_version['SHA'] = sha\n",
    "new_version['Released'] = 'TBD'\n",
    "new_version['Has_Docker'] = False\n",
    "new_version['Ruby'] = '3.2.2'\n",
    "\n",
    "compat_matrix = compat_matrix.append(new_version).sort_values('OpenStudio', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** If you want to do it manually **"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Force a new version (don't want to be prompted each time I parse df_files)\n",
    "new_version = compat_matrix.iloc[0].copy()\n",
    "new_version['OpenStudio'] = \"2.6.1\"\n",
    "new_version['E+'] = \"8.9.0\"\n",
    "new_version['SHA'] = '00a34e0b1f'\n",
    "new_version['Released'] = datetime.datetime(2018, 8, 2) # '2018-08-02'\n",
    "new_version['Has_Docker'] = True\n",
    "\n",
    "compat_matrix = compat_matrix.append(new_version).sort_values('OpenStudio', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick look at the compat matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compat_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compat_matrix['Has_Docker'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oldest versions to have a Docker image\n",
    "compat_matrix[compat_matrix['Has_Docker']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Number of OpenStudio versions within each E+ version\n",
    "compat_matrix.groupby('E+')['OpenStudio'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "compat_matrix.groupby('E+')['OpenStudio'].count().plot(kind='barh', ax=ax)\n",
    "ax.set_xlim(0, compat_matrix.groupby('E+')['OpenStudio'].count().max())\n",
    "ax.set_title('Number of OpenStudio version for each E+ version')\n",
    "ax.set_xlabel('Number of OpenStudio Versions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "# compat_matrix.to_csv('compat_matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix permissions and skin down the fuelcell OSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skinning it down is done in the model_tests.rb now\n",
    "help(regression_analysis.cleanup_bloated_osws)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!stat --format '%a' test/fuelcell.osm_2.2.0_out.osw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The permissions stuff is done in the launch docker shell scripts\n",
    "\n",
    "If you want to do it manually\n",
    "\n",
    "Need to do:\n",
    "    \n",
    "    sudo chown -R $USER * \n",
    "    sudo find . -type f -exec chmod 664 {} \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse out.osw files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size: 16px; text-align: center;'><strong style='color: red;'>NOTE</strong>: This section analyzes the results of the <strong style='color: red;'>MODEL</strong> tests (`model_tests.rb`)</p>\n",
    "\n",
    "Sections 8, 9 and 10 at the end of the notebook help in analyzing the results of the `SDD_tests.rb`, `utilities_tests.rb` and `sql_tests.rb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "reload(regression_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without custom tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = regression_analysis.find_info_osws(compat_matrix=compat_matrix, testtype='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to most recent versions so to no clutter display\n",
    "df_files = df_files[['23.1.0', '23.2.0', '24.1.0']]\n",
    "#df_files = df_files[['9.4.0', '9.5.0']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With custom Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = regression_analysis.find_info_osws_with_tags(compat_matrix=compat_matrix,\n",
    "                                                        # Switch to True/False\n",
    "                                                        tags_only=False,\n",
    "                                                        testtype='model')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Limit to most recent versions so to no clutter display\n",
    "df_files = df_files[[\n",
    "    #'9.2.0', '9.3.0', \n",
    "    #'9.4.0',\n",
    "    #'9.5.0',\n",
    "    #'9.6.0'\n",
    "    '22.1.0',\n",
    "    '22.2.0',\n",
    "    '23.1.0',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eplus_versions = 3\n",
    "df_files = df_files[\n",
    "            df_files.columns.get_level_values(level='E+')\n",
    "            .unique()[-max_eplus_versions:]\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "SPACE_DEFAULTED_TO_ON = '3.5.0'\n",
    "\n",
    "def filter_on_space_enabled(df_files: pd.DataFrame, space_enabled: bool) -> pd.DataFrame:\n",
    "    \n",
    "    cols = []\n",
    "    for col in df_files.columns:\n",
    "        ep_version, os_version, space_tag = col\n",
    "        v = version.parse(os_version)\n",
    "        if v >= version.parse(SPACE_DEFAULTED_TO_ON):\n",
    "            if space_enabled:\n",
    "                if space_tag != 'WithoutSpaces':\n",
    "                    cols.append(col)\n",
    "            else:\n",
    "                if space_tag == 'WithoutSpaces':\n",
    "                    cols.append(col)\n",
    "        else:\n",
    "            if space_enabled:\n",
    "                if space_tag == 'WithSpaces':\n",
    "                    cols.append(col)\n",
    "            else:\n",
    "                if space_tag != 'WithSpaces':\n",
    "                    cols.append(col)\n",
    "    return df_files[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WithSpaces\n",
    "df_files = filter_on_space_enabled(df_files, True)\n",
    "\n",
    "# WithoutSpaces\n",
    "# df_files = filter_on_space_enabled(df_files, False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Limit to most recent versions, specifically so to no clutter display\n",
    "# use `df_files.columns.tolist()` to get a list of all, then keep ones you need\n",
    "df_files = df_files[\n",
    "[('9.5.0', '3.2.1', ''),\n",
    " ('9.5.0', '3.2.2-alpha', 'pre-v9.6.0-IOFreeze'),\n",
    "# ('9.6.0', '3.3.0-alpha', 'v9.6.0-IOFreeze'),\n",
    " ('9.6.0', '3.3.0-alpha', 'd92b642abb_preSpacePR'),\n",
    " ('9.6.0', '3.3.0-alpha', '96b7cd7d9c_NoSpaces'),\n",
    " #('9.6.0', '3.3.0-alpha', '6876f93714_WithSpaces'),\n",
    " ('9.6.0', '3.3.0-alpha', '6a0a54b2e8_WithSpacesFix'),\n",
    " ('9.6.0', '3.3.0-alpha', '0b194b8aef_WithSpacesAndDaylighting'),\n",
    "]\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_files[('8.9.0', '2.7.0', '02e314c21b')].notnull().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_files[('9.0.0', '2.7.0', '9828fbad12')].notnull().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!ls test/*f5a5effa71.osw | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove python tests\n",
    "# df_files = df_files[df_files.index.get_level_values('Type') != 'py']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the test status: Fail/Success/Blank"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from imp import reload\n",
    "reload(regression_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataframe \n",
    "# If you get an error, make sure the df_files is up to date by reruning section 3.1 or 3.2 above\n",
    "success = regression_analysis.success_sheet(df_files)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# success = regression_analysis.success_sheet(df_files.drop(columns=[('8.9.0', '2.6.0')]))\n",
    "success = regression_analysis.success_sheet(\n",
    "df_files.drop(columns=[ ('8.9.0', '2.5.0', 'Linux_run1'),\n",
    " ('8.9.0', '2.5.0', 'Linux_run10'),\n",
    " ('8.9.0', '2.5.0', 'Linux_run2'),\n",
    " ('8.9.0', '2.5.0', 'Linux_run3'),\n",
    " ('8.9.0', '2.5.0', 'Linux_run4'),\n",
    " ('8.9.0', '2.5.0', 'Linux_run5'),\n",
    " ('8.9.0', '2.5.0', 'Linux_run6'),\n",
    " ('8.9.0', '2.5.0', 'Linux_run7'),\n",
    " ('8.9.0', '2.5.0', 'Linux_run8'),\n",
    " ('8.9.0', '2.5.0', 'Linux_run9'),])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entire success table"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Output and style entire results\n",
    "(success.style.applymap(regression_analysis.background_colors).set_table_styles(regression_analysis.styles)\n",
    "          .set_caption(\"Test Success\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for a few tests only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only those where some are missing or failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = success.loc[success.any(axis=1)].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# filt = success[(success == '').any(axis=1) |\n",
    "#                (success == 'Fail').any(axis=1)].index.get_level_values(0).unique().tolist()\n",
    "\n",
    "filt = success['n_fail+missing']>0\n",
    "\n",
    "headers = {\n",
    "    'selector': 'th.col_heading',\n",
    "    'props': 'border-style: solid; border-width: 0.5px;'\n",
    "}\n",
    "\n",
    "(\n",
    "    success.loc[filt].style\n",
    "    .applymap(regression_analysis.background_colors)\n",
    "    .set_table_styles(regression_analysis.getStyles())\n",
    "    .set_caption(\"Test Success\")\n",
    "    # .set_table_styles([headers])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(success[success[('24.1.0', '3.8.0-rc2', \n",
    "                  'ca1c536250'\n",
    "                  #'WithoutSpaces'\n",
    "                 )] != 'Success'].style\n",
    "          .applymap(background_colors)\n",
    "          .set_table_styles(styles)\n",
    "          .set_caption(\"Test Success\"))\n",
    "\n",
    "#(success[success[('9.2.0', '2.9.0', 'develop_merge')] != 'Success'][['9.1.0', '9.2.0']].style\n",
    "#          .applymap(background_colors)\n",
    "#          .set_table_styles(styles)\n",
    "#          .set_caption(\"Test Success\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Filter on a single containing string\n",
    "filt = success.index.get_level_values(0).str.contains('shadowcalculation')\n",
    "\n",
    "# Filter on a pattern\n",
    "#filt = success.index.get_level_values(0).str.match(r'(exterior_equipment)|(meters)|(plant_op_schemes)|(avms_temp)')\n",
    "\n",
    "(success.loc[filt].style.applymap(background_colors).set_table_styles(styles)\n",
    "          .set_caption(\"Test Success\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output command to rerun the tests that used to run in the previous version\n",
    "# and now don't\n",
    "torun = success[(success[('23.2.0', '3.7.0', '')] == 'Success') &\n",
    "                (success[('24.1.0', '3.8.0-rc2', 'ca1c536250')] != 'Success')]\n",
    "\n",
    "# s = \"CUSTOMTAG=SHA /home/julien/Software/Others/OS-build/Products/openstudio-2.7.0 model_tests.rb -n '/\"\n",
    "s = \"openstudio model_tests.rb -n '/\"\n",
    "tests = []\n",
    "for i, (test, ext) in enumerate(torun.index.tolist()):\n",
    "    test_name = \"test_{}_{}\".format(test, ext)\n",
    "    #test_name = \"test_{}\".format(test)\n",
    "    #s += \" --name test_{}_{}\".format(test, ext)\n",
    "    if i < len(torun)-1:\n",
    "        s+='({})|'.format(test_name)\n",
    "    else:\n",
    "        s+='({})'.format(test_name)\n",
    "    tests.append(test_name)\n",
    "\n",
    "s += \"/'\"\n",
    "if tests:\n",
    "    print(s)\n",
    "else:\n",
    "    print(\"No new failures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torun.style.applymap(background_colors).set_table_styles(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet = '/EffiBEM&NREL-Regression-Test_Status'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# If needed: read https://df2gspread.readthedocs.io/en/latest/overview.html#status\n",
    "d2g.get_credentials()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wks_name = 'Test_Status'\n",
    "d2g.upload(df=success.T.reset_index().T.reset_index(),\n",
    "           gfile=spreadsheet, wks_name=wks_name,\n",
    "           row_names=False, col_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Missing tests: ruby versus osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_impl = regression_analysis.test_implemented_sheet(df_files=df_files, success=success,\n",
    "                                   only_for_mising_osm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_impl[~test_impl['osm']]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wks_name = 'Tests_Implemented'\n",
    "d2g.upload(test_impl,\n",
    "           gfile=spreadsheet, wks_name=wks_name,\n",
    "           row_names=True, col_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ouput the total_site_energy (kBTU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_kbtu = df_files.applymap(regression_analysis.parse_total_site_energy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wks_name = 'SiteKBTU'\n",
    "d2g.upload(df=site_kbtu.T.reset_index().T.reset_index().fillna(''),\n",
    "           gfile=spreadsheet, wks_name=wks_name,\n",
    "           # Skip first row\n",
    "           start_cell='A1',\n",
    "           row_names=False, col_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the rolling percent difference of total kBTU from one version to the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_kbtu_change = site_kbtu.pct_change(axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Output to google\n",
    "wks_name = 'SiteKBTU_Percent_Change'\n",
    "d2g.upload(site_kbtu.pct_change(axis=1).T.reset_index().T.reset_index().fillna(''),\n",
    "           gfile=spreadsheet, wks_name=wks_name,\n",
    "           row_names=False, col_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site_kbtu_change.loc['pv_and_storage_facilityexcess']\n",
    "#site_kbtu_change.loc[site_kbtu_change.index.get_level_values(0).str.contains('flat_plate')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_threshold = 1. / 100.0\n",
    "display_threshold = 0.1 / 100.0\n",
    "\n",
    "s_last_diff = site_kbtu_change.iloc[:, -1].sort_values()\n",
    "fail_mask = s_last_diff.abs() > row_threshold\n",
    "failures_idx = s_last_diff.index[fail_mask]\n",
    "\n",
    "warn_mask = s_last_diff.abs() > display_threshold\n",
    "warnings_idx = s_last_diff.index[warn_mask & ~fail_mask]\n",
    "\n",
    "nrows = (not failures_idx.empty) + (not warnings_idx.empty)\n",
    "\n",
    "n_fail = s_last_diff[failures_idx].size\n",
    "n_warn = s_last_diff[warnings_idx].size\n",
    "if n_fail + n_warn == 0:\n",
    "    print(\"Nothing to display\")\n",
    "else:\n",
    "    if nrows > 1:\n",
    "        fig, axes = plt.subplots(nrows=nrows, ncols=1, figsize=(16, n_fail + n_warn),\n",
    "                                 gridspec_kw={'height_ratios': [n_fail, n_warn]})\n",
    "    else:\n",
    "        fig, ax = plt.subplots(nrows=nrows, ncols=1)\n",
    "        axes = [ax]\n",
    "    \n",
    "    if not failures_idx.empty:\n",
    "        s_last_diff[failures_idx].plot(kind='barh', \n",
    "                                       # figsize=(16, s_last_diff[failures_idx].size),\n",
    "                                       title=f'Above {row_threshold=:.3%}', \n",
    "                                       ax=axes[0])\n",
    "    else:\n",
    "        print(f\"No diff above {row_threshold=:.3%}\")\n",
    "\n",
    "\n",
    "    if not warnings_idx.empty:\n",
    "        s_last_diff[warnings_idx].plot(kind='barh',\n",
    "                                       # figsize=(16, s_last_diff[warnings_idx].size),\n",
    "                                       title=f'Above {display_threshold=:.3%}',\n",
    "                                       ax=axes[-1])\n",
    "    else:\n",
    "        print(f\"No diff above {display_threshold=:.3%}\")\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:.2%}'.format(y))) \n",
    "        #vals = ax.get_xticks()\n",
    "        #ax.set_xticklabels(['{:,.2%}'.format(x) for x in vals])\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torun = failures_idx.tolist() + warnings_idx.tolist()\n",
    "g_toplot = site_kbtu_change[(site_kbtu_change.abs() >\n",
    "                                 row_threshold).any(axis=1)]\n",
    "torun = g_toplot.index.tolist()\n",
    "\n",
    "# s = \"CUSTOMTAG=SHA /home/julien/Software/Others/OS-build/Products/openstudio-2.7.0 model_tests.rb -n '/\"\n",
    "s = \"openstudio model_tests.rb -n '/\"\n",
    "tests = []\n",
    "for i, (test, ext) in enumerate(torun):\n",
    "    test_name = \"test_{}_{}\".format(test, ext)\n",
    "    #test_name = \"test_{}\".format(test)\n",
    "    #s += \" --name test_{}_{}\".format(test, ext)\n",
    "    if i < len(torun)-1:\n",
    "        s+='({})|'.format(test_name)\n",
    "    else:\n",
    "        s+='({})'.format(test_name)\n",
    "    tests.append(test_name)\n",
    "\n",
    "s += \"/'\"\n",
    "if tests:\n",
    "    print(s)\n",
    "else:\n",
    "    print(\"No new failures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap > 1% change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "row_threshold = 1. / 100.0\n",
    "display_threshold = 0.1 / 100.0\n",
    "\n",
    "figname = 'site_kbtu_pct_change_row{}_display{}.png'.format(row_threshold, \n",
    "                                                            display_threshold)\n",
    "\n",
    "print(\"Row threshold = {:.2%}, Cell Display Threshold = {:.2%}\".format(row_threshold, display_threshold))\n",
    "\n",
    "regression_analysis.heatmap_sitekbtu_pct_change(site_kbtu_change=site_kbtu_change,\n",
    "                            row_threshold=row_threshold, display_threshold=display_threshold,\n",
    "                            savefig=True, figname=figname,\n",
    "                            show_plot=True, save_indiv_figs_for_ax=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y' - x' = alpha(y-x)\n",
    "y' - y = x - x' = a\n",
    "<=>\n",
    "a = (alpha-1) * (y-x) / 2\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1)\n",
    "plt.show()\n",
    "extent = (ax.get_tightbbox(fig.canvas.renderer)\n",
    "            .transformed(fig.dpi_scale_trans.inverted()))\n",
    "a = (alpha-1)*(extent.ymax-extent.ymin) / 2.0\n",
    "mpl.transforms.Bbox([[extent.xmin, extent.ymin], [extent.xmax, extent.ymax]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap > 0.5% change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "row_threshold  = 0.05 / 100.0\n",
    "display_threshold=0.01 / 100.0\n",
    "\n",
    "figname = 'site_kbtu_pct_change_row{}_display{}.png'.format(row_threshold, \n",
    "                                                            display_threshold)\n",
    "print(\"Row threshold = {:.2%}, Cell Display Threshold = {:.2%}\".format(row_threshold, display_threshold))\n",
    "regression_analysis.heatmap_sitekbtu_pct_change(site_kbtu_change=site_kbtu_change,\n",
    "                            row_threshold=row_threshold, display_threshold=display_threshold,\n",
    "                            savefig=True, figname=figname,\n",
    "                            show_plot=True, save_indiv_figs_for_ax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1 / 100.0\n",
    "NEW_EP = \"23.1.0\"\n",
    "NEW_OS = \"3.6.0\"\n",
    "new_diffs = site_kbtu_change.loc[site_kbtu_change[(NEW_EP, NEW_OS)].abs() >= threshold, NEW_EP]\n",
    "# Sort by abs of 2.4.2\n",
    "new_diffs = new_diffs.loc[new_diffs[NEW_OS].abs().sort_values(ascending=False).index]\n",
    "# new_diffs.to_csv('NewDiffs.csv')\n",
    "print(f\"New differences that are above a threshold of {threshold:.3%}\")\n",
    "(new_diffs.style\n",
    "          .format(lambda x: '{:.3%}'.format(x) if not np.isnan(x) else '-')\n",
    "          .set_table_styles([{'selector': 'tr', 'props': [('border-style','solid'),('border-width','1px')]}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"_\".join(x) for x in new_diffs.index.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output command to run the ones were we have big diffs\n",
    "torun = new_diffs[new_diffs[NEW_OS].abs()>= (0.1/100.0)]\n",
    "\n",
    "s = \"openstudio model_tests.rb -n '/\"\n",
    "tests = []\n",
    "for i, (test, ext) in enumerate(torun.index.tolist()):\n",
    "    if test == 'unitary_system_performance_multispeed':\n",
    "        continue\n",
    "    test_name = \"test_{}_{}\".format(test, 'osm')\n",
    "    #test_name = \"test_{}\".format(test)\n",
    "    #s += \" --name test_{}_{}\".format(test, ext)\n",
    "    if i < len(torun)-1:\n",
    "        s+='({})|'.format(test_name)\n",
    "    else:\n",
    "        s+='({})'.format(test_name)\n",
    "    tests.append(test_name)\n",
    "\n",
    "s += \"/'\"\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_diffs[new_diffs[NEW_OS].abs()>= (0.1/100.0)].index.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = site_kbtu.iloc[:, 1:].div(site_kbtu.iloc[:, 0], axis=0)-1\n",
    "diff = diff[(diff != 0.0).any(axis=1)]\n",
    "diff = diff[diff.notnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def style_zero(v, props=''):\n",
    "    return props if v == 0 else None\n",
    "diff.style.bar(align='mid', color=['#d65f5f', '#5fba7d']).applymap(style_zero, props='color:green;').format('{:.3%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_threshold = 1. / 100.0\n",
    "display_threshold = 0.1 / 100.0\n",
    "\n",
    "row_threshold  = 0.05 / 100.0\n",
    "display_threshold=0.01 / 100.0\n",
    "\n",
    "row_threshold = 0.005 / 100.0\n",
    "display_threshold = 0.001 / 100.0\n",
    "\n",
    "figname = 'site_kbtu_abs_pct_change_row{}_display{}.png'.format(row_threshold, \n",
    "                                                            display_threshold)\n",
    "\n",
    "print(\"Row threshold = {:.3%}, Cell Display Threshold = {:.3%}\".format(row_threshold, display_threshold))\n",
    "\n",
    "regression_analysis.heatmap_sitekbtu_pct_change(site_kbtu_change=diff,\n",
    "                            row_threshold=row_threshold, display_threshold=display_threshold,\n",
    "                            savefig=False, figname=figname,\n",
    "                            show_plot=True, save_indiv_figs_for_ax=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference in end use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input a threshold, here 0.5%\n",
    "threshold = 0.5/100.0\n",
    "\n",
    "over_threshold = (site_kbtu.pct_change(axis=1).abs() > threshold).sum(axis=0).to_frame()\n",
    "col = 'Count (ABS(pct_diff) > {:.2%})'.format(threshold)\n",
    "over_threshold.columns = [col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(over_threshold.style.set_table_styles([{'selector': 'tr', 'props': [('border-style','solid'),('border-width','1px')]}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_threshold.replace(0, np.nan).dropna().sort_values(col, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_1 = '3.1.0'\n",
    "version_2 = '3.2.0'\n",
    "\n",
    "\n",
    "all_diffs = {}\n",
    "failed = {}\n",
    "for index, row in  df_files.T.reset_index(level=0, drop=True).T.iterrows():\n",
    "    diff_ok = True\n",
    "    try:\n",
    "        cleaned_end_use_2 = regression_analysis.parse_end_use(row[version_2])\n",
    "        ok2 = True\n",
    "    except:\n",
    "        cleaned_end_use_2 = 'Failed'\n",
    "        diff_ok = False\n",
    "        ok2 = False\n",
    "    try:\n",
    "        cleaned_end_use_1 = regression_analysis.parse_end_use(row[version_1])\n",
    "        ok1 = True\n",
    "    except:\n",
    "        cleaned_end_use_1 = 'Failed'\n",
    "        diff_ok = False\n",
    "        ok1 = False\n",
    "    if diff_ok:\n",
    "        pct_diff = (cleaned_end_use_2 - cleaned_end_use_1) / cleaned_end_use_1\n",
    "        \n",
    "        all_diffs[index] = {version_1: cleaned_end_use_1,\n",
    "                            version_2: cleaned_end_use_2,\n",
    "                            'diff': pct_diff}\n",
    "    else:\n",
    "        failed[index] = {version_1: ok1,\n",
    "                         version_2: ok2}\n",
    "        \n",
    "df_failed = pd.DataFrame(failed).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the ones that changed: False means it fails, True means it worked\n",
    "df_failed[df_failed[version_1] != df_failed[version_2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_diffs = {}\n",
    "for test, d in all_diffs.items():\n",
    "    #dmax = \n",
    "    max_diffs[test] = {'Max': d['diff'].max().max(),\n",
    "                       'Min': d['diff'].min().min(),\n",
    "                       'Total Diff': (d[version_2][('Total', 'kBtu')].sum()\n",
    "                                      - d[version_1][('Total', 'kBtu')].sum()) / d[version_1][('Total', 'kBtu')].sum()}\n",
    "    \n",
    "    \n",
    "df_diffs = pd.DataFrame(max_diffs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diffs[~(df_diffs == 0).all(axis=1)].style.format(\"{:.5%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ('air_chillers', 'osm')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "\n",
    "fmt = lambda x,pos: '{:.0%}'.format(x)\n",
    "\n",
    "sns.heatmap(all_diffs[test]['diff'].dropna(how='all', axis=0).dropna(how='all', axis=1).abs(),\n",
    "            ax=ax, cmap='YlOrRd',\n",
    "            vmin=0, vmax=1,\n",
    "            cbar_kws={'format': mpl.ticker.FuncFormatter(fmt)},\n",
    "            annot=all_diffs[test]['diff'].dropna(how='all', axis=0).dropna(how='all', axis=1), fmt='.1%')\n",
    "ax.set_title(\"Percent difference in End Use By Fuel for test '{}.{}' between {}\"\n",
    "             \" and {}\".format(test[0], test[1], version_2, version_1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ruby versus Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = regression_analysis.find_info_osws(compat_matrix=compat_matrix, testtype='model')\n",
    "df_files = df_files[['23.2.0']]\n",
    "site_kbtu = df_files.applymap(regression_analysis.parse_total_site_energy)\n",
    "rb_py = site_kbtu.loc[:, ('23.2.0', '3.7.0')].unstack('Type')[['rb', 'py']]\n",
    "rb_py = rb_py[rb_py.any(axis=1)]\n",
    "rb_py = rb_py[rb_py.diff(axis=1).abs().iloc[:, -1] > 0]\n",
    "rb_py['abs_diff'] = rb_py['py'] - rb_py['rb']\n",
    "rb_py['rel_diff'] = rb_py['abs_diff'] / rb_py['rb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    rb_py.reindex(rb_py['rel_diff'].abs().sort_values(ascending=False).index)\n",
    "    .style\n",
    "    .format('{:,.0f}', subset=['rb', 'py', 'abs_diff'])\n",
    "    .format('{:.4%}', subset='rel_diff')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find missing tests: Map tests to Cpp classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grep in ruby and osm tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(ROOT_DIR, 'model/simulationtests/'))\n",
    "\n",
    "# Grep in ruby test for Model:: statements (works on Unix only)\n",
    "grep = !grep \"Model::\" *.rb\n",
    "objs = pd.DataFrame([x.split(':', maxsplit=1 ) for x in grep], columns=['file', 'grepped_line'])\n",
    "\n",
    "# Grep in ruby test for Model:: statements\n",
    "grep_lib = !/bin/grep \"Model::\" ./lib/*.rb\n",
    "objs_lib = pd.DataFrame(grep_lib, columns=['grepped_line'])\n",
    "objs_lib['file'] = 'lib/baseline_model.rb'\n",
    "\n",
    "# Find all Model namespace Classes by getting name from the cpp files\n",
    "os_classes = !ls ~/Software/Others/OpenStudio/openstudiocore/src/model/*.cpp\n",
    "os_classes = [os.path.split(os.path.splitext(p)[0])[1] for p in os_classes]\n",
    "\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_object_pat = re.compile(r'OpenStudio::Model::(.*?)\\.new')\n",
    "def parse_model_object(s):\n",
    "    m = model_object_pat.search(s)\n",
    "    if m:\n",
    "        return m.groups()[0]\n",
    "    else:\n",
    "        print('Cannot match {}'.format(s))\n",
    "        return None\n",
    "    \n",
    "objs['ModelObject'] = objs['grepped_line'].apply(parse_model_object)\n",
    "objs_lib['ModelObject'] = objs_lib['grepped_line'].apply(parse_model_object)\n",
    "\n",
    "# Concat both\n",
    "objs = pd.concat([objs, objs_lib])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'AirTerminalSingleDuctUncontrolled' was deprecated (by E+) \n",
    "# and replaced with 'AirTerminalSingleDuctConstantVolumeNoReheat'\n",
    "set(objs['ModelObject']) - set(os_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(os_classes) - set(objs['ModelObject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_os_classes = pd.DataFrame(index=os_classes)\n",
    "df_os_classes['In Ruby Test'] = False\n",
    "df_os_classes = df_os_classes.join(objs.groupby('ModelObject')['file'].apply(list))\n",
    "df_os_classes.loc[df_os_classes['file'].notnull(),\n",
    "                  'file'] = df_os_classes.loc[df_os_classes['file'].notnull(),\n",
    "                                              'file'].apply(np.unique)\n",
    "df_os_classes.loc[df_os_classes['file'].notnull(), 'In Ruby Test'] = True\n",
    "df_os_classes = df_os_classes.rename(columns={'file': 'files'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_os_classes['In Ruby Test'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_os_classes['In Ruby Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_os_classes.to_csv('Mapping_ruby_test_to_cpp_classes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get comments dict from the google sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from df2gspread import gspread2df as g2d\n",
    "\n",
    "spreadsheet = '/EffiBEM&NREL-Regression-Test_Status'\n",
    "wks_name = 'Mapping_ruby_test_to_cpp_classes'\n",
    "\n",
    "df = g2d.download(spreadsheet, wks_name, col_names = True, row_names = True)\n",
    "#comments_dict = df['IsNormal'].to_dict()\n",
    "comments_dict = df.loc[df['IsNormal'] != '', 'IsNormal'].to_dict()\n",
    "comments_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(comments_dict)\n",
    "s = s[s.str.lower().str.contains('added')].str.split(':', expand=True)[1].str.strip().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tot_obj = 0\n",
    "n_tot_tests = 0\n",
    "for index, val in s.reset_index().groupby(1)['index'].apply(list).items():\n",
    "    if index == 'pv_and_storage_facilityexcess.rb':\n",
    "        test = 'pv_and_storage_facilityexcess.rb and pv_and_storage_demandleveling.rb'\n",
    "        n_tot_tests += 1\n",
    "    else:\n",
    "        test = index\n",
    "    n_tot_tests += 1\n",
    "    n_tot_obj += len(val)\n",
    "    print(\"**{}** ({})\".format(test, len(val)))\n",
    "    print()\n",
    "    for x in val:\n",
    "        print(\"* {}\".format(x))\n",
    "    print(\"\\n\")\n",
    "print(\"\\n**Total Added: {} objects in {} tests**\".format(n_tot_obj, n_tot_tests))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Saved on 2018-12-18\n",
    "comments_dict = {'AccessPolicyStore': 'TRUE',\n",
    " 'AdditionalProperties': 'Tested in additional_props.rb',\n",
    " 'AirLoopHVACReturnPlenum': 'TRUE',\n",
    " 'AirLoopHVACSupplyPlenum': 'TRUE',\n",
    " 'AirLoopHVACZoneMixer': 'TRUE',\n",
    " 'AirLoopHVACZoneSplitter': 'TRUE',\n",
    " 'AirToAirComponent': 'TRUE',\n",
    " 'AvailabilityManagerAssignmentList': 'Normal, this is used via plant/airLoop addAvailabilityManager',\n",
    " 'AvailabilityManager': 'Base Class',\n",
    " 'AvailabilityManagerDifferentialThermostat': 'Just Added: plantloop_avms_temp.rb',\n",
    " 'AvailabilityManagerHighTemperatureTurnOff': 'Just Added: plantloop_avms_temp.rb',\n",
    " 'AvailabilityManagerHighTemperatureTurnOn': 'Just Added: plantloop_avms_temp.rb',\n",
    " 'AvailabilityManagerLowTemperatureTurnOff': 'Just Added: plantloop_avms_temp.rb',\n",
    " 'AvailabilityManagerLowTemperatureTurnOn': 'Just Added: plantloop_avms_temp.rb',\n",
    " 'AvailabilityManagerNightCycle': 'availability_managers.rb & airloop_avms.rb via AirLoopHVAC::setNightCycleControlType(\"CycleOnAny\")',\n",
    " 'BoilerSteam': \"I don't think this works because that's the only steam object...\",\n",
    " 'Building': 'TRUE',\n",
    " 'BuildingStory': 'TRUE',\n",
    " 'CentralHeatPumpSystem': 'Just Added: centralheatpumpsystem.rb',\n",
    " 'CentralHeatPumpSystemModule': 'Just Added: centralheatpumpsystem.rb',\n",
    " 'Component': 'TRUE',\n",
    " 'ComponentData': 'TRUE',\n",
    " 'ComponentWatcher': 'TRUE',\n",
    " 'Connection': 'TRUE',\n",
    " 'ConnectorMixer': 'TRUE',\n",
    " 'ConstructionBase': 'TRUE',\n",
    " 'CurveBicubic': 'TRUE',\n",
    " 'Curve': 'TRUE',\n",
    " 'CurveDoubleExponentialDecay': 'TRUE',\n",
    " 'CurveExponentialDecay': 'TRUE',\n",
    " 'CurveExponentialSkewNormal': 'TRUE',\n",
    " 'CurveFanPressureRise': 'TRUE',\n",
    " 'CurveFunctionalPressureDrop': 'TRUE',\n",
    " 'CurveLinear': 'TRUE',\n",
    " 'CurveQuadraticLinear': 'TRUE',\n",
    " 'CurveQuartic': 'TRUE',\n",
    " 'CurveRectangularHyperbola1': 'TRUE',\n",
    " 'CurveRectangularHyperbola2': 'TRUE',\n",
    " 'CurveSigmoid': 'TRUE',\n",
    " 'CurveTriquadratic': 'TRUE',\n",
    " 'DefaultConstructionSet': 'TRUE',\n",
    " 'DefaultScheduleSet': 'TRUE',\n",
    " 'DefaultSubSurfaceConstructions': 'TRUE',\n",
    " 'DefaultSurfaceConstructions': 'TRUE',\n",
    " 'DesignDay': 'TRUE',\n",
    " 'DesignSpecificationZoneAirDistribution': 'Not in ruby API',\n",
    " 'ElectricalStorage': 'TRUE',\n",
    " 'ElectricLoadCenterInverterLookUpTable': 'Just Added: pv_and_storage_facilityexcess.rb',\n",
    " 'ElectricLoadCenterStorageConverter': 'Just Added: pv_and_storage_facilityexcess.rb',\n",
    " 'ElectricLoadCenterStorageSimple': 'Just Added: pv_and_storage_facilityexcess.rb',\n",
    " 'ExteriorFuelEquipment': 'Just Added: exterior_equipment.rb',\n",
    " 'ExteriorFuelEquipmentDefinition': 'Just Added: exterior_equipment.rb',\n",
    " 'ExteriorLoadDefinition': 'Base Class',\n",
    " 'ExteriorLoadInstance': 'Base Class',\n",
    " 'ExteriorWaterEquipment': 'Just Added: exterior_equipment.rb',\n",
    " 'ExteriorWaterEquipmentDefinition': 'Just Added: exterior_equipment.rb',\n",
    " 'ExternalFile': 'Tested via schedule_file.rb',\n",
    " 'Facility': 'TRUE',\n",
    " 'FenestrationMaterial': 'Base class for ShadingMaterial, Glazing, GasLayer',\n",
    " 'FileOperations': 'TRUE',\n",
    " 'FloorplanJSForwardTranslator': 'TRUE',\n",
    " 'FoundationKivaSettings': 'foundation_kiva.rb, via model.getFoundationKivaSettings',\n",
    " 'Gas': 'TRUE',\n",
    " 'GasEquipment': 'Just Added: space_load_instances.rb',\n",
    " 'GasEquipmentDefinition': 'Just Added: space_load_instances.rb',\n",
    " 'Generator': 'TRUE',\n",
    " 'GeneratorMicroTurbine': 'Just Added: generator_microturbine.rb',\n",
    " 'GeneratorMicroTurbineHeatRecovery': 'Just Added: generator_microturbine.rb',\n",
    " 'GeneratorPhotovoltaic': 'Tested for in photovoltaics.rb',\n",
    " 'GenericModelObject': 'TRUE',\n",
    " 'Glazing': 'TRUE',\n",
    " 'HotWaterEquipment': 'Just Added: space_load_instances.rb',\n",
    " 'HotWaterEquipmentDefinition': 'Just Added: space_load_instances.rb',\n",
    " 'HVACComponent': 'TRUE',\n",
    " 'HVACTemplates': 'This is used in most files through the lib/baseline_model.rb',\n",
    " 'Inverter': 'TRUE',\n",
    " 'LayeredConstruction': 'TRUE',\n",
    " 'LifeCycleCost': 'It is tested for LifeCycleParameters.rb',\n",
    " 'LifeCycleCostParameters': 'It is tested for LifeCycleParameters.rb',\n",
    " 'LifeCycleCostUsePriceEscalation': 'It is tested for LifeCycleParameters.rb',\n",
    " 'Loop': 'TRUE',\n",
    " 'Luminaire': 'Not sure how to use it',\n",
    " 'LuminaireDefinition': 'Not sure how to use it',\n",
    " 'Material': 'TRUE',\n",
    " 'MeterCustom': 'Just Added: meters.rb',\n",
    " 'MeterCustomDecrement': 'Just Added: meters.rb',\n",
    " 'Mixer': 'TRUE',\n",
    " 'ModelExtensibleGroup': 'TRUE',\n",
    " 'ModelMerger': 'TRUE',\n",
    " 'ModelObject': 'TRUE',\n",
    " 'ModelObjectList': 'TRUE',\n",
    " 'Node': 'TRUE',\n",
    " 'OpaqueMaterial': 'True, base class',\n",
    " 'OtherEquipment': 'Just Added: space_load_instances.rb',\n",
    " 'OtherEquipmentDefinition': 'Just Added: space_load_instances.rb',\n",
    " 'OutputMeter': 'Just Added: meters.rb',\n",
    " 'ParentObject': 'TRUE',\n",
    " 'PhotovoltaicPerformance': 'Base class',\n",
    " 'PhotovoltaicPerformanceSimple': 'Just Added: pv_and_storage_facilityexcess.rb',\n",
    " 'PlanarSurface': 'TRUE',\n",
    " 'PlanarSurfaceGroup': 'TRUE',\n",
    " 'PlantEquipmentOperationOutdoorDewpoint': 'Just Added: plant_op_schemes_temp.rb',\n",
    " 'PlantEquipmentOperationOutdoorDewpointDifference': 'Just Added: plant_op_schemes_deltatemp.rb',\n",
    " 'PlantEquipmentOperationOutdoorDryBulb': 'Just Added: plant_op_schemes_temp.rb',\n",
    " 'PlantEquipmentOperationOutdoorDryBulbDifference': 'Just Added: plant_op_schemes_deltatemp.rb',\n",
    " 'PlantEquipmentOperationOutdoorRelativeHumidity': 'Just Added: plant_op_schemes_temp.rb',\n",
    " 'PlantEquipmentOperationOutdoorWetBulbDifference': 'Just Added: plant_op_schemes_deltatemp.rb',\n",
    " 'PlantEquipmentOperationRangeBasedScheme': 'Base Class',\n",
    " 'PlantEquipmentOperationScheme': 'Base Class',\n",
    " 'PortList': 'TRUE',\n",
    " 'RenderingColor': 'TRUE',\n",
    " 'ResourceObject': 'TRUE',\n",
    " 'RoofVegetation': 'Just Added: roof_vegetation.rb',\n",
    " 'ScheduleBase': 'Base Class',\n",
    " 'Schedule': 'TRUE',\n",
    " 'ScheduleTypeRegistry': 'TRUE',\n",
    " 'ScheduleWeek': 'TRUE',\n",
    " 'ScheduleYear': 'TRUE',\n",
    " 'SetpointManager': 'TRUE',\n",
    " 'Shade': 'TRUE',\n",
    " 'Site': 'TRUE',\n",
    " 'SizingPeriod': 'TRUE',\n",
    " 'SizingPlant': 'TRUE',\n",
    " 'SizingSystem': 'TRUE',\n",
    " 'SizingZone': 'TRUE',\n",
    " 'SkyTemperature': \"Forward translator does nothing, and it doesn't have setters nor getters: https://github.com/jmarrec/OpenStudio/blob/develop/openstudiocore/src/energyplus/ForwardTranslator/ForwardTranslateSkyTemperature.cpp#L42\",\n",
    " 'SolarCollectorPerformanceFlatPlate': 'Added explicit manipulation of object in solar_collector_flat_plate_water.rb',\n",
    " 'Space': 'TRUE',\n",
    " 'SpaceItem': 'TRUE',\n",
    " 'SpaceLoad': 'TRUE',\n",
    " 'SpaceLoadDefinition': 'TRUE',\n",
    " 'SpaceLoadInstance': 'TRUE',\n",
    " 'SpaceType': 'TRUE',\n",
    " 'Splitter': 'TRUE',\n",
    " 'SteamEquipment': 'Just Added: space_load_instances.rb',\n",
    " 'SteamEquipmentDefinition': 'Just Added: space_load_instances.rb',\n",
    " 'StraightComponent': 'Base Class',\n",
    " 'SubSurface': 'TRUE',\n",
    " 'Surface': 'TRUE',\n",
    " 'Thermostat': 'TRUE',\n",
    " 'ThreeJSForwardTranslator': 'TRUE',\n",
    " 'ThreeJSReverseTranslator': 'TRUE',\n",
    " 'UtilityCost_Charge_Block': 'Not Functional in API',\n",
    " 'UtilityCost_Charge_Simple': 'Not Functional in API',\n",
    " 'UtilityCost_Computation': 'Not Functional in API',\n",
    " 'UtilityCost_Qualify': 'Not Functional in API',\n",
    " 'UtilityCost_Ratchet': 'Not Functional in API',\n",
    " 'UtilityCost_Tariff': 'Not Functional in API',\n",
    " 'UtilityCost_Variable': 'Not Functional in API',\n",
    " 'Version': 'TRUE',\n",
    " 'WaterToAirComponent': 'TRUE',\n",
    " 'WaterToWaterComponent': 'TRUE',\n",
    " 'YearDescription': 'TRUE',\n",
    " 'ZoneHVACComponent': 'TRUE',\n",
    " 'ZoneHVACEquipmentList': 'TRUE'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comments.set_index('Test')['IsNormal'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge comments\n",
    "comments = pd.Series(comments_dict, name='IsNormal')\n",
    "df_os_classes = df_os_classes.join(comments)\n",
    "df_os_classes = df_os_classes[['In Ruby Test', 'IsNormal', 'files']]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filt1 = ~df_os_classes['In Ruby Test']\n",
    "filt2 = df_os_classes['IsNormal'].isnull()\n",
    "df_os_classes[filt1 & filt2] # .apply(lambda x: print(x.name), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find objects in the osm tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dir to the model test directory\n",
    "os.chdir(os.path.join(ROOT_DIR, 'model/simulationtests/'))\n",
    "\n",
    "# Compile a regex\n",
    "os_class_pattern = re.compile(r'OS:(.*?),')\n",
    "\n",
    "# Initialize a column of empty lists\n",
    "df_os_classes['osms'] = np.empty((len(df_os_classes), 0)).tolist()\n",
    "\n",
    "# Loop on all osms, and find OS objects\n",
    "for osm_path in gb.glob('*.osm'):\n",
    "    with open(osm_path) as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        m = os_class_pattern.match(line)\n",
    "        if m:\n",
    "            classname = m.groups()[0].replace(':','')\n",
    "            # Deprecated by E+\n",
    "            if classname == 'AirTerminalSingleDuctUncontrolled':\n",
    "                classname = 'AirTerminalSingleDuctConstantVolumeNoReheat'\n",
    "            if classname in df_os_classes.index:\n",
    "                # Append osm_path to list if not already in it\n",
    "                if not osm_path in df_os_classes.loc[classname, 'osms']:\n",
    "                    df_os_classes.loc[classname, 'osms'].append(osm_path)\n",
    "            else:\n",
    "                print(\"Cannot find {} in df_os_classes\".format(classname))\n",
    "                \n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_os_classes.loc[df_os_classes['osms'].apply(len) == 0, 'osms'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt1 = ~df_os_classes['In Ruby Test']\n",
    "filt2 = df_os_classes['IsNormal'].isnull()\n",
    "filt3 = df_os_classes['osms'].isnull()\n",
    "df_os_classes[filt1 & filt2 & filt3] # .apply(lambda x: print(x.name), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ruby = df_os_classes['In Ruby Test']\n",
    "in_ruby.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_normal = df_os_classes['IsNormal'].notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_osm = df_os_classes['osms'].notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(in_ruby | is_normal).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(in_ruby | is_normal | in_osm).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_os_classes.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Google"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spreadsheet = '/EffiBEM&NREL-Regression-Test_Status'\n",
    "wks_name = 'Mapping_ruby_test_to_cpp_classes'\n",
    "d2g.upload(df_os_classes.fillna(''),\n",
    "           gfile=spreadsheet, wks_name=wks_name,\n",
    "           row_names=True, col_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test convergence of OSMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the same in.OSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_N_TIMES = 5\n",
    "START_AT=5\n",
    "OS_CLI='openstudio'\n",
    "# OS_CLI='ruby'\n",
    "OS_CLI = '/home/julien/Software/Others/OS-build/Products/openstudio'\n",
    "# Windows path should be like:\n",
    "# OS_CLI = r'C:\\openstudio-2.5.0\\bin\\openstudio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from shutil import copyfile\n",
    "\n",
    "# Modify to suit your needs. You should have run the model_tests.rb with this\n",
    "# test first, so that the in.osw etc exists first...\n",
    "os.chdir(os.path.join(ROOT_DIR, 'testruns/evaporative_cooling.osm/'))\n",
    "\n",
    "r = {}\n",
    "o = {}\n",
    "e = {}\n",
    "for i in range(START_AT, START_AT + RUN_N_TIMES):\n",
    "    process = subprocess.Popen([OS_CLI, 'run', '-w', 'in.osw'], shell=False,\n",
    "                           stdout=subprocess.PIPE, \n",
    "                           stderr=subprocess.PIPE)\n",
    "\n",
    "    # wait for the process to terminate\n",
    "    out, err = process.communicate()\n",
    "    o[i] = out\n",
    "    e[i] = err\n",
    "    errcode = process.returncode\n",
    "    r[i] = regression_analysis.parse_total_site_energy('out.osw')\n",
    "    print(\"{} - {:,.0f}\".format(i, r[i]))\n",
    "    # Copy idf file\n",
    "    idf_path = os.path.abspath('./run/in.idf')\n",
    "    copyfile(idf_path, os.path.abspath('./run_{}.idf'.format(i)))\n",
    "    \n",
    "# Say to user\n",
    "if regression_analysis.platform.system() == 'Linux':\n",
    "    !echo \"THIS IS DONE\" | espeak\n",
    "\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = pd.concat([result, pd.Series(r)])\n",
    "except:\n",
    "    result = pd.Series(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ((result - result.iloc[0]) / result.iloc[0]).plot(kind='bar')\n",
    "def fmt(x, pos): return '{:.0%}'.format(x)\n",
    "\n",
    "ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(fmt))\n",
    "ax.set_title('% change compared to first run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare With Custom Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Sim Tests N Times with Custom Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLATFORM = 'Ubuntu'\n",
    "PLATFORM = None\n",
    "\n",
    "N = 5\n",
    "START_AT = 1\n",
    "# Override if you have only run N times before, and do not want to override\n",
    "# START_AT = 10\n",
    "\n",
    "# Save idf file next to out.osw? \n",
    "# Useful for checking IDF diffs for ruby tests that are unstable\n",
    "SAVE_IDF=False\n",
    "\n",
    "# Filter to pass to model_tests.rb. Input NONE for all tests\n",
    "REGRESSION_TEST_FILTER = None\n",
    "\n",
    "# Override examples:\n",
    "# REGRESSION_TEST_FILTER = '(test_absorption_chillers_rb)|(test_additional_props_rb)|(test_air_chillers_rb)|(test_air_terminals_rb)|(test_airloop_and_zonehvac_rb)|(test_airterminal_cooledbeam_rb)|(test_autosize_hvac_rb)|(test_centralheatpumpsystem_rb)|(test_coolingtowers_rb)|(test_dist_ht_cl_rb)|(test_dsn_oa_w_ideal_loads_rb)|(test_dual_duct_rb)|(test_ducts_and_pipes_rb)|(test_ems_rb)|(test_evaporative_cooling_rb)|(test_exterior_equipment_rb)|(test_fan_on_off_rb)|(test_fuelcell_rb)|(test_generator_microturbine_rb)|(test_headered_pumps_rb)|(test_hightemprad_rb)|(test_humidity_control_rb)|(test_interior_partitions_rb)|(test_lowtemprad_constflow_rb)|(test_lowtemprad_electric_rb)|(test_lowtemprad_varflow_rb)|(test_multi_stage_rb)|(test_plant_op_deltatemp_schemes_rb)|(test_plantloop_avms_temp_rb)|(test_plenums_rb)|(test_pv_and_storage_demandleveling_rb)|(test_pv_and_storage_facilityexcess_rb)|(test_roof_vegetation_rb)|(test_solar_collector_flat_plate_water_rb)|(test_space_load_instances_rb)|(test_surface_properties_rb)|(test_unitary_system_performance_multispeed_rb)|(test_vrf_rb)|(test_water_heaters_rb)|(test_zone_control_contaminant_controller_rb)|(test_zone_fan_exhaust_rb)'\n",
    "# REGRESSION_TEST_FILTER = 'centralheatpumpsystem_osm'\n",
    "# REGRESSION_TEST_FILTER = 'fourpipebeam'\n",
    "# REGRESSION_TEST_FILTER = 'somethingthatdoesntexistfortesting'\n",
    "\n",
    "# If you need to hardset the E+ executable path, otherwise leave as None\n",
    "ENERGYPLUS_EXE_PATH = None\n",
    "# ENERGYPLUS_EXE_PATH = '/home/julien/Software/Others/OS-build/EnergyPlus-8.9.0-40101eaafd-Linux-x86_64/EnergyPlus-8-9-0/energyplus-8.9.0'\n",
    "\n",
    "# Path to your cli, if use system ruby make sure it's setup correctly via openstudio.rb\n",
    "# If you pass None, defaults to 'openstudio' (has to be in PATH)\n",
    "OS_CLI = None\n",
    "# OS_CLI = '/home/julien/Software/Others/OS-build2/Products/openstudio-2.4.3'\n",
    "# OS_CLI = 'ruby'\n",
    "# Windows path should be like this (forward slashes work both in linux and windows)\n",
    "# OS_CLI = 'C:/openstudio-2.5.0/bin/openstudio'\n",
    "# or (notice the 'r' denoting a raw string)\n",
    "# OS_CLI = r'C:\\openstudio-2.5.0\\bin\\openstudio'\n",
    "# You can test this\n",
    "# regression_analysis.test_os_cli(OS_CLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regression_analysis.test_stability(os_cli=OS_CLI, test_filter=REGRESSION_TEST_FILTER,\n",
    "               run_n_times=N, start_at=START_AT,\n",
    "               save_idf=SAVE_IDF,\n",
    "               energyplus_exe_path=ENERGYPLUS_EXE_PATH,\n",
    "               platform_name=PLATFORM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Custom-tagged files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = regression_analysis.find_info_osws_with_tags(compat_matrix=None,\n",
    "                                                        tags_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = regression_analysis.success_sheet(df_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = success['n_fail+missing']>0\n",
    "\n",
    "(success.loc[filt].style\n",
    "          .applymap(background_colors)\n",
    "          .set_table_styles(styles)\n",
    "          .set_caption(\"Test Success\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First filter only tests that have some variations in site kBTU\n",
    "\n",
    "I check for tests where the min accross runs isn't equal to the max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_kbtu = df_files.applymap(regression_analysis.parse_total_site_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_kbtu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_analysis.heatmap_sitekbtu_pct_change(site_kbtu=site_kbtu, row_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to our version of interest, drop rows with all nan\n",
    "site_kbtu_this_version = site_kbtu['8.9.0']['2.4.4'].dropna(how='all')\n",
    "\n",
    "# Keep only the custom tagged ones\n",
    "# site_kbtu_this_version = site_kbtu_this_version[[x for x in site_kbtu_this_version.columns if x in keep_only_runs]]\n",
    "\n",
    "# Filter on rows where the min is not the max\n",
    "site_kbtu_this_version = site_kbtu_this_version[site_kbtu_this_version.apply(lambda row: min(row) != max(row), axis=1)]\n",
    "\n",
    "# Make a multiindex \n",
    "site_kbtu_this_version.columns = pd.MultiIndex.from_tuples([x.split('_') for x in site_kbtu_this_version.columns],\n",
    "                                                 names=['Platform', 'Run'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these tests where we have variations, we can visualize the deviation each run Platform/run using a boxplox:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "site_kbtu_this_version.boxplot(ax=ax, grid=False)\n",
    "ax.set_title('Boxplot of tests that have variations, by platform and run')\n",
    "ax.set_ylabel('Total site kBTU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check biggest differences by looking at CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, I calculate the coefficient of variation ($CV$) for each test = standard deviation ($\\sigma$) divided by mean ($\\mu$)\n",
    "\n",
    "$$CV = \\frac{\\sigma}{\\mu}$$\n",
    "\n",
    "I then use a set tolerance to filter out tests that have a CV that isn't above or equal to the tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient of variation: standard deviation divided by mean\n",
    "cv = site_kbtu_this_version.std(axis=1) / site_kbtu_this_version.mean(axis=1)\n",
    "cv.name = 'Coefficient of Variation'\n",
    "cv = cv.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.sort_values(ascending=False).to_frame().style.format('{:.5%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tol = 0.00001\n",
    "print(\"Setting CV Tolerance to {:.3%}\".format(cv_tol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "sns.set_palette('Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_kbtu_this_version.loc[('centralheatpumpsystem', 'rb')].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_kbtu_this_version.loc[('centralheatpumpsystem', 'rb')].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = cv[cv >= cv_tol].sort_values(ascending=True).plot(kind='barh', figsize=(16,9))\n",
    "# Or Plot all\n",
    "\n",
    "toplot = cv.sort_values(ascending=True)\n",
    "toplot.index = [\"_\".join(x) for x in toplot.index]\n",
    "\n",
    "ax = toplot.plot(kind='barh', figsize=(16,9))\n",
    "vals = ax.get_xticks()\n",
    "ax.set_xticklabels(['{:3.4f}%'.format(x*100) for x in vals])\n",
    "ax.set_title('Coefficient of Variation for tests that are above cv_tol={:.3%}'.format(cv_tol))\n",
    "\n",
    "for i, rect in enumerate(ax.patches):\n",
    "    label = ax.get_yticklabels()[i].get_text() \n",
    "    val = toplot[i]\n",
    "    ax.annotate(\"{:.5%}\".format(val), \n",
    "                xy=(rect.get_x()+rect.get_width(), rect.get_y()+rect.get_height()/2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the same tests, we can visualize the total site kBTU for each:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "site_kbtu_this_version.reindex(index=cv[cv >= cv_tol].index).plot(kind='bar', ax=ax)\n",
    "ax.set_title('Total site kBTU for tests that are above the CV tolerance')\n",
    "ax.set_ylabel('Total Site kBTU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 40px; color:red;\">ANYTHING PAST THIS POINT NEEDS CLEANING</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Could Ruby test just be unstable regardless of platform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) First,  **the big differences are in the ruby tests mostly** (except 2.). I've mentionned already that I fixed a bunch of instabilities in the ruby tests, but there are some I couldn't fix yet: **could the ruby tests in question just be unstable regardless of platform?**\n",
    "\n",
    "I plot the entire heatmap (all OS version) of these tests which have a CV >= cv_tol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toplot = site_kbtu.reindex(index=cv[cv >= cv_tol].index)\n",
    "toplot = toplot[[x for x in toplot.columns if x[2] in ([''] + keep_only_runs)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_analysis.heatmap_sitekbtu_pct_change(site_kbtu=toplot,\n",
    "                            row_threshold=0.0000, display_threshold=0.0001, \n",
    "                            savefig=False, show_plot=True, figsize=(16,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following tests are unstable regardless of platform:\n",
    "    \n",
    "* airloop_and_zonehvac.rb\n",
    "* evaporative_cooling.rb\n",
    "* surface_properties.rb \n",
    "* unitary_system_performance_multispeed.rb (edited)\n",
    "\n",
    "The big unknown is **what the heck happened in Windows Run 1 for unitary_vav_bypass**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One OSM test produces different results on different platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "2) One very important exception to (1) above is the `evaporative_cooling.osm` test: **seems to be stable on both platform, but it doesn't have the same numbers on Ubuntu versus windows! Further investigation is warranted.**\n",
    "\n",
    "Note: You might say it's hard to tell if the OSM is stable on a given platform with two runs. I ran it 10 times on Ubuntu, and it is stable.\n",
    "\n",
    "    count    1.000000e+01\n",
    "    mean     7.632714e+06\n",
    "    std      9.817002e-10\n",
    "    min      7.632714e+06\n",
    "    25%      7.632714e+06\n",
    "    50%      7.632714e+06\n",
    "    75%      7.632714e+06\n",
    "    max      7.632714e+06\n",
    "    dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run N more times on a given machine to have more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"'/\"\n",
    "\n",
    "torun = (cv[cv >= cv_tol].index.tolist())\n",
    "\n",
    "for i, (test, ext) in enumerate(torun):\n",
    "    test_name = \"test_{}_{}\".format(test, ext)\n",
    "    #test_name = \"test_{}\".format(test)\n",
    "    #s += \" --name test_{}_{}\".format(test, ext)\n",
    "    if i < len(torun)-1:\n",
    "        s+='({})|'.format(test_name)\n",
    "    else:\n",
    "        s+='({})'.format(test_name)\n",
    "\n",
    "s += \"/'\"\n",
    "print(\"ruby model_tests.rb -n {}\".format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload with more runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = regression_analysis.find_info_osws_with_tags(compat_matrix=compat_matrix)\n",
    "subset_files = df_files[[x for x in df_files.columns\n",
    "                         if x[1] == '2.4.1' \n",
    "                         #and x[2] != 'Ubuntu_run2'\n",
    "                        ]]\n",
    "# Keep only those that I run more than twice\n",
    "subset_files = subset_files.loc[subset_files[('8.8.0', '2.4.1', 'Ubuntu_run3')].notnull()]\n",
    "\n",
    "# Parse site_kbtu\n",
    "site_kbtu = subset_files.applymap(regression_analysis.parse_total_site_energy)\n",
    "\n",
    "# Restrict to our version of interest, drop rows with all nan\n",
    "site_kbtu_241 = site_kbtu['8.8.0']['2.4.1'].dropna(how='all')\n",
    "\n",
    "# Keep only the custom tagged ones\n",
    "site_kbtu_241 = site_kbtu_241[[x for x in site_kbtu_241.columns if x != '']]\n",
    "\n",
    "# Make a multiindex \n",
    "site_kbtu_241.columns = pd.MultiIndex.from_tuples([x.split('_') for x in site_kbtu_241.columns],\n",
    "                                                 names=['Platform', 'Run'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_kbtu_241.groupby(level='Platform', axis=1).mean().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_from_pct_diff(toplot, display_threshold=0.001, \n",
    "                          title=None):\n",
    "    # Prepare two custom cmaps with one single color\n",
    "    grey_cmap = mpl.colors.ListedColormap('#f7f7f7')\n",
    "    green_cmap = mpl.colors.ListedColormap('#f0f7d9')\n",
    "\n",
    "\n",
    "    w = 16\n",
    "    h = w * toplot.shape[0] / (3 * toplot.shape[1])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(w, h))\n",
    "\n",
    "    # Reserve 1.5 inches at bottom for explanation\n",
    "    #fig.subplots_adjust(bottom=1.5/h)\n",
    "\n",
    "    # Same as: fmt = lambda x,pos: '{:.1%}'.format(x)\n",
    "    def fmt(x, pos): return '{:.1%}'.format(x)\n",
    "\n",
    "\n",
    "    # Plot with colors, for those that are above the display_threshold\n",
    "    sns.heatmap(toplot.abs(), mask=toplot.abs() <= display_threshold,\n",
    "                ax=ax, cmap='YlOrRd',  # cmap='Reds', 'RdYlGn_r'\n",
    "                vmin=0, vmax=0.5,\n",
    "                cbar_kws={'format': mpl.ticker.FuncFormatter(fmt)},\n",
    "                annot=toplot, fmt='.2%', linewidths=.5)\n",
    "\n",
    "    # Plot a second heatmap on top, only for those that are below\n",
    "    sns.heatmap(toplot, mask=((toplot.abs() > display_threshold) |\n",
    "                              (toplot.abs() == 0)),\n",
    "                cbar=False,\n",
    "                annot=True, fmt=\".4%\", annot_kws={\"style\": \"italic\"},\n",
    "                ax=ax, cmap=grey_cmap)\n",
    "\n",
    "    # Plot a third heatmap on top, only for those that are zero,\n",
    "    # no annot just green\n",
    "    sns.heatmap(toplot, mask=(toplot.abs() != 0),\n",
    "                cbar=False,  # linewidths=.5, linecolor='#cecccc',\n",
    "                annot=False,\n",
    "                ax=ax, cmap=green_cmap)\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % from the mean siteKBTU of the test\n",
    "toplot = ((site_kbtu_241.T - site_kbtu_241.T.mean())/(site_kbtu_241.T.mean())).T\n",
    "\n",
    "heatmap_from_pct_diff(toplot, title='Percentage difference from mean of test (both_versions)',\n",
    "                     display_threshold=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % from the mean siteKBTU of the test\n",
    "mean_ubuntu = site_kbtu_241['Ubuntu'].mean(axis=1)\n",
    "toplot = ((site_kbtu_241.T - mean_ubuntu)/(mean_ubuntu)).T\n",
    "\n",
    "heatmap_from_pct_diff(toplot, title='Percentage difference from mean of test for Ubuntu platform',\n",
    "                     display_threshold=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous helper scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = gb.glob('./test/plant_op_schemes_*')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at one cleaned out.osw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls test/*generator*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = regression_analysis.load_osw('test/generator_microturbine.rb_2.0.4_out.osw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename out.oswS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "os.mkdir('test/Windows')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "for f in gb.glob('test/*_2.4.1_out.osw'):\n",
    "    #outpath = '../model/simulationtests/'\n",
    "    #dst_path = os.path.join(outpath, f.replace('.rb_2.1.1', '') )\n",
    "    dst_path = f.replace('2.4.1_out', '2.4.1_out_Windows_run1').replace('test/', 'test/Windows/')\n",
    "    print(dst_path)\n",
    "    os.rename(f, dst_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for f in gb.glob('test/*2.4.1_out_Windows_run1.osw'):\n",
    "    dst_path  = f.replace('test/', 'test/Windows/')\n",
    "    os.rename(f, dst_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for f in gb.glob('test/Windows/*'):\n",
    "    os.rename(f, f.replace('run1', 'run2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zip custom tagged files for sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all tagged runs into a 'Tagged' directory for manual zipping\n",
    "os.mkdir('test/Tagged')\n",
    "for f in gb.glob('test/*out_*.osw'):\n",
    "    print(f)\n",
    "    dst_path  = f.replace('test/', 'test/Tagged/')\n",
    "    copyfile(f, dst_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip in one go...\n",
    "import zipfile\n",
    "import glob as gb\n",
    "with zipfile.ZipFile(\"Tagged.zip\", \"w\") as z:\n",
    "    for f in gb.glob('test/*out_*.osw'):\n",
    "        z.write(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify it worked\n",
    "z = zipfile.ZipFile(\"Tagged.zip\")\n",
    "z.printdir()\n",
    "z.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing SDD tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIR = os.path.join(ROOT_DIR, 'test')\n",
    "\n",
    "SDD_TEST_DIR = os.path.join(ROOT_DIR, 'testruns/SddReverseTranslator/')\n",
    "SDD_SIM_XML_DIR = os.path.join(ROOT_DIR, 'model/sddtests/')\n",
    "MODEL_TEST_DIR = os.path.join(ROOT_DIR, 'model/simulationtests/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escapeName(filename):\n",
    "    return (filename.replace('-','_')\n",
    "                    .replace('+','_')\n",
    "                    .replace(' ','_')\n",
    "                    .replace(\"=\",'_'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate tests for SDD RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tests = [os.path.relpath(x, SDD_SIM_XML_DIR) for x in gb.glob(os.path.join(SDD_SIM_XML_DIR, '*.xml'))]\n",
    "all_tests = sorted(all_tests)\n",
    "for t in all_tests:\n",
    "    filename = os.path.basename(t).replace(' - ap.xml', '')\n",
    "    print(\"  def test_RT_{}\".format(escapeName(filename)))\n",
    "    print(\"    sdd_rt_test('{}')\".format(t))\n",
    "    print(\"  end\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate tests for SDD FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_paths = gb.glob('model/simulationtests/*.osm')\n",
    "all_model_filenames = [ os.path.basename(p) for p in all_model_paths]\n",
    "all_model_filenames = sorted(all_model_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in all_model_filenames:\n",
    "    print(\"  def test_FT_{}\".format(escapeName(filename.replace('.osm', ''))))\n",
    "    print(\"    sdd_ft_test('{}')\".format(filename))\n",
    "    print(\"  end\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDD Reverse Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = regression_analysis.find_info_osws(compat_matrix=compat_matrix, testtype='sddrt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Or with custom tags\n",
    "df_files = regression_analysis.find_info_osws_with_tags(compat_matrix=compat_matrix,\n",
    "                                                        # Switch to True/False\n",
    "                                                        tags_only=False,\n",
    "                                                        testtype='sddrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataframe \n",
    "# If you get an error, make sure the df_files is up to date by reruning section 3.1 or 3.2 above\n",
    "success = regression_analysis.success_sheet(df_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = (success['n_fail+missing'] > 0)\n",
    "n_failed = filt.sum()\n",
    "print(\"There are {} test(s) that failed at least once\".format(n_failed))\n",
    "\n",
    "last_version = success.columns[-4]\n",
    "n_failed_last_version = (success.iloc[:, -4] != 'Success').sum()\n",
    "print(\"In the last version ran {}, {} tests failed\".format(last_version, n_failed_last_version))\n",
    "\n",
    "(success.loc[filt].style\n",
    "          .applymap(regression_analysis.background_colors)\n",
    "          .set_table_styles(regression_analysis.getStyles())\n",
    "          .set_caption(\"Test Success\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output command to rerun the tests that used to run in the previous version\n",
    "# and now don't\n",
    "torun = success[(success[('9.3.0', '3.0.1')] == 'Success') &\n",
    "                (success[('9.4.0', '3.1.0')] != 'Success')]\n",
    "\n",
    "# s = \"CUSTOMTAG=SHA /home/julien/Software/Others/OS-build/Products/openstudio-2.7.0 model_tests.rb -n '/\"\n",
    "s = \"openstudio SDD_tests.rb -n '/\"\n",
    "tests = []\n",
    "for i, (test, ext) in enumerate(torun.index.tolist()):\n",
    "    test_name = \"test_RT_{}\".format(test)\n",
    "    #test_name = \"test_{}\".format(test)\n",
    "    #s += \" --name test_{}_{}\".format(test, ext)\n",
    "    if i < len(torun)-1:\n",
    "        s+='({})|'.format(test_name)\n",
    "    else:\n",
    "        s+='({})'.format(test_name)\n",
    "    tests.append(test_name)\n",
    "\n",
    "s += \"/'\"\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_kbtu = df_files.applymap(regression_analysis.parse_total_site_energy)\n",
    "site_kbtu_change = site_kbtu.pct_change(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap > 1% change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_threshold = 1. / 100.0\n",
    "display_threshold = 0.1 / 100.0\n",
    "\n",
    "figname = 'site_kbtu_pct_change_row{}_display{}.png'.format(row_threshold, \n",
    "                                                            display_threshold)\n",
    "\n",
    "print(\"Row threshold = {:.2%}, Cell Display Threshold = {:.2%}\".format(row_threshold, display_threshold))\n",
    "\n",
    "regression_analysis.heatmap_sitekbtu_pct_change(site_kbtu=site_kbtu,\n",
    "                            row_threshold=row_threshold, display_threshold=display_threshold,\n",
    "                            savefig=True, figname=figname,\n",
    "                            show_plot=True, save_indiv_figs_for_ax=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap > 0.5% change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_threshold  = 0.05 / 100.0\n",
    "display_threshold=0.01 / 100.0\n",
    "\n",
    "figname = 'site_kbtu_pct_change_row{}_display{}.png'.format(row_threshold, \n",
    "                                                            display_threshold)\n",
    "print(\"Row threshold = {:.2%}, Cell Display Threshold = {:.2%}\".format(row_threshold, display_threshold))\n",
    "regression_analysis.heatmap_sitekbtu_pct_change(site_kbtu=site_kbtu,\n",
    "                            row_threshold=row_threshold, display_threshold=display_threshold,\n",
    "                            savefig=True, figname=figname,\n",
    "                            show_plot=True, save_indiv_figs_for_ax=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def find_sdd_rt_osms(compat_matrix=None, test_dir=None):\n",
    "    \"\"\"\n",
    "    Looks for files in the test/ folder, and parses version and type (rb, osm)\n",
    "    Constructs a dataframe that has E+/OS versions in column (by looking E+\n",
    "    version in compat_matrix)\n",
    "\n",
    "    IMPORTANT: this WILL NOT parse the custom-tagged out.osws\n",
    "    (see `find_info_osws_with_tags`)\n",
    "\n",
    "    Args:\n",
    "    ------\n",
    "    * compat_matrix (pd.DataFrame or None)\n",
    "        if None, calls parse_compatibility_matrix. Otherwise you can supply it\n",
    "    * test_dir (str path or None): if None uses the global TEST_DIR constant\n",
    "    Returns:\n",
    "    ---------\n",
    "    * df_files (pd.DataFrame): A multi indexed dataframe in rows and columns\n",
    "        Levels are as follows:\n",
    "        index: ['Test', 'Type'], eg ('absorption_chiller', 'rb')\n",
    "        columns: ['E+', 'OS'], the versions eg ('8.6.0', '2.4.0')\n",
    "\n",
    "        values are the path of the corresponding out.osw,\n",
    "        eg: 'absorption_chillers.rb_2.0.4_out.osw'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if test_dir is None:\n",
    "        test_dir = TEST_DIR\n",
    "\n",
    "    if compat_matrix is None:\n",
    "        compat_matrix = parse_compatibility_matrix()\n",
    "\n",
    "    files = gb.glob(os.path.join(test_dir, '*out.osm'))\n",
    "\n",
    "    df_files = pd.DataFrame(files, columns=['path'])\n",
    "    # With this pattern, we exclude the custom-tagged out.osw files\n",
    "    filepattern = (r'(?P<Test>.*?)_'\n",
    "                   r'(?P<version>\\d+\\.\\d+\\.\\d+)_out\\.osm')\n",
    "    version = (df_files['path'].apply(lambda p: os.path.relpath(p, test_dir))\n",
    "                               .str.extract(pat=filepattern, expand=True))\n",
    "    df_files = pd.concat([df_files,\n",
    "                          version],\n",
    "                         axis=1)\n",
    "    df_files = (df_files.set_index(['Test',\n",
    "                                    'version'])['path'].unstack(['version'])\n",
    "                        .sort_index(axis=1))\n",
    "\n",
    "    version_dict = compat_matrix.set_index('OpenStudio')['E+'].to_dict()\n",
    "\n",
    "    # Handle the case where you're working on a develop branch that is ahead\n",
    "    # of the compatibility matrix\n",
    "    all_versions = df_files.columns.unique()\n",
    "    unknown_versions = set(all_versions) - set(version_dict.keys())\n",
    "\n",
    "    latest_eplus = compat_matrix.iloc[0]['E+']\n",
    "\n",
    "    if unknown_versions:\n",
    "        msg = (\"OpenStudio Version {} is not in the compatibility matrix\\n\"\n",
    "               \"Please input the corresponding E+ version (default='{}'):\\n\")\n",
    "        for v in unknown_versions:\n",
    "            # Skip the ones we hard mapped\n",
    "            if v in OS_EPLUS_DICT.keys():\n",
    "                is_correct = True\n",
    "                eplus = OS_EPLUS_DICT[v]\n",
    "            else:\n",
    "                is_correct = False\n",
    "\n",
    "            while not is_correct:\n",
    "                # Ask user. If blank, then default to latest eplus known\n",
    "                eplus = input(msg.format(v, latest_eplus))\n",
    "                if not eplus:\n",
    "                    eplus = latest_eplus\n",
    "\n",
    "                # Sanitize: it should be in the form \"X.Y.Z\"\n",
    "                if len(eplus.split('.')) == 3:\n",
    "                    try:\n",
    "                        [float(x) for x in eplus.split('.')]\n",
    "                        is_correct = True\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            print(\"Mapping OS '{}' to '{}'\".format(v, eplus))\n",
    "            # Add to the version_dict\n",
    "            version_dict[v] = eplus\n",
    "\n",
    "    # Prepend a column level for E+ version\n",
    "    df_files.columns = pd.MultiIndex.from_tuples([(version_dict[x], x)\n",
    "                                                  for x in df_files.columns],\n",
    "                                                 names=['E+', 'OS'])\n",
    "\n",
    "    return df_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDD Forward Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "reload(regression_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rest assured that this will find the XMLs (not OSWs)\n",
    "df_files = regression_analysis.find_info_osws(compat_matrix=compat_matrix, testtype='sddft')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Or with custom tags\n",
    "df_files = regression_analysis.find_info_osws_with_tags(compat_matrix=compat_matrix,\n",
    "                                                        # Switch to True/False\n",
    "                                                        tags_only=False,\n",
    "                                                        testtype='sddft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "success = regression_analysis.success_sheet_sddft(df_files)\n",
    "\n",
    "filt = (success['n_fail'] > 0)\n",
    "n_failed = filt.sum()\n",
    "print(\"There are {} test(s) that failed\".format(n_failed))\n",
    "\n",
    "last_version = success.columns[-3]\n",
    "n_failed_last_version = (success.iloc[:, -3] != 'Success').sum()\n",
    "print(\"In the last version ran {}, {} tests failed\".format(last_version, n_failed_last_version))\n",
    "\n",
    "(success.loc[filt].style\n",
    "         .applymap(regression_analysis.background_colors)\n",
    "         .set_table_styles(regression_analysis.getStyles())\n",
    "         .set_caption(\"SDD FT: Test Success\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = regression_analysis.diff_all_xmls(df_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff.style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "reload(regression_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rest assured that this will find the .sqltest files (not OSWs)\n",
    "df_files = regression_analysis.find_info_osws(compat_matrix=compat_matrix, testtype='sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = regression_analysis.success_sheet_sql(df_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = (success['n_fail'] > 0)\n",
    "n_failed = filt.sum()\n",
    "print(\"There are {} test(s) that failed at least once\".format(n_failed))\n",
    "\n",
    "last_version = success.columns[-2]\n",
    "n_failed_last_version = (success.iloc[:, -2] == 'Fail').sum()\n",
    "print(\"In the last version ran {}, {} tests failed\".format(last_version, n_failed_last_version))\n",
    "\n",
    "(success\n",
    "# .loc[filt]\n",
    " .style\n",
    " .applymap(regression_analysis.background_colors)\n",
    " .set_table_styles(regression_analysis.getStyles())\n",
    " .set_caption(\"sql_tests: Test Success\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "reload(regression_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rest assured that this will find the .status files (not OSWs)\n",
    "df_files = regression_analysis.find_info_osws(compat_matrix=compat_matrix, testtype='utilities')\n",
    "\n",
    "success = regression_analysis.success_sheet_utilities(df_files)\n",
    "\n",
    "\n",
    "filt = (success['n_fail'] > 0)\n",
    "n_failed = filt.sum()\n",
    "print(\"There are {} test(s) that failed at least once\".format(n_failed))\n",
    "\n",
    "last_version = success.columns[-2]\n",
    "n_failed_last_version = (success.iloc[:, -2] == 'Fail').sum()\n",
    "print(\"In the last version ran {}, {} tests failed\".format(last_version, n_failed_last_version))\n",
    "\n",
    "(success\n",
    "# .loc[filt]\n",
    " .style\n",
    " .applymap(regression_analysis.background_colors)\n",
    " .set_table_styles(regression_analysis.getStyles())\n",
    " .set_caption(\"utilities_tests: Test Success\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_analysis.encoding_sheet_utilities(df_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "743px",
    "left": "348px",
    "right": "1171px",
    "top": "111.133px",
    "width": "450px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
