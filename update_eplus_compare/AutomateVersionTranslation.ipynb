{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 48px; text-align: center; color:#009dff;\">\n",
    "              Transition from E+ 24.1.0 to E+ 24.2.0</p>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<p style='font-size: 24px; text-align: center;'>Foreword</p>\n",
    "\n",
    "<div style='font-size:16px;'><strong>This notebook has two big parts:</strong>\n",
    "\n",
    "<ul>\n",
    "  <li><strong>Part 1:</strong> aims to transition all regression tests from one E+ version to the next, and allows you to run each test in both the old and the new version\n",
    "    <ul>\n",
    "      <li>These will take quite some time to run (about 1hr to run the tests in the OLD OpenStudio version, transition the IDFs to the new E+ version and run them, and run the tests in the NEW OpenStudio Version, based on almost 200 files currently)</li>\n",
    "      <li>By default it will just copy over the SQL from the regression test to place in OLD_DIR, but if you want to force rerun the IDF in the old E+ version you can.</li>\n",
    "      <li>At the end of Part 1, you will have three CSV files, one per version, with the site KBTUs for each test. And you also have an organized tree of VERSION/TEST_NAME/ output directories that have the SQL files we will use for sections 6+.</li>\n",
    "    </ul>  \n",
    "  </li>\n",
    "  <li><strong>Part 2:</strong> aims to analyze the differences between versions\n",
    "    <ul>\n",
    "      <li>Section 3.1 just re-queries all SQL file (or you can reload the three CSV files) to highlight the tests with the biggest site KBTU differences</li>\n",
    "      <li>Section 3.2 provides a high-level interface that only requires to pass a test name and it will query the relevant SQL files and produce visualization (tables, grouped bar charts, and heatmaps) to analyze where differences may be coming from</li>\n",
    "    </ul>\n",
    "    If you have already run Part 1 successfully once, you only need to run Section 1. and you can jump to Part 2 directly.\n",
    "  </li>\n",
    "</ul>\n",
    "     \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**Note**\n",
    "\n",
    "* It might be a good idea to monitor your system after each big tasks to ensure you don't have processes that are still hanging. It happened to me for intersection test for eg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 2.x / 3.x compatibility\n",
    "from __future__ import division, print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import tempfile\n",
    "\n",
    "#import csv\n",
    "import glob as gb\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import datetime\n",
    "import sqlite3\n",
    "\n",
    "import shutil\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "\n",
    "#import tqdm\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from ipywidgets import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "import shlex\n",
    "\n",
    "# from df2gspread import df2gspread as d2g\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (16, 9)\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args\n",
    "\n",
    "These should match your actual installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF_DIR = Path('.').absolute()\n",
    "IDF_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the OpenStudio-resources/testruns/ directory\n",
    "TESTRUNS_DIR = Path('../testruns').resolve()\n",
    "OS_RES_DIR = Path('..').resolve()\n",
    "\n",
    "EPLUS_OLD_VERSION = '24.1.0'\n",
    "OS_OLD_VERSION = '3.8.0'\n",
    "\n",
    "EPLUS_NEW_VERSION = '24.2.0'\n",
    "OS_NEW_VERSION = '3.9.0' # This is really an alpha, with 24.2.0 upgrade...\n",
    "\n",
    "# Careful: We'll cd into the TRANSITION_CLI_DIR to run it, so it must be a writtable location...\n",
    "TRANSITION_CLI_DIR = Path('~/Software/Others/EnergyPlus-build-release-develop/Products').expanduser()\n",
    "# TRANSITION_CLI_DIR = Path('~/Software/Others/OS-build-release/EnergyPlus-24.1.0-9d7789a3ac-Linux-Ubuntu20.04-x86_64/PreProcess/IDFVersionUpdater').expanduser()\n",
    "\n",
    "# Force a given number of parallel process \n",
    "# (defaults to nproc - 2, leaving one physical core free if you have hyperthreading)\n",
    "N = None\n",
    "WEATHER_FILE= (IDF_DIR / '../weatherdata/USA_IL_Chicago-OHare.Intl.AP.725300_TMY3.epw').resolve()\n",
    "\n",
    "# Path to EnergyPlus application & idd \n",
    "OLD_EPLUS_EXE = Path('/usr/local/EnergyPlus-24-1-0/energyplus')\n",
    "\n",
    "# NEW_EPLUS_EXE = os.path.join(TRANSITION_CLI_DIR, 'energyplus-9.2.0')\n",
    "NEW_EPLUS_EXE = Path('/usr/local/EnergyPlus-24-2-0/energyplus')\n",
    "# NEW_EPLUS_EXE = '/home/julien/Software/Others/EnergyPlus-build-release-develop/Products/energyplus'\n",
    "# NEW_EPLUS_EXE = '/home/julien/Software/Others/OS-build2/EnergyPlus-8.9.0-1c5ba897d1-Linux-x86_64/EnergyPlus-8-9-0/energyplus-8.9.0'\n",
    "# NEW_EPLUS_EXE = '/home/julien/Downloads/Temp/EnergyPlus-23.1.0-ff86a13c18-Linux-Ubuntu20.04-x86_64/energyplus'\n",
    "\n",
    "# Path to OpenStudio CLIs\n",
    "OLD_OS_CLI = Path('/usr/local/openstudio-3.8.0/bin/openstudio')\n",
    "NEW_OS_CLI = Path('~/Software/Others/OS-build-release/Products/openstudio').expanduser()\n",
    "# OLD_IDD_FILE = '/usr/local/EnergyPlus-8-8-0/Energy+.idd'\n",
    "# NEW_IDD_FILE = os.path.join(TRANSITION_DIR, 'Energy+.idd')\n",
    "\n",
    "# Put None if you want to run all tests\n",
    "REGRESSION_TEST_FILTER = '(kiva)|(baseline)'\n",
    "REGRESSION_TEST_FILTER = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert TRANSITION_CLI_DIR.is_dir()\n",
    "assert WEATHER_FILE.is_file()\n",
    "\n",
    "assert OLD_EPLUS_EXE.exists()\n",
    "assert OLD_EPLUS_EXE.exists()\n",
    "assert OLD_OS_CLI.exists()\n",
    "assert NEW_OS_CLI.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_old_v_dashed = EPLUS_OLD_VERSION.replace('.', '-')\n",
    "ep_new_v_dashed = EPLUS_NEW_VERSION.replace('.', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the translation to work,\n",
    "# you'll have to chdir to the Transition CLI's folder\n",
    "TRANSITION_CLI = TRANSITION_CLI_DIR / f\"Transition-V{ep_old_v_dashed}-to-V{ep_new_v_dashed}\"\n",
    "assert TRANSITION_CLI.is_file()\n",
    "\n",
    "OLD_IDD_FILE = TRANSITION_CLI_DIR / f\"V{ep_old_v_dashed}-Energy+.idd\"\n",
    "NEW_IDD_FILE = TRANSITION_CLI_DIR / f\"V{ep_new_v_dashed}-Energy+.idd\"\n",
    "assert OLD_IDD_FILE.is_file()\n",
    "assert NEW_IDD_FILE.is_file()\n",
    "\n",
    "TRANSITION_REPORT_VAR_PATH = TRANSITION_CLI_DIR / f\"Report Variables {ep_old_v_dashed} to {ep_new_v_dashed}.csv\"\n",
    "if TRANSITION_REPORT_VAR_PATH.is_file():\n",
    "    print(f\"Found {TRANSITION_REPORT_VAR_PATH}\")\n",
    "\n",
    "# TODO: Temp override\n",
    "# TRANSITION_CLI = Path('/home/julien/Software/Others/EnergyPlus-build-release-develop/Products/Transition-V9-5-0-to-V9-6-0')\n",
    "TRANSITION_CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import psutil\n",
    "    physical_cpus = psutil.cpu_count(logical=False)\n",
    "    multiprocessing.cpu_count() * (physical_cpus - 1) / physical_cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parallel processes\n",
    "if not N:\n",
    "    N = multiprocessing.cpu_count() - 2\n",
    "    print(\"Defaulting number of processes to {}\".format(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "OLD_OS_DIR = IDF_DIR / f\"{OS_OLD_VERSION}-{EPLUS_OLD_VERSION}\"\n",
    "\n",
    "NEW_OS_DIR = IDF_DIR / f\"{OS_NEW_VERSION}-{EPLUS_NEW_VERSION}\"\n",
    "\n",
    "TRANSITION_DIR = IDF_DIR / f\"Transition-{EPLUS_NEW_VERSION}\"\n",
    "\n",
    "for p in [OLD_OS_DIR, NEW_OS_DIR, TRANSITION_DIR]:\n",
    "    if not p.is_dir():\n",
    "        p.mkdir(parents=True)\n",
    "        print(f\"Creating directory: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dicts to store all info\n",
    "OLD_OS_INFO = {'OS_VERSION': OS_OLD_VERSION,\n",
    "               'EPLUS_VERSION': EPLUS_OLD_VERSION,\n",
    "               'DIR': OLD_OS_DIR}\n",
    "\n",
    "NEW_OS_INFO = {'OS_VERSION': OS_NEW_VERSION,\n",
    "               'EPLUS_VERSION': EPLUS_NEW_VERSION,\n",
    "               'DIR': NEW_OS_DIR}\n",
    "\n",
    "TRANSITION_INFO = {'OS_VERSION': 'Transition',\n",
    "                   'EPLUS_VERSION': EPLUS_NEW_VERSION,\n",
    "                   'DIR': TRANSITION_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify_cmd(cmd) -> list[str]:\n",
    "    \"\"\"Takes a list of arguments that may be Path or str or int and turns into a List[str].\n",
    "    So that is it suitable to pass to subprocess.\n",
    "    \"\"\"\n",
    "    return list(map(str, cmd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: Transition all regression tests and run them in both E+ versions\n",
    "\n",
    "**Part 1 is ommited in the HTML because the interesting part is the output and analysis at the end**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run in Previous OpenStudio Version based on old E+\n",
    "\n",
    "This will go in the `TESTRUNS_DIR ` (`OpenStudio-resources/testruns`) directory and find all IDF files and copy them to the `IDF_DIR` directory (typically the directory in which this notebook resides)\n",
    "\n",
    "<span style=\"font-size: 18px; color: red;\">It goes without saying: you need to have already run all simulation tests with the last OpenStudio version that is based on the old E+ version before running this section.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the tests in the old version\n",
    "\n",
    "You can also just do that manually... but if you do, please delete the testruns/ folder beforehand just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all testruns to ensure we don't end up grabbing the idf and sql from another version\n",
    "if TESTRUNS_DIR.is_dir():\n",
    "    shutil.rmtree(TESTRUNS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pass 'CUSTOMTAG=' if you don't want a tag, or 'CUSTOMTAG=sha' for the build sha,\n",
    "# or any custom string such as 'CUSTOMTAG=Ubuntu_run1'\n",
    "CUSTOMTAG = 'SHA'\n",
    "CUSTOMTAG = ''\n",
    "\n",
    "if REGRESSION_TEST_FILTER is None:\n",
    "    filt = ''\n",
    "else:\n",
    "    filt = \"-n /{}/\".format(REGRESSION_TEST_FILTER)\n",
    "\n",
    "command = \"env CUSTOMTAG={c} USE_EPLUS_SPACES=true {cli} {m} {filt}\".format(c=CUSTOMTAG,\n",
    "                                                      m=os.path.join(OS_RES_DIR,\n",
    "                                                                     'model_tests.rb'),\n",
    "                                                      cli=OLD_OS_CLI,\n",
    "                                                      filt=filt)\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "c_args = shlex.split(command)\n",
    "\n",
    "# Run it\n",
    "process = subprocess.Popen(c_args, shell=False,\n",
    "                           stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "# wait for the process to terminate\n",
    "#out, err = process.communicate()\n",
    "#errcode = process.returncode\n",
    "for line in iter(process.stdout.readline, b''):\n",
    "    l = line.rstrip().decode()\n",
    "    if 'extensions are not built' in l:\n",
    "        continue\n",
    "    print(l)\n",
    "process.stdout.close()\n",
    "process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the Previous IDFs\n",
    "\n",
    "These end up directly in IDF_DIR. They will get copied to the `OLD_OS_DIR` during the Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = len([x for x in gb.iglob(os.path.join(TESTRUNS_DIR, '**/*/in.idf'))])\n",
    "x_n = !grep -c \"^ *def test_\" ../model_tests.rb\n",
    "x_n = int(x_n[0])\n",
    "\"{}/{} {:.2%}\".format(x, x_n, x/x_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup directory\n",
    "all_files = gb.glob(os.path.join(IDF_DIR, '*.idf'))\n",
    "all_files += gb.glob(os.path.join(IDF_DIR, '*.idfnew'))\n",
    "all_files += gb.glob(os.path.join(IDF_DIR, '*.idfold'))\n",
    "all_files += gb.glob(os.path.join(IDF_DIR, '*.VCpErr'))\n",
    "\n",
    "for f in all_files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "found_idfs = []\n",
    "for f in TESTRUNS_DIR.glob('**/*/in.idf'):\n",
    "    f2 = f.relative_to(TESTRUNS_DIR)\n",
    "    \n",
    "    test_name = f2.parts[0]\n",
    "    #print(test_name)\n",
    "    dst_path = IDF_DIR / f\"{test_name}.idf\"\n",
    "    shutil.copyfile(f, dst_path)\n",
    "    found_idfs.append(test_name)\n",
    "found_idfs = set(found_idfs)\n",
    "len(found_idfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy all existing SQL files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in TESTRUNS_DIR.glob(\"**/*/*.sql\"):\n",
    "    f2 = f.relative_to(TESTRUNS_DIR)\n",
    "    \n",
    "    test_name = f2.parts[0]\n",
    "    # print(test_name)\n",
    "    dst_folder = OLD_OS_DIR / test_name\n",
    "    if not dst_folder.is_dir():\n",
    "        dst_folder.mkdir(parents=False, exist_ok=False)\n",
    "        \n",
    "    dst_path = dst_folder / \"eplusout.sql\"\n",
    "    # print(dst_path)\n",
    "    shutil.copyfile(f, dst_path)\n",
    "    found_sqls.append(test_name)\n",
    "found_sqls = set(found_sqls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(found_idfs), len(found_sqls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_idfs - found_sqls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Rename directories and files\n",
    "for fn in os.listdir(NEW_DIR):\n",
    "    os.rename(fn, fn.replace('_8.9.0', ''))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_temp_dir_for_transition() -> Path:\n",
    "    temp_dir = Path(tempfile.mkdtemp(prefix='transition-'))\n",
    "    shutil.copy(OLD_IDD_FILE, temp_dir)\n",
    "    shutil.copy(NEW_IDD_FILE, temp_dir)\n",
    "    if TRANSITION_REPORT_VAR_PATH.is_file():\n",
    "        shutil.copy(TRANSITION_REPORT_VAR_PATH, temp_dir)\n",
    "    return temp_dir\n",
    "\n",
    "def translate_file_parallelizable(idf_path: Path):\n",
    "    assert idf_path.is_file(), f\"Could not find {idf_path}\"\n",
    "    \n",
    "    temp_dir = _prepare_temp_dir_for_transition()\n",
    "    cmd = stringify_cmd([TRANSITION_CLI, idf_path])\n",
    "    r = subprocess.run(\n",
    "        cmd,\n",
    "        capture_output=True,\n",
    "        shell=False,\n",
    "        encoding='utf-8',\n",
    "        cwd=temp_dir,\n",
    "    )\n",
    "    \n",
    "    if r.returncode == 0:        \n",
    "        # Move the resulting IDF into the new dir\n",
    "        new_file = idf_path.with_suffix('.idfnew')\n",
    "        assert new_file.is_file(), f\"Could not find {new_file}\"\n",
    "        new_dest = TRANSITION_DIR / idf_path.name\n",
    "        shutil.move(new_file, new_dest)\n",
    "\n",
    "        # Move the old version into its directory\n",
    "        old_file = idf_path.with_suffix('.idfold')\n",
    "        assert old_file.is_file(), f\"Could not find {old_file}\"\n",
    "        old_dest = OLD_OS_DIR / idf_path.name\n",
    "        shutil.move(old_file, old_dest)\n",
    "\n",
    "        # Delete original file\n",
    "        idf_path.unlink()\n",
    "\n",
    "        # print('Done for {}.idf - {}'.format(eplus_file, path))\n",
    "    else:\n",
    "        print(f\"Error for {idf_path}:\")\n",
    "        print(r.stdout)\n",
    "        print(r.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_file_regular_serially(path):\n",
    "    \"\"\"\n",
    "    Runs the file throught the transition utility and save in the right folder\n",
    "    Will move the ori file to the subdirectory OLD_DIR (eg `./8.8.0/`)\n",
    "    and the transitionned one to NEW_DIR (eg: `./8.9.0/`)\n",
    "    \"\"\"\n",
    "    \n",
    "    eplus_file, ext = os.path.splitext(os.path.split(path)[1])\n",
    "    \n",
    "    cmd = stringify_cmd([TRANSITION_CLI, path])\n",
    "    process = subprocess.Popen(cmd,\n",
    "                               shell=False,\n",
    "                               stdout=subprocess.PIPE,\n",
    "                               stderr=subprocess.PIPE)\n",
    "    # wait for the process to terminate\n",
    "    out, err = process.communicate()\n",
    "    errcode = process.returncode\n",
    "    if errcode == 0:        \n",
    "        # Move the resulting IDF into the new dir\n",
    "        new_file = os.path.join(IDF_DIR, \"{f}.idfnew\".format(f=eplus_file))\n",
    "        new_dest = os.path.join(TRANSITION_DIR, \"{f}.idf\".format(f=eplus_file))\n",
    "        shutil.move(new_file, new_dest)\n",
    "        \n",
    "        # Move the old version into its directory\n",
    "        old_file = os.path.join(IDF_DIR, \"{f}.idfold\".format(f=eplus_file))\n",
    "        old_dest = os.path.join(OLD_OS_DIR, \"{f}.idf\".format(f=eplus_file))\n",
    "        shutil.move(old_file, old_dest)\n",
    "        \n",
    "        # Delete original file\n",
    "        ori_file = os.path.join(IDF_DIR, \"{f}.idf\".format(f=eplus_file))\n",
    "        os.remove(ori_file)\n",
    "        \n",
    "        # print('Done for {}.idf - {}'.format(eplus_file, path))\n",
    "    else:\n",
    "        print(\"Error for {}\".format(path))\n",
    "        print(out)\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(IDF_DIR.glob('*.idf'))\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in problems.index[problems[next(x for x in problems.columns if x[1] == 'Transition')].isna()]:\n",
    "#     shutil.copy(f\"{OLD_OS_DIR}/{x}.idf\", f\"{IDF_DIR}/{x}.idf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transitionning in parallel seems to fail, so do it serially...**\n",
    "\n",
    "**Edit**: I made it work by using a temporary directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # Takes about 10minutes on my machine with 12 threads allocated\n",
    "    pool = multiprocessing.Pool(processes=N)\n",
    "\n",
    "    desc = f'<h3>Translation from {EPLUS_OLD_VERSION} to {EPLUS_NEW_VERSION}</h3>'\n",
    "    label = HTML(desc)\n",
    "    display(label)\n",
    "    for _ in tqdm(pool.imap_unordered(translate_file_parallelizable, files), total=len(files)):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    os.chdir(TRANSITION_CLI_DIR)\n",
    "\n",
    "    desc = '<h3>Translation from {} to {}</h3>'.format(EPLUS_OLD_VERSION,\n",
    "                                                       EPLUS_NEW_VERSION)\n",
    "    label = HTML(desc)\n",
    "    for file in tqdm(files):\n",
    "        translate_file_regular_serially(file)\n",
    "\n",
    "    os.chdir(IDF_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, you shouldn't have any .idf files in the IDF_DIR directory\n",
    "# If you do, means that the transition failed\n",
    "all_files = []\n",
    "for e in ['idf', 'idfnew', 'idfold']:\n",
    "    all_files += list(IDF_DIR.glob(f'*.{e}'))\n",
    "\n",
    "all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze VCpErr files\n",
    "\n",
    "For 8.8.0 to 8.9.0 I had very few VCpErr files, but 8.9.0 to 9.9.0 I have a bunch.\n",
    "A preliminary look shows that it could be just related to the renaming/replacing of `AirTerminal:SingleDuct:Uncontrolled` to `AirTerminal:SingleDuct:ConstantVolume:NoReheat`\n",
    "but I'd like to make sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_files = gb.glob('*.VCpErr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(err_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_err_lines = []\n",
    "all_dfs = []\n",
    "for err_file in err_files:\n",
    "    with open(err_file, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "        all_err_lines += lines\n",
    "        all_dfs.append(pd.DataFrame([[err_file.replace('.VCpErr', '')]*len(lines), lines],\n",
    "             index=['File', 'Line']).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_err = pd.concat(all_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip out Line numbers\n",
    "df_err['Line'] = df_err['Line'].str.replace('line~\\d+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude common messages that aren't actually informative\n",
    "common_things_to_exclude = [\n",
    "    'Conversion Completed Successfully',\n",
    "    'Program Version,Conversion',\n",
    "    'entered with less than minimum number of fields'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_err = df_err[~df_err['Line'].apply(lambda x: any(thing in x for thing in common_things_to_exclude))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so indeed, most of the warnings come are due to ATU Single Duct Uncontrolled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_err['Line'].value_counts().to_frame().style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Strip out the ATU warning\n",
    "df_err = df_err[~df_err['Line'].str.contains('AirTerminal:SingleDuct')]\n",
    "len(df_err)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most problematic files\n",
    "df_err.groupby('File').count().sort_values(by='Line', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of the warnings are like this one:\n",
    "   > ** Severe ** Out of range value Numeric Field#10 (Low Speed Standard Design Capacity), value=0.00000, range={>0.0}, in EvaporativeFluidCooler:TwoSpeed=Evaporative Fluid Cooler Two Speed 3\n",
    "   \n",
    "In fact, the value for that specific field is `Autosize`\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_err.groupby('File')['Line'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real problems are potentially\n",
    "* asymmetric_interior_constructions.osm: Really this seems only about names that are way too long\n",
    "* multiple_airloops.rb\n",
    "* multiple_airloops.osm\n",
    "* multiple_loops_w_plenums.rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_err.set_index('File').loc[[\n",
    "    #'asymmetric_interior_constructions.osm',\n",
    "    'multiple_airloops.rb',\n",
    "    'multiple_airloops.osm',\n",
    "    'multiple_loops_w_plenums.rb']].reset_index().style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Simulation in E+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GJ_TO_KBTU= 947.8171203133173\n",
    "\n",
    "SQL_QUERY_TOTAL_SITE_KBTU = \"SELECT Value FROM tabulardatawithstrings WHERE \\\n",
    "                              ReportName='AnnualBuildingUtilityPerformanceSummary' AND \\\n",
    "                              ReportForString='Entire Facility' AND \\\n",
    "                              TableName='Site and Source Energy' AND \\\n",
    "                              RowName='Total Site Energy' AND \\\n",
    "                              ColumnName='Total Energy' AND \\\n",
    "                              Units='GJ'\"\n",
    "\n",
    "SQL_QUERY_SIM_INFO = 'SELECT EnergyPlusVersion FROM Simulations'\n",
    "\n",
    "VERSION_REGEX = re.compile(r'Version (?P<Major>\\d+)\\.(?P<Minor>\\d+)\\.'\n",
    "                           r'(?P<Patch>\\d+)-(?P<SHA>\\w+),\\s+'\n",
    "                           r'YMD=(?P<datestring>[0-9\\.: ]+)')\n",
    "\n",
    "\n",
    "# Remove all files in the output directory except these\n",
    "KEEP_EXT = ['.err', '.sql']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sql_version_and_sitekbtu(output_directory: Path) -> Optional[pd.Series]:\n",
    "    \"\"\"\n",
    "    This function grabs the EnergyPlusVersion and the total site energy\n",
    "    from the SQL file.\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    * output_directory (str): the path were the SQL should be.\n",
    "        eg: `./8.8.0/absorption_chillers.osm_8.8.0/`\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    * pd.Series that has the version with SHA and site kbtu\n",
    "        (or None if it didn't run), which name is the test_name\n",
    "        (gotten from the name of the output_directory)\n",
    "    \"\"\"\n",
    "    sql_files = list(Path(output_directory).absolute().glob(\"*.sql\"))\n",
    "\n",
    "    version_with_sha = None\n",
    "    site_kbtu = None\n",
    "\n",
    "    if len(sql_files) == 1:\n",
    "        sql_path = sql_files[0]\n",
    "        sql_uri = f\"{sql_path.as_uri()}?mode=ro\"\n",
    "        with sqlite3.connect(sql_uri, uri=True) as con:\n",
    "                cursor = con.cursor()\n",
    "                r = cursor.execute(SQL_QUERY_SIM_INFO).fetchone()\n",
    "                if r:\n",
    "                    simulation_info = r[0]\n",
    "                    m = VERSION_REGEX.search(simulation_info)\n",
    "                    if m:\n",
    "                        gpdict = m.groupdict()\n",
    "                        version_with_sha = \"{}.{}.{}-{}\".format(gpdict['Major'],\n",
    "                                                                gpdict['Minor'],\n",
    "                                                                gpdict['Patch'],\n",
    "                                                                gpdict['SHA'])\n",
    "                else:\n",
    "                    msg = (\"Cannot find the EnergyPlusVersion in the SQL file. \"\n",
    "                           \"For:\\n{}\".format(output_directory))\n",
    "                    #raise ValueError(msg)\n",
    "                    print(msg)\n",
    "\n",
    "                # Get Site kBTU\n",
    "                r = cursor.execute(SQL_QUERY_TOTAL_SITE_KBTU).fetchone()\n",
    "                if r:\n",
    "                    site_gj = float(r[0])\n",
    "                    site_kbtu = site_gj * GJ_TO_KBTU\n",
    "                    msg = (\"Cannot find the Total Site Energy in the SQL file. \"\n",
    "                           \"For:\\n{}\".format(output_directory))\n",
    "    return pd.Series([version_with_sha, site_kbtu],\n",
    "                     index=['E+', 'SiteKBTU'],\n",
    "                     name = os.path.split(output_directory)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eplus_sim(\n",
    "    eplus_file: Path, ep_cli: Path,\n",
    "    expand_objects: bool = False,\n",
    "    verbose: bool = False\n",
    ") -> pd.Series:\n",
    "    eplus_file = Path(eplus_file).absolute()\n",
    "    assert eplus_file.is_file(), f\"Could not find {eplus_file}\"\n",
    "    \n",
    "    output_directory = eplus_file.parent / eplus_file.stem\n",
    "    # If directory exists, delete it\n",
    "    if output_directory.is_dir():\n",
    "        shutil.rmtree(output_directory)\n",
    "        \n",
    "    # Recreate it\n",
    "    output_directory.mkdir(parents=True, exist_ok=False)\n",
    "    \n",
    "    cmd = [\n",
    "        ep_cli\n",
    "    ]\n",
    "    if expand_objects:\n",
    "        cmd.append('-x')\n",
    "    cmd += [\n",
    "        # '-i', OLD_IDD_FILE,\n",
    "        '-w', WEATHER_FILE,\n",
    "        '-d', output_directory,\n",
    "       eplus_file\n",
    "    ]\n",
    "    cmd = stringify_cmd(cmd)\n",
    "    if verbose:\n",
    "        print(\" \".join(cmd))\n",
    "    r = subprocess.run(\n",
    "        cmd,\n",
    "        capture_output=True,\n",
    "        shell=False,\n",
    "        encoding='utf-8',\n",
    "    )\n",
    "    \n",
    "    if r.returncode == 0:         \n",
    "        # Clean up output directory\n",
    "        [\n",
    "            x.unlink() for x in output_directory.glob('*') \n",
    "            if x.suffix not in KEEP_EXT\n",
    "        ]\n",
    "        return parse_sql_version_and_sitekbtu(output_directory)\n",
    "    else:\n",
    "        print(f\"ERROR with code {r.returncode}: {eplus_file}\")\n",
    "        # print(r.stdout)\n",
    "        # print(r.stderr)\n",
    "        return pd.Series([None, None],\n",
    "                     index=['E+', 'SiteKBTU'],\n",
    "                     name = os.path.split(output_directory)[1])\n",
    "\n",
    "def run_OLD_eplus_sim(eplus_file: Path, expand_objects: bool = False, verbose: bool = False):\n",
    "    \"\"\"\n",
    "    Runs the simulation with OLD_EPLUS_EXE and calls parse_sql\n",
    "    \"\"\"\n",
    "    return run_eplus_sim(eplus_file=eplus_file, ep_cli=OLD_EPLUS_EXE,\n",
    "                         expand_objects=expand_objects, verbose=verbose)\n",
    "\n",
    "    \n",
    "def run_NEW_eplus_sim(eplus_file: Path, expand_objects: bool = False, verbose: bool = False) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Runs the simulation with NEW_EPLUS_EXE and calls parse_sql\n",
    "    \"\"\"\n",
    "    return run_eplus_sim(eplus_file=eplus_file, ep_cli=NEW_EPLUS_EXE,\n",
    "                         expand_objects=expand_objects, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test one sim\n",
    "if False:    \n",
    "    eplus_file = next(TRANSITION_DIR.glob('*.idf'))\n",
    "    run_NEW_eplus_sim(eplus_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all files in Old EnergyPlus version (or just load SQL)\n",
    "\n",
    "#### Rerun in old E+\n",
    "\n",
    "This should take about 10-15 minutes depending on your machine.\n",
    "\n",
    "It is currently disabled (as RawNBConvert) because we should have already copied the needed SQL files from the old OpenStudio version that is based on the old EnergyPlus Version. Switch this cell to \"Code\" if you do want to rerun with your old installed E+ version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    files = list(OLD_DIR.glob(\"*.idf\"))\n",
    "\n",
    "    pool = multiprocessing.Pool(processes=N)\n",
    "\n",
    "    desc = '<h3>Running files for version {}</h3>'.format(OLD_VERSION)\n",
    "    label = HTML(desc)\n",
    "    display(label)\n",
    "    all_results = []\n",
    "    for result in tqdm(pool.imap_unordered(run_OLD_eplus_sim, files), total=len(files)):\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Concat in dataframe and save to CSV\n",
    "old_results = pd.concat(all_results, axis=1).T\n",
    "old_results.to_csv(os.path.join(IDF_DIR, 'kbtus_8.8.0.csv'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just parse copied SQLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_results = pd.concat([parse_sql_version_and_sitekbtu(os.path.join(OLD_OS_DIR, x)) \n",
    "                         for x in os.listdir(OLD_OS_DIR)\n",
    "                         if os.path.isdir(os.path.join(OLD_OS_DIR, x))],\n",
    "                        axis=1).T\n",
    "\n",
    "old_results['OS'] = OS_OLD_VERSION\n",
    "\n",
    "old_results.to_csv(os.path.join(IDF_DIR, 'kbtus_{o}-{e}.csv'.format(e=EPLUS_OLD_VERSION,\n",
    "                                                                    o=OS_OLD_VERSION)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy IDF\n",
    "\n",
    "In case we need to diff or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_idfs = []\n",
    "for f in gb.iglob(os.path.join(TESTRUNS_DIR, '**/*/in.idf')):\n",
    "    f2 = os.path.relpath(f, TESTRUNS_DIR)\n",
    "    \n",
    "    test_name = os.path.split(os.path.split(f2)[0])[0]\n",
    "    #print(test_name)\n",
    "    dst_path = os.path.join(OLD_OS_DIR, \"{}.idf\".format(test_name))\n",
    "    shutil.copyfile(f, dst_path)\n",
    "    found_idfs.append(test_name)\n",
    "found_idfs = set(found_idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_idfs = [os.path.basename(x) for x in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(found_idfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all transitioned files with new EnergyPlus version\n",
    "This should take about 10-15 minutes depending on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = gb.glob(os.path.join(TRANSITION_DIR, '*.idf'))\n",
    "print(\"{} Files in total\".format(len(files) ))\n",
    "print(\"{} processes\".format(N))\n",
    "files[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 15-20minutes with 12 threads for all tests\n",
    "# New: as of 2.7.0, this takes almost 1.5 hour (with a debug build of E+)!\n",
    "# With 9.2.0 built locally in release, back to 10min\n",
    "# files = gb.glob(os.path.join(TRANSITION_DIR, '*.idf'))\n",
    "\n",
    "pool = multiprocessing.Pool(processes=N)\n",
    "\n",
    "desc = '<h3>Running Transitioned files in E+ {}</h3>'.format(EPLUS_NEW_VERSION)\n",
    "label = HTML(desc)\n",
    "display(label)\n",
    "all_results = []\n",
    "for result in tqdm(pool.imap_unordered(run_NEW_eplus_sim, files), total=len(files)):\n",
    "    all_results.append(result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "```\n",
    "ERROR: /home/julien/Software/Others/OpenStudio-resources/update_eplus_compare/Transition-9.3.0/schedule_file.osm.idf\n",
    "```\n",
    "\n",
    "This is likely due to the fact that the Fortran Transition utility will truncate long strings, in my case it looks like I was one char too long, extension should be `.csv` not `.cs`\n",
    "```\n",
    "  Schedule:File,\n",
    "    Schedule File 1,         !- Name\n",
    "    Fractional,              !- Schedule Type Limits Name\n",
    "    /home/julien/Software/Others/OpenStudio-resources/testruns/schedule_file.osm/files/schedulefile.cs,  !- File Name\n",
    "    3,                       !- Column Number\n",
    "    1,                       !- Rows to Skip at Top\n",
    "    ,                        !- Number of Hours of Data\n",
    "    Comma,                   !- Column Separator\n",
    "    No;                      !- Interpolate to Timestep\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: in 2.7.0 zone_hvac.rb has been hanging for 2 hr 05 min, I'm killing it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you ever send a Keyboard Interrupt signal to kill the above cell\n",
    "# (via Kernel > Interrupt), you need to run this too\n",
    "pool.terminate()\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat in dataframe and save to CSV\n",
    "transitioned_results = pd.concat(all_results, axis=1).T\n",
    "transitioned_results['OS'] = 'Transition'\n",
    "transitioned_results.to_csv(os.path.join(IDF_DIR, 'kbtus_Transition-{e}.csv'.format(e=EPLUS_NEW_VERSION)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitioned_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(old_results.index), len(transitioned_results.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "set(old_results.index) - set(transitioned_results.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is it so damn Slow?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check section 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Simulation in new OpenStudio based on new EnergyPlus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run New OS Version regression tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all testruns to ensure we don't end up grabbing the idf and sql from another version\n",
    "# model_tests.rb only cleans out testruns/testXXX directories for tests we do request\n",
    "# So if you use a regression test filter, you could have left overs\n",
    "if TESTRUNS_DIR.is_dir():\n",
    "    shutil.rmtree(TESTRUNS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass 'CUSTOMTAG=' if you don't want a tag, or 'CUSTOMTAG=sha' for the build sha,\n",
    "# or any custom string such as 'CUSTOMTAG=Ubuntu_run1'\n",
    "CUSTOMTAG='SHA'\n",
    "\n",
    "# REGRESSION_TEST_FILTER = 'surface_properties_lwr'\n",
    "# REGRESSION_TEST_FILTER = 'afn_single_zone_nv'\n",
    "if REGRESSION_TEST_FILTER is None:\n",
    "    filt = ''\n",
    "else:\n",
    "    filt = \"-n /{}/\".format(REGRESSION_TEST_FILTER)\n",
    "\n",
    "command = \"env CUSTOMTAG={c} USE_EPLUS_SPACES=true {cli} {m} {filt}\".format(c=CUSTOMTAG,\n",
    "                                                      m=os.path.join(OS_RES_DIR,\n",
    "                                                                     'model_tests.rb'),\n",
    "                                                      cli=NEW_OS_CLI,\n",
    "                                                      filt=filt)\n",
    "print(command)\n",
    "c_args = shlex.split(command)\n",
    "c_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/julien/Software/Others/OS-build-release/Products/openstudio -e 'puts \"#{OpenStudio::energyPlusVersion}-#{OpenStudio::energyPlusBuildSHA}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = subprocess.Popen(c_args, shell=False,\n",
    "                           stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "# wait for the process to terminate\n",
    "#out, err = process.communicate()\n",
    "#errcode = process.returncode\n",
    "lines = []\n",
    "for line in iter(process.stdout.readline, b''):\n",
    "    print(line.rstrip().decode())\n",
    "    lines.append(line)\n",
    "process.stdout.close()\n",
    "process.wait()\n",
    "    \n",
    "#os.chdir(IDF_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy IDF\n",
    "\n",
    "Copy to the `NEW_OS_DIR` directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTRUNS_DIR, NEW_OS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_idfs = []\n",
    "for f in TESTRUNS_DIR.glob('**/*/in.idf'):\n",
    "    f2 = f.relative_to(TESTRUNS_DIR)\n",
    "    \n",
    "    test_name = f2.parts[0]\n",
    "    #print(test_name)\n",
    "    dst_path = NEW_OS_DIR / f\"{test_name}.idf\"\n",
    "    shutil.copyfile(f, dst_path)\n",
    "    found_idfs.append(test_name)\n",
    "found_idfs = set(found_idfs)\n",
    "len(found_idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_sqls = []\n",
    "\n",
    "for f in TESTRUNS_DIR.glob(\"**/*/*.sql\"):\n",
    "    f2 = f.relative_to(TESTRUNS_DIR)\n",
    "    \n",
    "    test_name = f2.parts[0]\n",
    "    # print(test_name)\n",
    "    dst_folder = NEW_OS_DIR / test_name\n",
    "    if not dst_folder.is_dir():\n",
    "        dst_folder.mkdir(parents=False, exist_ok=False)\n",
    "        \n",
    "    dst_path = dst_folder / \"eplusout.sql\"\n",
    "    # print(dst_path)\n",
    "    shutil.copyfile(f, dst_path)\n",
    "    found_sqls.append(test_name)\n",
    "found_sqls = set(found_sqls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(found_idfs), len(found_sqls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_idfs - found_sqls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse new SQLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results = pd.concat([parse_sql_version_and_sitekbtu(os.path.join(NEW_OS_DIR, x)) \n",
    "                         for x in os.listdir(NEW_OS_DIR)\n",
    "                         if os.path.isdir(os.path.join(NEW_OS_DIR, x))],\n",
    "                        axis=1).T\n",
    "\n",
    "new_results['OS'] = OS_NEW_VERSION\n",
    "\n",
    "new_results.to_csv(os.path.join(IDF_DIR, 'kbtus_{o}-{e}.csv'.format(e=EPLUS_NEW_VERSION,\n",
    "                                                                    o=OS_NEW_VERSION)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Analyzing differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing E+ runtime\n",
    "\n",
    "**THIS NO LONGER WORKS, last version where it worked was 9.2.0... I don't know where the runtime went in the SQLFile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_QUERY_RUNTIME = \"\"\"SELECT ErrorMessage FROM Errors\n",
    "                       WHERE ErrorMessage LIKE '%Elapsed%'\n",
    "\"\"\"\n",
    "\n",
    "TIME_REGEX = re.compile(r'.* Elapsed Time=(?P<hours>\\d+)hr *(?P<minutes>\\d+)min'\n",
    "                        r' *(?P<seconds>[\\d\\.]+)sec')\n",
    "\n",
    "def parse_sql_runtime(output_directory: Path):\n",
    "    \"\"\"\n",
    "    This function grabs the E+ runtime from the SQL file.\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    * output_directory (str): the path were the SQL should be.\n",
    "        eg: `./8.8.0/absorption_chillers.osm_8.8.0/`\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    * runtime (datetime.time)\n",
    "    \"\"\"\n",
    "    output_directory = Path(output_directory).absolute()\n",
    "    sql_files = list(output_directory.glob(\"*.sql\"))\n",
    "    \n",
    "    runtime = None\n",
    "    if len(sql_files) == 1:\n",
    "        sql_path = sql_files[0]\n",
    "        sql_uri = f'{sql_path.as_uri()}?mode=ro'\n",
    "        with sqlite3.connect(sql_uri, uri=True) as con:\n",
    "                cursor = con.cursor()\n",
    "                r = cursor.execute(SQL_QUERY_RUNTIME).fetchone()\n",
    "                if r:\n",
    "                    time_info = r[0]\n",
    "                    m = TIME_REGEX.search(time_info)\n",
    "                    if m:\n",
    "                        gpdict = m.groupdict()\n",
    "                        # runtime = datetime.time(int(gpdict['hours']),\n",
    "                        #                         int(gpdict['minutes']),\n",
    "                        #                         int(float(gpdict['seconds'])))\n",
    "                        runtime = datetime.timedelta(hours=float(gpdict['hours']),\n",
    "                                                     minutes=float(gpdict['minutes']),\n",
    "                                                     seconds=float(gpdict['seconds']))\n",
    "                    else:\n",
    "                        msg = (\"REGEX Failed for \"\n",
    "                               \"For:\\n{}\".format(output_directory))\n",
    "                        raise msg\n",
    "                else:\n",
    "                    msg = (\"Cannot find the Runtime in the SQL file. \"\n",
    "                           \"For:\\n{}\".format(output_directory))\n",
    "                    #raise ValueError(msg)\n",
    "                    print(msg)\n",
    "                    \n",
    "    return [output_directory.name, runtime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimes_transition = [parse_sql_runtime(os.path.join(TRANSITION_DIR, x)) \n",
    "                        for x in next(os.walk(TRANSITION_DIR))[1]]\n",
    "df_time_transition = pd.DataFrame(runtimes_transition, columns=['test', 'runtime']).set_index('test')\n",
    "# Store as fractional minutes\n",
    "df_time_transition['Transition'] = df_time_transition['runtime'].dt.total_seconds() / 60\n",
    "s_time_transition = df_time_transition['Transition']\n",
    "\n",
    "runtimes_old = [parse_sql_runtime(os.path.join(OLD_OS_DIR, x)) \n",
    "                        for x in next(os.walk(OLD_OS_DIR))[1]]\n",
    "df_time_old = pd.DataFrame(runtimes_old, columns=['test', 'runtime']).set_index('test')\n",
    "# Store as fractional minutes\n",
    "df_time_old['OLD_OS'] = df_time_old['runtime'].dt.total_seconds() / 60\n",
    "s_time_old = df_time_old['OLD_OS']\n",
    "\n",
    "runtimes_new = [parse_sql_runtime(os.path.join(NEW_OS_DIR, x)) \n",
    "                        for x in next(os.walk(NEW_OS_DIR))[1]]\n",
    "df_time_new = pd.DataFrame(runtimes_new, columns=['test', 'runtime']).set_index('test')\n",
    "# Store as fractional minutes\n",
    "df_time_new['NEW_OS'] = df_time_new['runtime'].dt.total_seconds() / 60\n",
    "s_time_new = df_time_new['NEW_OS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = pd.concat([s_time_old, s_time_transition, s_time_new], axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time['Abs Diff Min (OLD_OS - Transition)'] = df_time['Transition'] - df_time['OLD_OS']\n",
    "df_time['Abs Diff Min (OLD_OS - NEW_OS)'] = df_time['NEW_OS'] - df_time['OLD_OS']\n",
    "\n",
    "df_time.sort_values('Abs Diff Min (OLD_OS - NEW_OS)', ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time['% Diff Min (OLD_OS - NEW_OS)'] = 100*df_time['Abs Diff Min (OLD_OS - NEW_OS)'] / df_time['OLD_OS']\n",
    "df_time['% Diff Min (OLD_OS - Transition)'] = 100*df_time['Abs Diff Min (OLD_OS - Transition)'] / df_time['OLD_OS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time.describe().style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_from = 'OLD_OS'\n",
    "compare_to = 'NEW_OS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slower = df_time[df_time[compare_to] > df_time[compare_to].quantile(.75)]\n",
    "\n",
    "ax = df_slower[[compare_from, compare_to]].sort_values(compare_to, ascending=True).plot(kind='barh', figsize=(9,18))\n",
    "ax.set_xlim(0)\n",
    "\n",
    "ax.axvline(df_time[compare_to].mean(), c='r', linestyle='--', label='Transition - Mean of All tests')\n",
    "ax.axvline(df_time[compare_from].mean(), c='orange', linestyle='--', label='{} - Mean of All tests'.format(compare_from))\n",
    "\n",
    "ax.set_xlabel('Minutes to run')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reparse SQLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, we could just reparse the SQLs...\n",
    "\n",
    "old_results = pd.concat([parse_sql_version_and_sitekbtu(os.path.join(OLD_OS_DIR, x)) \n",
    "                         for x in next(os.walk(OLD_OS_DIR))[1]],\n",
    "                        axis=1).T\n",
    "old_results['OS'] = OS_OLD_VERSION\n",
    "\n",
    "\n",
    "transitioned_results = pd.concat([parse_sql_version_and_sitekbtu(os.path.join(TRANSITION_DIR, x)) \n",
    "                        for x in next(os.walk(TRANSITION_DIR))[1]],\n",
    "                        axis=1).T\n",
    "transitioned_results['OS'] = 'Transition'\n",
    "\n",
    "\n",
    "new_results = pd.concat([parse_sql_version_and_sitekbtu(os.path.join(NEW_OS_DIR, x)) \n",
    "                         for x in next(os.walk(NEW_OS_DIR))[1]],\n",
    "                        axis=1).T\n",
    "new_results['OS'] = OS_NEW_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(old_results.index), len(transitioned_results.index), len(new_results.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(old_results.index) - set(transitioned_results.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(old_results.index) - set(new_results.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(new_results.index) - set(old_results.index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_results.to_csv(os.path.join(IDF_DIR,\n",
    "                                'kbtus_{o}-{e}.csv'.format(e=EPLUS_OLD_VERSION,\n",
    "                                                           o=OS_OLD_VERSION)))\n",
    "\n",
    "transitioned_results.to_csv(os.path.join(IDF_DIR,\n",
    "                                         'kbtus_Transition-{e}.csv'.format(e=EPLUS_NEW_VERSION)))\n",
    "\n",
    "new_results.to_csv(os.path.join(IDF_DIR,\n",
    "                                'kbtus_{o}-{e}.csv'.format(e=EPLUS_NEW_VERSION,\n",
    "                                                           o=OS_NEW_VERSION)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload site kbtu csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(IDF_DIR)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "old_results = pd.read_csv(os.path.join(IDF_DIR, 'kbtus_{o}-{e}.csv'.format(e=EPLUS_OLD_VERSION,\n",
    "                                                                    o=OS_OLD_VERSION)), index_col=0)\n",
    "\n",
    "transitioned_results = pd.read_csv(os.path.join(IDF_DIR, \n",
    "                                                'kbtus_Transition-{e}.csv'.format(e=EPLUS_NEW_VERSION)),\n",
    "                                                index_col=0)\n",
    "\n",
    "new_results = pd.read_csv(os.path.join(IDF_DIR, 'kbtus_{o}-{e}.csv'.format(e=EPLUS_NEW_VERSION,\n",
    "                                                                    o=OS_NEW_VERSION)), index_col=0)\n",
    "\n",
    "# Strip the version from the index (shouldn't be needed anymore)\n",
    "#old_results.index = [x.replace('_{}'.format(OLD_VERSION),'') for x in old_results.index]\n",
    "#new_results.index = [x.replace('_{}'.format(NEW_VERSION),'') for x in new_results.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat frames and look at files that failed to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([old_results.dropna(how='all'),\n",
    "                transitioned_results.dropna(how='all'),\n",
    "                new_results.dropna(how='all')])\n",
    "\n",
    "df = df.set_index(['E+', 'OS'], append=True).unstack([1,2])['SiteKBTU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throw those that failed throughout\n",
    "df = df[~df.isnull().all(axis=1)]\n",
    "df = df.loc[:, ~df.isnull().all(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problems\n",
    "problems = df[df.isnull().any(axis=1)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip new stuff\n",
    "problems = problems[~problems.iloc[:, :-1].isna().all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def background_colors(val):\n",
    "    fmt = ''\n",
    "    s = 'background-color: {}'\n",
    "    if pd.isnull(val):\n",
    "        fmt = s.format('#F4C7C3')\n",
    "    return fmt\n",
    "print(\"These are the files were we have some (but not all) failures\")\n",
    "problems.style.map(background_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun transition files that need expand objects (can't do it in parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f'{TRANSITION_DIR}/{x}.idf' for x\n",
    "         in problems.index[problems[next(x for x in problems.columns if x[1] == 'Transition')].isna()]]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR: Could not find input data dictionary: /path/to/OpenStudio-resources/update_eplus_compare/Energy+.idd.\n",
    "shutil.copy(OLD_EPLUS_EXE.parent / 'Energy+.idd', IDF_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in tqdm(files):\n",
    "#     run_NEW_eplus_sim(f, expand_objects=True)\n",
    "    \n",
    "# Reparse the SQLs above again"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torun = problems[problems[('23.2.0-7636e6b3e9', 'Transition')].isnull()]\n",
    "\n",
    "s = \"CUSTOMTAG=SHA {} model_tests.rb -n '/\".format(NEW_OS_CLI)\n",
    "tests = []\n",
    "for i, test_name in enumerate(problems.index.tolist()):\n",
    "    test, ext = os.path.splitext(test_name)\n",
    "    ext = ext.replace('.', '')\n",
    "    test_name = \"test_{}_{}\".format(test, ext)\n",
    "    #test_name = \"test_{}\".format(test)\n",
    "    #s += \" --name test_{}_{}\".format(test, ext)\n",
    "    if i < len(problems)-1:\n",
    "        s+='({})|'.format(test_name)\n",
    "    else:\n",
    "        s+='({})'.format(test_name)\n",
    "    tests.append(test_name)\n",
    "\n",
    "s += \"/'\"\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throwing out these problems for further analysis (they need to be investigated manually)\n",
    "df = df.dropna(how='any', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First look at where we have deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "pct_threshold = 0.001\n",
    "print(\"Setting % diff threshold to {:.3%}\".format(pct_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(index='vrf_watercooled.rb')\n",
    "#df = df.drop(index='autosize_hvac.rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.index.str.startswith('sql_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deviations in Transition and/or new OpenStudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "df_diff = df.pct_change(axis=1).iloc[:, 1:].dropna()\n",
    "df_diff = df_diff[(df_diff.abs() >= pct_threshold).any(axis=1)]\n",
    "#df_diff = df_diff.sort_values(by=df_diff.columns[-1],\n",
    "#                              ascending=True)\n",
    "# Sort by max absolute diff\n",
    "df_diff = df_diff.loc[df_diff.abs().max(axis=1).sort_values(ascending=True).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(ncols=2, nrows=1, sharex=True, sharey=True,\n",
    "                               figsize=(16, len(df_diff)/2))\n",
    "df_diff.iloc[:,0].plot(kind='barh', ax=ax0)\n",
    "df_diff.iloc[:,1].plot(kind='barh', ax=ax1)\n",
    "\n",
    "vals = ax0.get_xticks()\n",
    "ax0.set_xticklabels(['{:3.2f}%'.format(x*100) for x in vals])\n",
    "\n",
    "ax0.set_title(\"% difference between {}\\nand {}\".format(df.columns[0], df.columns[1]), pad=30)\n",
    "ax0.set_xlabel(\"% difference\")\n",
    "\n",
    "ax1.set_title(\"% difference between {}\\nand {}\".format(df.columns[1], df.columns[2]), pad=30)\n",
    "ax1.set_xlabel(\"% difference\")\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "ax0.annotate(\"% diff threshold set to {:.3%}\".format(pct_threshold),\n",
    "            xy=(0,0), xycoords='axes fraction',\n",
    "            ha='left', va='top',\n",
    "            xytext=(0, -40), textcoords='offset points', \n",
    "            fontsize=14, fontweight='bold',)\n",
    "\n",
    "#ax0p = ax0.twiny()\n",
    "#vals = ax0.get_xticks()\n",
    "#ax0p.set_xticklabels(['{:3.2f}%'.format(x*100) for x in vals])\n",
    "\n",
    "#ax1p = ax1.twiny()\n",
    "#vals = ax1.get_xticks()\n",
    "#ax1.get_xticks\n",
    "#ax1p.set_xticklabels(['{:3.2f}%'.format(x*100) for x in vals])\n",
    "#ax1p.set_xticks(vals)\n",
    "\n",
    "ax0.axvline(0, color='gray')\n",
    "ax1.axvline(0, color='gray')\n",
    "#ax0.xaxis.set_ticks_position('both')\n",
    "#ax0.xaxis.set_ticks_position('both')\n",
    "ax0.tick_params(labelbottom=True, labeltop=True,\n",
    "                bottom=True, top=True)\n",
    "ax1.tick_params(labelbottom=True, labeltop=True,\n",
    "                bottom=True, top=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_eplus = df_diff.iloc[:, 0]\n",
    "s_eplus.index = pd.MultiIndex.from_tuples(s_eplus[s_eplus.abs() >= pct_threshold].index.str.split('.').tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_eplus = s_eplus.swaplevel(0, 1).loc['osm']\n",
    "s_eplus.name = 'Diff from 24.1.0 to 24.2.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_eplus.to_frame().style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_eplus_diffs = s_eplus.index.tolist()\n",
    "large_eplus_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = IDF_DIR / 'large_eplus_diffs'\n",
    "temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "for f in large_eplus_diffs:\n",
    "    old_idf = OLD_OS_DIR / f\"{f}.osm.idf\"\n",
    "    assert old_idf.is_file()\n",
    "    trans_idf = TRANSITION_DIR / f\"{f}.osm.idf\"\n",
    "    assert trans_idf.is_file()\n",
    "    shutil.copyfile(old_idf, temp_dir / f\"{f}.osm_{EPLUS_OLD_VERSION.replace('.', '_')}.idf\")\n",
    "    copied_new_idf = temp_dir / f\"{f}.osm_{EPLUS_NEW_VERSION.replace('.', '_')}.idf\"\n",
    "    shutil.copyfile(trans_idf, copied_new_idf)\n",
    "    \n",
    "    subprocess.check_output([\n",
    "        '/home/julien/Software/Others/OS-build-release/Products/openstudio',\n",
    "        '-e',\n",
    "        f\"w = OpenStudio::Workspace.load('{copied_new_idf}').get(); w.save('{copied_new_idf}', true)\"\n",
    "    ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPLUS_NEW_VERSION.replace('.', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deviations from Transition to new OS only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_os_diff = df_diff[df_diff.iloc[:,-1].abs() >= pct_threshold]\n",
    "print(f\"Deviations from Transition to new OS only: Differences above {pct_threshold:.3%}\")\n",
    "new_os_diff.style.format('{:.3%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_os_diff.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_os_diff.empty:\n",
    "    print(\"No new diffs due to OS\")\n",
    "else:\n",
    "    fig, (ax0, ax1) = plt.subplots(ncols=2, nrows=1, sharex=True, sharey=True,\n",
    "                                   figsize=(16, len(new_os_diff)/2))\n",
    "    new_os_diff.iloc[:,0].plot(kind='barh', ax=ax0)\n",
    "    new_os_diff.iloc[:,1].plot(kind='barh', ax=ax1)\n",
    "\n",
    "    vals = ax0.get_xticks()\n",
    "    ax0.set_xticklabels(['{:3.2f}%'.format(x*100) for x in vals])\n",
    "\n",
    "    ax0.set_title(\"% difference between {}\\nand {}\".format(df.columns[0], df.columns[1]))\n",
    "    ax0.set_xlabel(\"% difference\")\n",
    "\n",
    "    ax1.set_title(\"% difference between {}\\nand {}\".format(df.columns[1], df.columns[2]))\n",
    "    ax1.set_xlabel(\"% difference\")\n",
    "\n",
    "    sns.despine()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Grouped bar chart of differences compared to the old OpenStudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to read this chart**:\n",
    "\n",
    "The percentage differences are calculated compared to the Old OpenStudio results for both the transitioned results and the new OpenStudio results.\n",
    "\n",
    "**What you need to pay special attention to is when you don't have the same difference between the Transition to Old OS and the New OS to old OS** (meaning the difference is not E+'s fault, but OpenStudio's)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels='autosize_hvac.rb', errors='ignore', inplace=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2.columns = df2.columns.droplevel(0)\n",
    "\n",
    "df_diff_from_old_os = df2.iloc[:,1:].subtract(df2.iloc[:,0], axis=0).divide(df2.iloc[:,0], axis=0)\n",
    "\n",
    "# Keep only over threshold\n",
    "df_diff_from_old_os = df_diff_from_old_os[(df_diff_from_old_os.abs() >= pct_threshold).any(axis=1)]\n",
    "\n",
    "# Sort by max absolute diff\n",
    "df_diff_from_old_os = df_diff_from_old_os.loc[df_diff_from_old_os.abs().max(axis=1).sort_values(ascending=True).index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, len(df_diff_from_old_os)/2))\n",
    "df_diff_from_old_os.plot(kind='barh', stacked=False, ax=ax)\n",
    "\n",
    "vals = ax.get_xticks()\n",
    "ax.set_xticklabels(['{:3.2f}%'.format(x*100) for x in vals])\n",
    "\n",
    "ax.set_title(\"% difference compared to  {}\".format(df.columns[0]))\n",
    "ax.set_xlabel(\"% difference\")\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Sort the other way round for the table\n",
    "(df_diff_from_old_os\n",
    "    .loc[df_diff_from_old_os.abs().max(axis=1)\n",
    "                            .sort_values(ascending=False).index]\n",
    "    .style.format('{:.2%}'))\n",
    "\n",
    "#html = df_diff_from_old_os.style.format('{:.2%}').render()\n",
    "#display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked bar chart of differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ymax_bylabel(label, ax, y_is_top=False):\n",
    "    \"\"\"\n",
    "    Given a label, find the y for that bar, and the max x based on two stacked\n",
    "    bars\n",
    "    \"\"\"\n",
    "    for i, x in enumerate(ax.get_yticklabels()):\n",
    "        if x.get_text() == label:\n",
    "            # y = x.get_position()[1]\n",
    "            \n",
    "            # Find the max x between the two rects\n",
    "            rect1 =  ax.patches[i]\n",
    "            rect_1_xmax = rect1.get_x() + rect1.get_width()\n",
    "            \n",
    "            if y_is_top:\n",
    "                y = rect1.get_y() + rect1.get_height()\n",
    "            else:\n",
    "                y = rect1.get_y() + rect1.get_height() / 2.0\n",
    "                y = x.get_position()[1]\n",
    "            \n",
    "            rect2 = ax.patches[int(i+(len(ax.patches) / 2))]\n",
    "            rect_2_xmax = rect2.get_x() + rect2.get_width()\n",
    "            \n",
    "            return y, max(rect_1_xmax, rect_2_xmax)\n",
    "    return None, None\n",
    "\n",
    "def plot_stacked_bar_difference_compared_to_base(toplot):\n",
    "    fig, ax = plt.subplots(figsize=(16, len(toplot)/2))\n",
    "\n",
    "    # Total % change from old os to new os\n",
    "    s_tot_change = toplot.sum(axis=1)\n",
    "\n",
    "    toplot.plot(kind='barh', stacked=True, ax=ax)\n",
    "\n",
    "    vals = ax.get_xticks()\n",
    "    ax.set_xticklabels(['{:3.2f}%'.format(x*100) for x in vals])\n",
    "\n",
    "    ax.set_title(\"% difference compared to  {}\".format(df.columns[0]))\n",
    "    ax.set_xlabel(\"% difference\")\n",
    "\n",
    "    # for i, rect in enumerate(ax.patches):\n",
    "    #     label = ax.get_yticklabels()[int(i % (len(ax.patches) / 2))].get_text()\n",
    "    #     tot_change = s_tot_change[label]\n",
    "    #     ax.annotate(\"{}-{}-{:.3%}\".format(i, label, tot_change), \n",
    "    #                 xy=(rect.get_x()+rect.get_width(), rect.get_y()))\n",
    "\n",
    "    # Need to draw first otherwise we can't get the position\n",
    "    ax.figure.canvas.draw()\n",
    "\n",
    "\n",
    "\n",
    "    # Add TOTAL % change (sum of both)           \n",
    "    for label, val in s_tot_change.items():\n",
    "        ymid, xmax = find_ymax_bylabel(label=label, ax=ax, y_is_top=False)\n",
    "        ytop, xmax = find_ymax_bylabel(label=label, ax=ax, y_is_top=True)\n",
    "        if xmax is not None:\n",
    "            ax.annotate(\"{:.3%}\".format(val),\n",
    "                        xy=(val, ytop), xycoords='data',\n",
    "                        ha='center', va='bottom', color='k', fontsize=8,\n",
    "                        xytext=(0, 4), textcoords='offset points') \n",
    "            ax.plot(val, ytop, marker='v', c='#494949', alpha=1, zorder=3)\n",
    "\n",
    "    # Custom annotations\n",
    "    #label = 'baseline_sys07.rb'\n",
    "    #y, xmax = find_ymax_bylabel(label=label, ax=ax, y_is_top=False)\n",
    "    #if y is not None:\n",
    "    #    ax.annotate(\"Slight OpenStudio deviation here\", xy=(xmax, y), xycoords='data',\n",
    "    #                ha='left', va='center',\n",
    "    #                xytext=(20, 0), textcoords='offset points',\n",
    "    #                arrowprops=dict(arrowstyle=\"->\",\n",
    "    #                            connectionstyle=\"arc3\"))\n",
    "\n",
    "    #label = 'centralheatpumpsystem.rb'\n",
    "    #y, xmax = find_ymax_bylabel(label=label, ax=ax, y_is_top=False)\n",
    "    #if y is not None:\n",
    "    #    ax.annotate(\"This ruby test is unstable, period\", xy=(xmax, y), xycoords='data',\n",
    "    #                ha='left', va='center',\n",
    "    #                xytext=(20, 0), textcoords='offset points',\n",
    "    #                arrowprops=dict(arrowstyle=\"->\",\n",
    "    #                            connectionstyle=\"arc3\")) \n",
    "\n",
    "    sns.despine()\n",
    "\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to read this chart**:\n",
    "\n",
    "The percentage differences are calculated compared to the Old OpenStudio results for both the transitioned results and the new OpenStudio results. Then I do\n",
    "\n",
    "    % diff New version = % diff New version - % diff Transition\n",
    "\n",
    "and plot that as a stacked bar chart.\n",
    "The goal is to more clearly see the differences that are due to OpenStudio by removing the differences due to the new E+.\n",
    "\n",
    "**What you need to pay special attention to is when you see % differences for the new OS.**\n",
    "\n",
    "**A cursor along with the total % difference between Old OS and New OS is also plotted**.\n",
    "Please see the below example to get a better sense of how the graph is constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff_from_old_example = pd.DataFrame([[0.003, 0.0005],\n",
    "                                         [-0.0004, +0.0003],\n",
    "                                         [0.0002, 0.0004]],\n",
    "                                        index=['test1.rb', 'test2.osm', 'test3.rb'],\n",
    "                                        columns=[TRANSITION_INFO['OS_VERSION'],\n",
    "                                                \"{}-{}\".format(NEW_OS_INFO['EPLUS_VERSION'],\n",
    "                                                               NEW_OS_INFO['OS_VERSION'])]\n",
    "                                       )\n",
    "plot_stacked_bar_difference_compared_to_base(df_diff_from_old_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's do the actual plotting now:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_diff_from_old_os_as_pct = df_diff_from_old_os.copy()\n",
    "df_diff_from_old_os_as_pct.loc[:,OS_NEW_VERSION] = df_diff_from_old_os_as_pct.loc[:,OS_NEW_VERSION] - df_diff_from_old_os_as_pct.loc[:,'Transition']\n",
    "\n",
    "plot_stacked_bar_difference_compared_to_base(df_diff_from_old_os_as_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect biggest differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_annual_energy_by_fuel_and_enduse(sql_path, version_info):\n",
    "    \"\"\"\n",
    "    Queries SQL file and returns the ABUPS' End Uses table\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sql_path (str): path to the sql file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_annual: pd.DataFrame\n",
    "        Annual End Use table\n",
    "        index = 'EndUse'\n",
    "        columns = ['FuelType','Units']\n",
    "    \"\"\"\n",
    "\n",
    "    abs_sql_path = Path(sql_path).absolute()\n",
    "    sql_uri = '{}?mode=ro'.format(abs_sql_path.as_uri())\n",
    "    \n",
    "    # RowName = '#{end_use}'\n",
    "    # ColumnName='#{fuel_type}'\n",
    "    annual_end_use_query = \"\"\"SELECT RowName, ColumnName, Units, Value\n",
    "        FROM TabularDataWithStrings\n",
    "        WHERE ReportName='AnnualBuildingUtilityPerformanceSummary'\n",
    "        AND ReportForString='Entire Facility'\n",
    "        AND TableName='End Uses'\n",
    "    \"\"\"\n",
    "\n",
    "    with sqlite3.connect(sql_uri, uri=True) as con:\n",
    "        df_annual = pd.read_sql(annual_end_use_query, con=con)\n",
    "\n",
    "    # Convert Value to Float\n",
    "    df_annual['Value'] = pd.to_numeric(df_annual['Value'])\n",
    "\n",
    "    df_annual = df_annual.set_index(['RowName',\n",
    "                                     'ColumnName',\n",
    "                                     'Units'])['Value'].unstack([1, 2])\n",
    "    df_annual.index.name = 'EndUse'\n",
    "    df_annual.columns.names = ['FuelType', 'Units']\n",
    "    \n",
    "    end_use_order = ['Heating', 'Cooling',\n",
    "                     'Interior Lighting', 'Exterior Lighting',\n",
    "                     'Interior Equipment', 'Exterior Equipment',\n",
    "                     'Fans', 'Pumps', 'Heat Rejection', 'Humidification',\n",
    "                     'Heat Recovery', 'Water Systems',\n",
    "                     'Refrigeration', 'Generators']\n",
    "\n",
    "    col_order = ['Electricity', 'Natural Gas', 'Additional Fuel',\n",
    "                 'District Cooling', 'District Heating', 'Water']\n",
    "\n",
    "    # 23.2.0: District Heating was split in \"District Heating Water\" and \"District Heating Steam\"\n",
    "    if tuple([int(x) for x in version_info['EPLUS_VERSION'].split('.')]) >= (23, 2, 0):\n",
    "        df_annual[('District Heating', 'GJ')] = df_annual[[\n",
    "            ('District Heating Water', 'GJ'), ('District Heating Steam', 'GJ')\n",
    "        ]].sum(axis=1)\n",
    "    \n",
    "    df_annual = df_annual[[x for x in col_order if x in df_annual.columns.get_level_values(0)]].loc[end_use_order]\n",
    "\n",
    "    return df_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sql_file(test, version_info):\n",
    "    \"\"\"\n",
    "    Find the sql file given a test name and the version_info dict\n",
    "    \n",
    "    Args:\n",
    "    ------\n",
    "    * test (str): the test name\n",
    "    * version_info (dict): should have at least one 'DIR' key with the path\n",
    "    to the directory\n",
    "    \"\"\"\n",
    "    search_path = os.path.join(version_info['DIR'], test, \"*.sql\")\n",
    "\n",
    "    sql_files = gb.glob(search_path)\n",
    "\n",
    "    if len(sql_files) == 0:\n",
    "        return None\n",
    "    elif len(sql_files) > 1:\n",
    "        print(\"Found more than one sql file for {t} in \"\n",
    "              \"{p}\".format(t=test, p=search_path))\n",
    "    return sql_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_single_end_use(test, version_info, add_eplus_version=True):\n",
    "    \"\"\"\n",
    "    Helper to load the end use by fuel\n",
    "    \n",
    "    Args:\n",
    "    ------\n",
    "    * test (str): the test name\n",
    "    * version_info (dict): should have at least one 'DIR' key with the path\n",
    "    to the directory\n",
    "    * add_eplus_version (bool): Whether to construct the name as EP-OS or\n",
    "    just OS. Pass True if both OLD_OS and NEW_OS are the same OS version\n",
    "    \"\"\"\n",
    "    sql_path = find_sql_file(test, version_info=version_info)\n",
    "    if add_eplus_version:\n",
    "        idx = \"{}-{}\".format(version_info['EPLUS_VERSION'], version_info['OS_VERSION'])\n",
    "    else:\n",
    "        idx = version_info['OS_VERSION']\n",
    "        \n",
    "    if sql_path is None:\n",
    "        print(\"Cannot find the sql file for test '{}' and version \"\n",
    "              \"{}\".format(test, idx))\n",
    "        return None\n",
    "\n",
    "    end_use = get_annual_energy_by_fuel_and_enduse(sql_path=sql_path, version_info=version_info)\n",
    "    end_use.columns = pd.MultiIndex.from_tuples([(idx,) + x for x in end_use.columns],\n",
    "                                                names = ['Version'] + end_use.columns.names)\n",
    "    return end_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def parse_before_after_enduse(test,\n",
    "                              old_os=True, transition=True, new_os=True,\n",
    "                              add_eplus_version=True):\n",
    "    \"\"\"\n",
    "    Given a test name, will parse both the old and the new SQL file to return\n",
    "    a table that has, for both versions, the end use by fuel values\n",
    "    \n",
    "    Args:\n",
    "    ------\n",
    "    * test (str): the name of the test. eg 'foundation_kiva.osm'\n",
    "    * old_os, transition, new_os (bool): whether to include these versions\n",
    "    Note that it relies on the respective global dictionaries\n",
    "    OLD_OS_INFO, TRANSITION_INFO, NEW_OS_INFO\n",
    "    * add_eplus_version (bool): Whether to construct the name as EP-OS or\n",
    "    just OS. Pass True if both OLD_OS and NEW_OS are the same OS version\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    * df_all_end_use (pd.DataFrame): a multiindex dataframe of end uses by fuel\n",
    "        index: ['EndUse'] ('Heating', 'Cooling', etc)\n",
    "        columns = ['Version', 'FuelType', 'Units']\n",
    "\n",
    "    \"\"\"\n",
    "    concat_list = []\n",
    "    \n",
    "    if (old_os + transition + new_os) < 2:\n",
    "        print(\"You should request at least 2 versions to compare them...\")\n",
    "        return False\n",
    "    \n",
    "    if old_os:\n",
    "        concat_list.append(parse_single_end_use(test, OLD_OS_INFO,\n",
    "                                                add_eplus_version=add_eplus_version))\n",
    "    if transition:\n",
    "        concat_list.append(parse_single_end_use(test, TRANSITION_INFO,\n",
    "                                                add_eplus_version=add_eplus_version))\n",
    "    if new_os:\n",
    "        concat_list.append(parse_single_end_use(test, NEW_OS_INFO,\n",
    "                                                add_eplus_version=add_eplus_version))\n",
    "\n",
    "        \n",
    "    df_all_end_use = pd.concat([x for x in concat_list if x is not None],\n",
    "                               axis=1)\n",
    "\n",
    "    return df_all_end_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_end_use_diff(df_all_end_use, test,\n",
    "                      add_legend=True, fontsize=None,\n",
    "                      outer_i=None, fig=None,\n",
    "                      add_eplus_version=True\n",
    "                      ):\n",
    "    \"\"\"\n",
    "    Plots the difference in end use by fuel between the new and old versions\n",
    "    Will have subplots by units (water versus energy), a subplot is only shown\n",
    "    if there is consumption in the said end use (eg if no Water, there will \n",
    "    be only one subplot)\n",
    "    Displays a grouped bar chart by end use, and annotates % difference in the\n",
    "    given end use\n",
    "    \n",
    "    Args:\n",
    "    ------\n",
    "    * both_end_use (pd.DataFrame): dataframe from `parse_before_after_enduse`\n",
    "    \n",
    "    * outer_i and fig: if you want to customize the layout yourself. \n",
    "        Pass None otherwise (default)\n",
    "        \n",
    "    * old_info, new_info (dict): dict with information such as 'DIR', \n",
    "    'EPLUS_VERSION' and 'OS_VERSION'\n",
    "    \n",
    "    * add_eplus_version (bool): Whether to construct the name as EP-OS or\n",
    "    just OS. Pass True if both OLD_OS and NEW_OS are the same OS version\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    * None, displays a plot\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if fontsize is None:\n",
    "        fontsize = 10\n",
    "    \n",
    "    diff = (df_all_end_use.groupby(level=['Version', 'Units'], axis=1).sum()\n",
    "                          .replace(0, np.nan)\n",
    "                          .dropna(how='all', axis=0)\n",
    "                          .dropna(how='all', axis=1))\n",
    "\n",
    "    if add_eplus_version:\n",
    "        l_order = [\"{}-{}\".format(OLD_OS_INFO['EPLUS_VERSION'],\n",
    "                                  OLD_OS_INFO['OS_VERSION']),\n",
    "                   \n",
    "                   \"{}-{}\".format(TRANSITION_INFO['EPLUS_VERSION'],\n",
    "                                  TRANSITION_INFO['OS_VERSION']),\n",
    "                   \n",
    "                   \"{}-{}\".format(NEW_OS_INFO['EPLUS_VERSION'],\n",
    "                                  NEW_OS_INFO['OS_VERSION']),\n",
    "                  ]\n",
    "    else:\n",
    "        l_order = [OLD_OS_INFO['OS_VERSION'],\n",
    "                   TRANSITION_INFO['OS_VERSION'],\n",
    "                   NEW_OS_INFO['OS_VERSION']]\n",
    "    # Reorder properly\n",
    "    diff = diff[[x for x in l_order if x in diff.columns]]\n",
    "    \n",
    "    grouped = diff.groupby(level='Units', axis=1)\n",
    "\n",
    "    ncols = min(len(grouped), 2)\n",
    "    nrows = int(np.ceil(grouped.ngroups/ncols))\n",
    "\n",
    "    # If you don't supply outer_i, we take care of everything\n",
    "    if outer_i is None:\n",
    "        fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(8*ncols,5*nrows))\n",
    "        if not isinstance(axes, np.ndarray):\n",
    "            axes = np.array([axes])\n",
    "    else:\n",
    "        inner = mpl.gridspec.GridSpecFromSubplotSpec(nrows, ncols,\n",
    "                    subplot_spec=outer_i, wspace=0.1, hspace=0.1)\n",
    "        \n",
    "        #np.array([[\"{},{}\".format(x,y) for y in range(ncols)] for x in range(nrows)])\n",
    "        axes = np.array([plt.Subplot(fig, inner[j]) for j in range(nrows*ncols)])\n",
    "        [fig.add_subplot(ax) for ax in axes]\n",
    "\n",
    "    # Plot each subplot\n",
    "    first_legend = add_legend\n",
    "    for (key, ax) in zip(grouped.groups.keys(), axes.flatten()):\n",
    "        gp = grouped.get_group(key)\n",
    "        gp.columns= gp.columns.droplevel('Units')\n",
    "        gp.index.name = ''\n",
    "        \n",
    "        # Sort by max absolute difference\n",
    "        gp = gp.loc[gp.apply(lambda row: max(row) - min(row), axis=1)\n",
    "                      .sort_values(ascending=False).index]\n",
    "        gp.plot(kind='bar', ax=ax)\n",
    "        if key == 'GJ':\n",
    "            title = \"Energy (GJ)\"\n",
    "        elif key == 'm3':\n",
    "            title = \"Water (m3)\"\n",
    "        else:\n",
    "            # shouldn't happen\n",
    "            title = key\n",
    "            \n",
    "        # Add labels with fontsize\n",
    "        ax.set_title(title, fontsize=fontsize+2)\n",
    "        ax.set_ylabel(key, fontsize=fontsize)\n",
    "        # Set tick size\n",
    "        for tick in ax.xaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(fontsize)\n",
    "            tick.label.set_rotation(45)\n",
    "        for tick in ax.yaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(fontsize)\n",
    "            \n",
    "        # Add % difference if any\n",
    "        for i, x in enumerate(ax.get_xticklabels()):\n",
    "            # Return 'Heating', 'Cooling', etc\n",
    "            idx = x.get_text()\n",
    "\n",
    "            # Loop on each successive versions\n",
    "            for k in range(len(gp.columns)-1):\n",
    "                v_old = gp.loc[idx].iloc[k]\n",
    "                v_new = gp.loc[idx].iloc[k+1]\n",
    "                if abs(v_new - v_old) > 0:\n",
    "                    pct = (v_new - v_old) / v_old\n",
    "\n",
    "                    if v_old > v_new:\n",
    "                        # Base on the first bar\n",
    "                        rect = ax.patches[i+k*len(ax.patches) // len(gp.columns)]\n",
    "                        # Offset needed to be at mid between both bars\n",
    "                        x_offset = rect.get_width()\n",
    "                    else:\n",
    "                        # Based on second bar\n",
    "                        rect = ax.patches[i+(k+1)*len(ax.patches) // len(gp.columns)]\n",
    "                        x_offset = 0\n",
    "\n",
    "                    ax.annotate(\"{:.2%}\".format(pct),\n",
    "                                xy=(rect.get_x() + x_offset, rect.get_height()+0.05),\n",
    "                                xytext=(15*(2*k-1),20), textcoords='offset points',\n",
    "                                ha='center', va='bottom',\n",
    "                                fontweight='normal',\n",
    "                                fontsize=fontsize-2, color='k',\n",
    "                                arrowprops=dict(arrowstyle=\"->\",\n",
    "                                connectionstyle=\"arc3\"))\n",
    "        \n",
    "        # Display legend or not\n",
    "        if not first_legend:\n",
    "            ax.legend().set_visible(False)\n",
    "        else:\n",
    "            ax.legend()\n",
    "        first_legend = False\n",
    "            \n",
    "    sns.despine()\n",
    "    \n",
    "    # Title\n",
    "    title = \"End Use for {}\".format(test)\n",
    "    # fig.suptitle(title)\n",
    "    axes[0].annotate(title,\n",
    "                     xy=(0.5*ncols, 1.0), xycoords='axes fraction',\n",
    "                     xytext=(0, 20), textcoords='offset points',\n",
    "                     va='bottom', ha='center',\n",
    "                     fontsize=fontsize+4, fontweight='bold',\n",
    "                     )\n",
    "    if outer_i is None:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "#test = 'centralheatpumpsystem.rb'\n",
    "#test = 'evaporative_cooling.osm'\n",
    "#df_all_end_use = parse_before_after_enduse(test)\n",
    "#plot_end_use_diff(df_all_end_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_absolute_difference(df_all_end_use, is_incremental=True,\n",
    "                             add_eplus_version=True):\n",
    "    \"\"\"\n",
    "    Computes the absolute difference in (GJ/m3).\n",
    "    If is_incremental, compares from one version to the next\n",
    "        eg: returns [Transition - 2.4.2] and '2.4.3 - Transition')\n",
    "    if false, compares to the oldest one\n",
    "        eg: returns [Transition - 2.4.2] and '2.4.3 - 2.4.2')\n",
    "\n",
    "    Args:\n",
    "    ------\n",
    "    * df_all_end_use (pd.DataFrame): dataframe from parse_before_after_enduse\n",
    "    * is_incremental (bool): compare each version to the previous version\n",
    "        or to the oldest one\n",
    "    * add_eplus_version (bool): Whether to construct the name as EP-OS or\n",
    "    just OS. Pass True if both OLD_OS and NEW_OS are the same OS version\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    * abs_diff (pd.DataFrame)\n",
    "    * html_abs = HTML object\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if add_eplus_version:\n",
    "        l_order = [\"{}-{}\".format(OLD_OS_INFO['EPLUS_VERSION'],\n",
    "                                  OLD_OS_INFO['OS_VERSION']),\n",
    "                   \n",
    "                   \"{}-{}\".format(TRANSITION_INFO['EPLUS_VERSION'],\n",
    "                                  TRANSITION_INFO['OS_VERSION']),\n",
    "                   \n",
    "                   \"{}-{}\".format(NEW_OS_INFO['EPLUS_VERSION'],\n",
    "                                  NEW_OS_INFO['OS_VERSION']),\n",
    "                  ]\n",
    "    else:\n",
    "        l_order = [OLD_OS_INFO['OS_VERSION'],\n",
    "                   TRANSITION_INFO['OS_VERSION'],\n",
    "                   NEW_OS_INFO['OS_VERSION']]\n",
    "        \n",
    "    # Sort in the right order\n",
    "    cols_in_order = [x for x in l_order \n",
    "                     if x in df_all_end_use.columns]\n",
    "        \n",
    "    abs_diff = df_all_end_use.copy()\n",
    "    for i, col in enumerate(cols_in_order[1:]):\n",
    "        if is_incremental:\n",
    "            k = i\n",
    "        else:\n",
    "            k = 0\n",
    "        abs_diff[col] = df_all_end_use[col] - df_all_end_use[cols_in_order[k]]\n",
    "    abs_diff = abs_diff[cols_in_order[1:]]\n",
    "    \n",
    "    abs_diff = (abs_diff.replace(0, np.nan)\n",
    "                        .dropna(how='all', axis=0)\n",
    "                        .dropna(how='all', axis=1))\n",
    "    \n",
    "    if is_incremental:\n",
    "        ann = \"<strong>Comparing from one version to the next</strong>\"\n",
    "    else:\n",
    "        ann = \"<strong>Comparing each version to {}</strong>\".format(cols_in_order[0])\n",
    "    if abs_diff.empty:\n",
    "        # print(\"There are ZERO absolute differences for {}\".format(test))\n",
    "        html = HTML('<p>{}</p>\\n<p style=\"font-size: 18px; text-align: center\">'\n",
    "                    'There are <strong>ZERO</strong> absolute differences '\n",
    "                    'for {}</p>'.format(ann, test))\n",
    "    else:\n",
    "        html = (abs_diff.style.set_table_styles(styles)\n",
    "                 .set_caption(\"Absolute diff for {}\\n{}\".format(test, ann))\n",
    "                 .format(lambda x: \"{:.0f}\".format(x) if not np.isnan(x) else '-'))\n",
    "        # display(html)\n",
    "    return abs_diff, html\n",
    "\n",
    "def table_percent_difference_by_end_use_and_fuel(df_all_end_use,\n",
    "                                                 is_incremental=True,\n",
    "                                                 add_eplus_version=True):\n",
    "    \"\"\"\n",
    "    Computes the percentage difference in between the old and the new for each\n",
    "    end use and fuel.\n",
    "    \n",
    "    eg: Heating Electricity % is calculated as\n",
    "        (heating-electricity-kbtu-new) - (heating-electricity-kbtu-old)\n",
    "        / (heating-electricity-kbtu-old)\n",
    "    \n",
    "    Args:\n",
    "    ------\n",
    "    * df_all_end_use (pd.DataFrame): dataframe from parse_before_after_enduse\n",
    "    \n",
    "    * add_eplus_version (bool): Whether to construct the name as EP-OS or\n",
    "    just OS. Pass True if both OLD_OS and NEW_OS are the same OS version\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    * pct_diff (pd.DataFrame)\n",
    "    * html_diff = HTML object\n",
    "    \"\"\"\n",
    "    if add_eplus_version:\n",
    "        l_order = [\"{}-{}\".format(OLD_OS_INFO['EPLUS_VERSION'],\n",
    "                                  OLD_OS_INFO['OS_VERSION']),\n",
    "                   \n",
    "                   \"{}-{}\".format(TRANSITION_INFO['EPLUS_VERSION'],\n",
    "                                  TRANSITION_INFO['OS_VERSION']),\n",
    "                   \n",
    "                   \"{}-{}\".format(NEW_OS_INFO['EPLUS_VERSION'],\n",
    "                                  NEW_OS_INFO['OS_VERSION']),\n",
    "                  ]\n",
    "    else:\n",
    "        l_order = [OLD_OS_INFO['OS_VERSION'],\n",
    "                   TRANSITION_INFO['OS_VERSION'],\n",
    "                   NEW_OS_INFO['OS_VERSION']]\n",
    "        \n",
    "    # Sort in the right order\n",
    "    cols_in_order = [x for x in l_order \n",
    "                     if x in df_all_end_use.columns]\n",
    "        \n",
    "    pct_diff = df_all_end_use.copy()\n",
    "    for i, col in enumerate(cols_in_order[1:]):\n",
    "        if is_incremental:\n",
    "            k = i\n",
    "        else:\n",
    "            k = 0\n",
    "        pct_diff[col] = (df_all_end_use[col] - df_all_end_use[cols_in_order[k]]) / df_all_end_use[cols_in_order[k]]\n",
    "\n",
    "    pct_diff = pct_diff[cols_in_order[1:]]\n",
    "    \n",
    "    pct_diff = (pct_diff.replace(0, np.nan)\n",
    "                        .dropna(how='all', axis=0)\n",
    "                        .dropna(how='all', axis=1))\n",
    "    \n",
    "    if is_incremental:\n",
    "        ann = \"<strong>Comparing from one version to the next</strong>\"\n",
    "    else:\n",
    "        ann = \"<strong>Comparing each version to {}</strong>\".format(cols_in_order[0])\n",
    "    \n",
    "    if pct_diff.empty:\n",
    "        # print(\"There are ZERO percentage differences for {}\".format(test))\n",
    "        html = HTML('<p>{}</p><p style=\"font-size: 18px; text-align: center\">'\n",
    "                    'There are <strong>ZERO</strong> percentage differences '\n",
    "                    'for {}</p>'.format(ann, test))\n",
    "    else:\n",
    "        html = (pct_diff.style.set_table_styles(styles)\n",
    "                 .set_caption(\"Relative individual % diff for each end use and\"\n",
    "                              \" fuel for '{}'\\n{}\".format(test, ann))\n",
    "                 .format(lambda x: \"{:.2%}\".format(x) if not np.isnan(x) else '-'))\n",
    "        #display(html)\n",
    "    return pct_diff, html\n",
    "\n",
    "def table_percent_difference_of_total(df_all_end_use, is_incremental=True,\n",
    "                                      add_eplus_version=True):\n",
    "    \"\"\"\n",
    "    Computes the percentage difference in between the old and the new for each\n",
    "    type Water or GJ.\n",
    "    \n",
    "    eg: Heating % is calculated as\n",
    "        sum(heating-GJ-each-fuel-new) - sum(heating-GJ-each-fueld-old)\n",
    "        / sum(all_GJ)\n",
    "    \n",
    "    Args:\n",
    "    ------\n",
    "    * df_all_end_use (pd.DataFrame): dataframe from parse_before_after_enduse\n",
    "    * is_incremental (bool): compare each version to the previous version\n",
    "        or to the oldest one\n",
    "    * add_eplus_version (bool): Whether to construct the name as EP-OS or\n",
    "    just OS. Pass True if both OLD_OS and NEW_OS are the same OS version\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    * percentage_of_total (pd.DataFrame)\n",
    "    * html_tot = HTML object\n",
    "  \n",
    "    \"\"\"\n",
    "    \n",
    "    if add_eplus_version:\n",
    "        l_order = [\"{}-{}\".format(OLD_OS_INFO['EPLUS_VERSION'],\n",
    "                                  OLD_OS_INFO['OS_VERSION']),\n",
    "                   \n",
    "                   \"{}-{}\".format(TRANSITION_INFO['EPLUS_VERSION'],\n",
    "                                  TRANSITION_INFO['OS_VERSION']),\n",
    "                   \n",
    "                   \"{}-{}\".format(NEW_OS_INFO['EPLUS_VERSION'],\n",
    "                                  NEW_OS_INFO['OS_VERSION']),\n",
    "                  ]\n",
    "    else:\n",
    "        l_order = [OLD_OS_INFO['OS_VERSION'],\n",
    "                   TRANSITION_INFO['OS_VERSION'],\n",
    "                   NEW_OS_INFO['OS_VERSION']]\n",
    "        \n",
    "    # Sort in the right order\n",
    "    cols_in_order = [x for x in l_order \n",
    "                     if x in df_all_end_use.columns]\n",
    "\n",
    "    concat_dict = {}\n",
    "    for i, col in enumerate(cols_in_order[1:]):\n",
    "        if is_incremental:\n",
    "            k = i\n",
    "        else:\n",
    "            k = 0\n",
    "        sum_old = (df_all_end_use[cols_in_order[k]]\n",
    "                   .groupby(level='Units', axis=1).sum().sum())\n",
    "        abs_diff_end_use = ((df_all_end_use[col] \n",
    "                            - df_all_end_use[cols_in_order[k]])\n",
    "                            .stack(0)\n",
    "                            .groupby(level='EndUse').sum())\n",
    "\n",
    "        percentage_of_total = (abs_diff_end_use / sum_old)\n",
    "        d = {'GJ': 'Energy', 'm3': 'Water'}\n",
    "        percentage_of_total.columns = pd.MultiIndex.from_tuples([(d[x], x) \n",
    "                                                                 for x in percentage_of_total.columns],\n",
    "                                                                names=['Type', 'Units'])\n",
    "        concat_dict[col] = percentage_of_total\n",
    "    \n",
    "    percentage_of_total = pd.concat(concat_dict, axis=1)[cols_in_order[1:]]\n",
    "    # Drop end uses where we have nothing\n",
    "    percentage_of_total = (percentage_of_total.reindex(df_all_end_use.index)\n",
    "                                               .replace(0, np.nan)\n",
    "                                               .dropna(how='all', axis=0)\n",
    "                           )\n",
    "\n",
    "    # drop Fuel Type (units) where none of the versions have a change\n",
    "    sum_by_type =  percentage_of_total.groupby(level='Units', axis=1).sum().sum()\n",
    "    percentage_of_total.loc[:,\n",
    "                            (percentage_of_total.columns\n",
    "                             .get_level_values('Units')\n",
    "                             .isin(sum_by_type.index[sum_by_type != 0]))]\n",
    "    if is_incremental:\n",
    "        ann = \"<strong>Comparing from one version to the next</strong>\"\n",
    "    else:\n",
    "        ann = \"<strong>Comparing each version to {}</strong>\".format(cols_in_order[0])\n",
    "    \n",
    "    if percentage_of_total.empty:\n",
    "        # print(\"There are ZERO percentage differences for {}\".format(test))\n",
    "        html = HTML('<p>{}</p><p style=\"font-size: 18px; text-align: center\">'\n",
    "                    'There are <strong>ZERO</strong> percentage differences '\n",
    "                    'for {}</p>'.format(ann, test))\n",
    "    else:\n",
    "        html = (percentage_of_total.style.set_table_styles(styles)\n",
    "                 .set_caption(\"% diff of total GJ/m3 {}\\n{}\".format(test, ann))\n",
    "                 .format(lambda x: \"{:.2%}\".format(x) if not np.isnan(x) else '-'))\n",
    "        # display(html)\n",
    "    return percentage_of_total, html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_for_test(test, is_incremental=True,\n",
    "                    old_os=True, transition=True, new_os=True,\n",
    "                    plot_if_no_diff=True, add_legend=True,\n",
    "                    display_tables=False,\n",
    "                    plot_heatmap=False, heatmap_as_pct_of_total=False,\n",
    "                    outer_i=None, fig=None, add_eplus_version=True):\n",
    "    \"\"\"\n",
    "    High level method to investigate differences\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    * test (str): test name, eg 'centralheatpumpsystem.rb\n",
    "    * is_incremental (bool): compare each version to the previous version\n",
    "        or to the oldest one\n",
    "\n",
    "    * old_os, transition, new_os (bool): whether to include these versions\n",
    "    Note that it relies on the respective global dictionaries\n",
    "    OLD_OS_INFO, TRANSITION_INFO, NEW_OS_INFO\n",
    "    * display_tables (bool): if true, shows `table_absolute_difference`\n",
    "        and `table_percent_difference_table`\n",
    "    * plot_if_no_diff (bool): if there are no difference, whether to show \n",
    "        `plot_end_use_diff` anyways or not\n",
    "    * gs (matplotlib GridSpec): Pass one if you want to organize the figures\n",
    "        in a given layout, otherwise a new plot is created\n",
    "    * add_eplus_version (bool): Whether to construct the name as EP-OS or\n",
    "    just OS. Pass True if both OLD_OS and NEW_OS are the same OS version\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    * fig (matplotlib.figure)\n",
    "    \n",
    "    Can display requested things on the fly too\n",
    "    \n",
    "    \"\"\"\n",
    "    has_diff = False\n",
    "    \n",
    "    df_all_end_use = parse_before_after_enduse(test,\n",
    "                                               old_os=old_os,\n",
    "                                               transition=transition,\n",
    "                                               new_os=new_os)\n",
    "    abs_diff, html_abs = table_absolute_difference(df_all_end_use, is_incremental, add_eplus_version=add_eplus_version)\n",
    "    # display(html_abs)\n",
    "    if not abs_diff.empty:\n",
    "        has_diff = True\n",
    "        if display_tables:\n",
    "            pct_diff, html_pct = table_percent_difference_by_end_use_and_fuel(df_all_end_use, is_incremental, add_eplus_version=add_eplus_version)\n",
    "            percentage_of_total, html_tot = table_percent_difference_of_total(df_all_end_use, is_incremental, add_eplus_version=add_eplus_version)\n",
    "\n",
    "            display(HTML(\"\"\"\n",
    "            <div style='display: grid; grid-template-columns: 1fr 1fr 1fr; grid-column-gap: 10px;'>\n",
    "                <div style='align-self: center; width: '> {html_abs} </div>\n",
    "                <div style='align-self: center;'> {html_pct} </div>\n",
    "                <div style='align-self: center;'> {html_tot} </div>\n",
    "            </div>\"\"\".format(html_abs=html_abs.to_html(),\n",
    "                             html_pct=html_pct.to_html(),\n",
    "                             html_tot=html_tot.to_html())))\n",
    "        \n",
    "    else:\n",
    "        display(html_abs)\n",
    "    \n",
    "    if (has_diff | plot_if_no_diff):\n",
    "        plot_end_use_diff(df_all_end_use=df_all_end_use, test=test,\n",
    "                          outer_i=outer_i, fig=fig,\n",
    "                          add_legend=add_legend)\n",
    "        \n",
    "    if has_diff & plot_heatmap:\n",
    "        if heatmap_as_pct_of_total:\n",
    "            plot_heatmap_pct_diff(test=test, pct_diff=percentage_of_total,\n",
    "                                  is_incremental=is_incremental,\n",
    "                                  as_pct_of_total=True)\n",
    "        else:\n",
    "            plot_heatmap_pct_diff(test=test, pct_diff=pct_diff,\n",
    "                                  is_incremental=is_incremental,\n",
    "                                  as_pct_of_total=False)\n",
    "        \n",
    "def plot_heatmap_pct_diff(test, is_incremental=False,\n",
    "                          pct_diff=None, df_all_end_use=None,\n",
    "                          ax=None, figsize=None, short_title=False,\n",
    "                          as_pct_of_total=False, vmax=None):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of percentage difference. It will show the xlabels\n",
    "    as \"Fuel\" only if one unit, if more than one it's \"Fuel-Units\"\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    * test (str): test name\n",
    "    * pct_diff (pd.DataFrame): from `table_percent_difference_table(both_end_use)`\n",
    "        If not supplied, it is recomputed\n",
    "        * as_pct_of_total (bool): if True, calls `table_percent_difference_of_total`\n",
    "        otherwise calls `table_percent_difference_by_end_use_and_fuel`\n",
    "    * df_all_end_use (pd.DataFrame): from `parse_before_after_enduse(test)`\n",
    "        If pct_diff is not supplied, it uses this dataframe to recompute pct_diff\n",
    "        If also not supplied, it is recomputed\n",
    "    * ax (matplotlib.axes._subplots.AxesSubplot): The axis on which to plot,\n",
    "        pass None to create a new figure\n",
    "    * figsize (tuple of int): force a given figure size\n",
    "        pass None to autocalculate\n",
    "    * short_title (bool): display only the test name or also with versions\n",
    "    * vmax (float, typically between 0 and 1): the maximum for the colorbar\n",
    "        if None, defaults to 0.25 (25%) is as_pct_of_total is False, and\n",
    "        0.05 (5%) if as_pct_of_total is True\n",
    "    Returns:\n",
    "    --------\n",
    "    None, plots the heatmap\n",
    "    \"\"\"\n",
    "    if vmax is None:\n",
    "        if as_pct_of_total:\n",
    "            # Colorbar goes from 0 (yellow)to 5% (red)\n",
    "            vmax = 0.05\n",
    "        else:\n",
    "            vmax = 0.25\n",
    "    \n",
    "    # Modularity in arguments, compute only what's needed\n",
    "    if pct_diff is None:\n",
    "        if df_all_end_use is None:\n",
    "            df_all_end_use = parse_before_after_enduse(test)\n",
    "        if as_pct_of_total:\n",
    "            pct_diff, _ = table_percent_difference_of_total(df_all_end_use=df_all_end_use,\n",
    "                                                            is_incremental=is_incremental)\n",
    "        else:\n",
    "            pct_diff, _ = table_percent_difference_by_end_use_and_fuel(df_all_end_use=df_all_end_use,\n",
    "                                                                       is_incremental=is_incremental)\n",
    "\n",
    "    show_plot = False\n",
    "    if ax is None:\n",
    "        show_plot = True\n",
    "        if figsize is None:\n",
    "            w = min(pct_diff.shape[0], 16)\n",
    "            h = pct_diff.shape[0] * w / (3*pct_diff.shape[1])\n",
    "        else:\n",
    "            w = figsize[0]\n",
    "            h = figsize[1]\n",
    "        fig, ax = plt.subplots(figsize=(w, h))\n",
    "\n",
    "    fmt = lambda x,pos: '{:.0%}'.format(x)\n",
    "\n",
    "    toplot = pct_diff.copy()\n",
    "    if len(toplot.columns.get_level_values('Units').unique()) == 1:\n",
    "        toplot.columns = toplot.columns.droplevel('Units')\n",
    "\n",
    "    sns.heatmap(toplot.abs(),\n",
    "                ax=ax, cmap='YlOrRd',\n",
    "                vmin=0, vmax=vmax,\n",
    "                cbar_kws={'format': mpl.ticker.FuncFormatter(fmt)},\n",
    "                annot=toplot, fmt='.2%')\n",
    "    if short_title:\n",
    "        title = test\n",
    "    else:\n",
    "        if as_pct_of_total:\n",
    "            title = (\"Percent difference of total GJ/m3 by End Use for test \"\n",
    "                     \"'{}'\".format(test))\n",
    "        else:\n",
    "            title = (\"Relative Individual % diff for each End Use / Fuel for test \"\n",
    "                     \"'{}'\".format(test))\n",
    "        if is_incremental:\n",
    "            title += '\\nComparing from one version to the next'\n",
    "        else:\n",
    "            title += '\\nComparing each to the oldest version ({})'.format(OLD_OS_INFO['OS_VERSION'])\n",
    "    ax.set_title(title)\n",
    "    if show_plot:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hover(hover_color=\"#ffff99\"):\n",
    "    return dict(selector=\"tr:hover\",\n",
    "                props=[(\"background-color\", \"%s\" % hover_color)])\n",
    "\n",
    "styles = [\n",
    "    hover(),\n",
    "    dict(selector=\"th\", props=[(\"font-size\", \"110%\"),\n",
    "                               (\"text-align\", \"center\")]),\n",
    "    dict(selector=\"td\", props=[(\"text-align\", \"center\")]),\n",
    "    dict(selector=\"caption\", props=[(\"caption-side\", \"bottom\"),\n",
    "                                    (\"text-align\", \"center\")])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make graph centered on page\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Has differences, but no water\n",
    "test = 'unitary_vav_bypass_plenum.rb'\n",
    "\n",
    "#test = 'surfacecontrol_moveableinsulation.rb'\n",
    "report_for_test(test, is_incremental=False,\n",
    "                old_os=True, transition=True, new_os=True,\n",
    "                plot_if_no_diff=True, display_tables=True,\n",
    "                plot_heatmap=True, heatmap_as_pct_of_total=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = 'lifecyclecostparameters.rb'\n",
    "#test = 'vrf.osm'\n",
    "print(f\"Investigating the {test=}\")\n",
    "df_all_end_use = parse_before_after_enduse(test,\n",
    "                                           old_os=True, transition=True,\n",
    "                                           new_os=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show absolute values for only fuel/end use that have a value\n",
    "print(f'Deviations in end uses for {test=}')\n",
    "df_all_end_use.replace(0, np.nan).dropna(how='all', axis=0).dropna(how='all', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_diff, html = table_percent_difference_by_end_use_and_fuel(df_all_end_use)\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't supply pct_diff, it will be computed again\n",
    "# if also you don't supply both_end_use, it will be computed again\n",
    "# Do notice the is_incremental keyword again...\n",
    "plot_heatmap_pct_diff(test, pct_diff=None,\n",
    "                      is_incremental=True,\n",
    "                      df_all_end_use=None,\n",
    "                      figsize=(16,9), short_title=False,\n",
    "                      # Switch as_pct_of_total to see the difference\n",
    "                      as_pct_of_total=True, vmax=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Tests - one per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only really care about differences between Transition and new OS\n",
    "s_diff = df_diff.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report_for_largest_n = 6\n",
    "print(\"Reporting for {} largest differences in Transition to New OS\".format(report_for_largest_n))\n",
    "\n",
    "add_legend=True\n",
    "for test in s_diff.abs().nlargest(report_for_largest_n).index:\n",
    "    report_for_test(test, plot_if_no_diff=True, display_tables=False, \n",
    "                    add_legend=True)\n",
    "    add_legend=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Tests - in a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_for_largest_n = 6\n",
    "print(\"Reporting for {} largest differences in Transition to New OS\".format(report_for_largest_n))\n",
    "\n",
    "outer_ncols = 2\n",
    "outer_nrows = int(np.ceil(report_for_largest_n/outer_ncols))\n",
    "\n",
    "fig = plt.figure(figsize=(16, 4*outer_nrows))\n",
    "outer = mpl.gridspec.GridSpec(outer_nrows, outer_ncols, wspace=0.1, hspace=0.5)\n",
    "\n",
    "add_legend = True\n",
    "for i, test in enumerate(s_diff.abs().nlargest(report_for_largest_n).index):\n",
    "    df_all_end_use = parse_before_after_enduse(test)\n",
    "    plot_end_use_diff(df_all_end_use=df_all_end_use, test=test, \n",
    "                      outer_i=outer[i], \n",
    "                      fig=fig, fontsize=7, add_legend=add_legend)\n",
    "    add_legend = False\n",
    "\n",
    "# fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Tests - heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report_for_largest_n = 10\n",
    "print(\"Reporting for {} largest differences in Transition to New OS\".format(report_for_largest_n))\n",
    "\n",
    "ncols = 2\n",
    "nrows = int(np.ceil(report_for_largest_n/outer_ncols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 4*outer_nrows))\n",
    "\n",
    "add_legend = True\n",
    "for test, ax in zip(s_diff.abs().nlargest(report_for_largest_n).index, axes.flatten()):\n",
    "    # Switch as_pct_of_total, force vmax (max of colorbar) if you want\n",
    "    plot_heatmap_pct_diff(test, ax=ax,\n",
    "                          is_incremental=True,\n",
    "                          short_title=True,\n",
    "                          as_pct_of_total=True, vmax=None)\n",
    "    \n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare IDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openstudio\n",
    "openstudio.openStudioLongVersion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_num_objects(test):\n",
    "    old_idf_path = os.path.join(OLD_OS_DIR, f'{test}.idf')\n",
    "    transitioned_idf_path = os.path.join(TRANSITION_DIR, f'{test}.idf')\n",
    "    new_idf_path = os.path.join(NEW_OS_DIR, f'{test}.idf')\n",
    "    \n",
    "    old_idf = openstudio.Workspace.load(openstudio.toPath(old_idf_path)).get()\n",
    "    transitioned_idf = openstudio.Workspace.load(openstudio.toPath(transitioned_idf_path)).get()\n",
    "    new_idf = openstudio.Workspace.load(openstudio.toPath(new_idf_path)).get()\n",
    "    \n",
    "    old_n = len(old_idf.objects(True))\n",
    "    trans_n = len(transitioned_idf.objects(True))\n",
    "    new_n = len(new_idf.objects(True))\n",
    "        \n",
    "    return test, old_n, trans_n, new_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [x for x in next(os.walk(OLD_OS_DIR))[1]]\n",
    "\n",
    "pool = multiprocessing.Pool(processes=N)\n",
    "\n",
    "desc = '<h3>Checking number of objects in IDF files</h3>'\n",
    "label = HTML(desc)\n",
    "display(label)\n",
    "all_results = []\n",
    "for result in tqdm(pool.imap_unordered(check_num_objects, tests), total=len(tests)):\n",
    "    all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_objects = pd.DataFrame(all_results, columns=['Test', OLD_OS_INFO['OS_VERSION'], TRANSITION_INFO['OS_VERSION'], NEW_OS_INFO['OS_VERSION']])\n",
    "df_num_objects.set_index('Test', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Any difference in number of objects\")\n",
    "df_num_objects[(df_num_objects.diff(axis=1).abs() > 0).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tests where the {OS_OLD_VERSION} is different from Transition:\")\n",
    "df_num_objects[df_num_objects[OS_OLD_VERSION] != df_num_objects['Transition']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tests where the difference between {OS_OLD_VERSION} and {OS_NEW_VERSION} is strictly greater than one\")\n",
    "df_num_objects[(df_num_objects[OS_NEW_VERSION] - df_num_objects[OS_OLD_VERSION]).abs() > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Analysis: there are no problems:**\n",
    " \n",
    " * From 3.4.0 to 3.5.0, we wrote transition rules for PTACs/PTHPs\n",
    "     * cf https://github.com/NREL/OpenStudio/blob/develop/developer/doc/ReleaseNotes/OpenStudio_Release_Notes_3_5_0_20221110.md\n",
    "     \n",
    "     > There are unusual `VersionTranslator` Rules for Packaged Systems (PTAC or PTHP) that use a `FanConstantVolume` and that do not have a `Supply Air Fan Operating Mode Schedule`. In 22.1.0 this would effectively, and mistakenly, function as a cycling fan, but this is now disallowed in E+ 22.2.0. In order to retain a similar functionality and energy usage, the `FanConstantVolume` will be replaced by a `FanSystemModel` with an Always Off Schedule (=cycling fan, similar to a `Fan:OnOff`), mapping inputs such as pressure rise and efficiency appropriately.\n",
    "     * The extra 1 object is the the Always Off Discrete schedule.\n",
    "\n",
    "* There are also CommentOnly difference when the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'unitary_vav_bypass_plenum.rb'\n",
    "\n",
    "old_idf_path = os.path.join(OLD_OS_DIR, f'{test}.idf')\n",
    "transitioned_idf_path = os.path.join(TRANSITION_DIR, f'{test}.idf')\n",
    "new_idf_path = os.path.join(NEW_OS_DIR, f'{test}.idf')\n",
    "\n",
    "old_idf = openstudio.Workspace.load(openstudio.toPath(old_idf_path)).get()\n",
    "transitioned_idf = openstudio.Workspace.load(openstudio.toPath(transitioned_idf_path)).get()\n",
    "new_idf = openstudio.Workspace.load(openstudio.toPath(new_idf_path)).get()\n",
    "\n",
    "len(old_idf.objects(True)), len(transitioned_idf.objects(True)), len(new_idf.objects(True))\n",
    "\n",
    "df_transitioned = pd.DataFrame([(obj.iddObject().name(), obj.nameString()) for obj in transitioned_idf.objects()],\n",
    "                               columns=['Type', 'Name'])\n",
    "\n",
    "df_new = pd.DataFrame([(obj.iddObject().name(), obj.nameString()) for obj in new_idf.objects()],\n",
    "                               columns=['Type', 'Name'])\n",
    "\n",
    "df_obj_type_diff = pd.concat(\n",
    "    [df_transitioned['Type'].value_counts(),\n",
    "     df_new['Type'].value_counts()],\n",
    "    axis=1, keys=['Transitioned', 'New'])\n",
    "\n",
    "df_obj_type_diff[df_obj_type_diff.diff(axis=1)['New'].abs() != 0]\n",
    "\n",
    "# transitioned_idf.save(transitioned_idf_path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strip input cell and warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local import\n",
    "import os\n",
    "import glob as gb\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "#filelist = gb.glob('*.html')\n",
    "# filelist = ['AutomateVersionTranslation.html']\n",
    "filelist = ['Analysis_from_3.7.0(23.2.0)_to_3.8.0-rc2(24.1.0).html']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Older jupyter install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_path in filelist:\n",
    "    print(s_path)\n",
    "    \n",
    "    save_path = \"{}_stripped.html\".format(os.path.splitext(s_path)[0])\n",
    "    print(\"Deleting input cells and warnings\")\n",
    "    with open(s_path,\"r+\") as htmlDoc:\n",
    "        soup = BeautifulSoup(htmlDoc, \"lxml\")\n",
    "        # Get input divs\n",
    "        tg = soup.find_all(attrs={\"class\": \"input\"})\n",
    "        # Add input stderr (warnings and errors)\n",
    "        tg += soup.find_all(attrs={\"class\": \"output_stderr\"})\n",
    "        # Replace with nothing\n",
    "        for i in range(len(tg)):\n",
    "            tg[i].replace_with(\"\")\n",
    "\n",
    "    # Prettify\n",
    "    html = soup.prettify(\"utf-8\")\n",
    "    #exportpath = os.path.splitext(htmlpath)[0]+'-noinput'+os.path.splitext(htmlpath)[1]\n",
    "\n",
    "    # Write\n",
    "    with open(save_path, \"wb\") as file:\n",
    "        file.write(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newer jupyter install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_path in filelist:\n",
    "    print(s_path)\n",
    "    \n",
    "    save_path = \"{}_stripped.html\".format(os.path.splitext(s_path)[0].replace('_ori', ''))\n",
    "    with open(s_path,\"r+\") as htmlDoc:\n",
    "        soup = BeautifulSoup(htmlDoc, \"lxml\")\n",
    "    \n",
    "    # Remove Raw Nb Convert\n",
    "    [x.decompose() for x in soup.select('body > p')]\n",
    "    # Get input divs\n",
    "    [x.decompose() for x in soup.select('.jp-CodeCell .jp-Cell-inputWrapper')]\n",
    "    \n",
    "    print(\"Deleting sections\")\n",
    "    in_del_block = True\n",
    "    for i, div in enumerate(soup.find_all('div', attrs={'class': 'jp-Cell'})):\n",
    "        if i == 0:\n",
    "            in_del_block = True\n",
    "            continue\n",
    "        if h1 := div.find('h1'):\n",
    "            if h1.attrs['id'].startswith('PART-1'):\n",
    "                print('PART-1 found')\n",
    "                in_del_block = True\n",
    "                continue\n",
    "            elif h1.attrs['id'].startswith('PART-2'):\n",
    "                print('PART-2 found')\n",
    "                in_del_block = False\n",
    "            elif h1.attrs['id'] == 'Compare-IDFs':\n",
    "                print('Compare-IDFS found')\n",
    "                in_del_block = True\n",
    "                \n",
    "        if h2 := div.find('h2'):\n",
    "            if h2.attrs['id'] == 'Analyzing-E+-runtime':\n",
    "                print('Analyzing-E+-runtime found')\n",
    "                in_del_block = True\n",
    "                continue\n",
    "            elif h2.attrs['id'].startswith('Concat-frames'):\n",
    "                print('Concat-frames found')\n",
    "                in_del_block = False\n",
    "\n",
    "        if in_del_block:\n",
    "            div.decompose()\n",
    "    \n",
    "    print(\"Deleting input cells and warnings\")\n",
    "    \n",
    "\n",
    "    \n",
    "    tg = soup.find_all(attrs={\"class\": \"jp-InputArea-editor\"}) # \"jp-InputArea\"})\n",
    "    # Add input stderr (warnings and errors)\n",
    "    tg += soup.find_all(attrs={\"data-mime-type\": \"application/vnd.jupyter.stderr\"})\n",
    "    # tg += soup.find_all(attrs={\"class\": \"jp-OutputArea-executeResult\"})\n",
    "    # Replace with nothing\n",
    "    for i in range(len(tg)):\n",
    "        tg[i].replace_with(\"\")\n",
    "\n",
    "    # Prettify\n",
    "    html = soup.prettify(\"utf-8\")\n",
    "    #exportpath = os.path.splitext(htmlpath)[0]+'-noinput'+os.path.splitext(htmlpath)[1]\n",
    "\n",
    "    # Write\n",
    "    with open(save_path, \"wb\") as file:\n",
    "        file.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sql_version_and_sitekbtu_for_sqlfile(sql_path):\n",
    "    \"\"\"\n",
    "    This function grabs the EnergyPlusVersion and the total site energy\n",
    "    from the SQL file.\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    * output_directory (str): the path were the SQL should be.\n",
    "        eg: `./8.8.0/absorption_chillers.osm_8.8.0/`\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    * pd.Series that has the version with SHA and site kbtu\n",
    "        (or None if it didn't run), which name is the test_name\n",
    "        (gotten from the name of the output_directory)\n",
    "    \"\"\"\n",
    "\n",
    "    abs_sql_path = Path(sql_path).absolute()\n",
    "    sql_uri = '{}?mode=ro'.format(abs_sql_path.as_uri())\n",
    "    with sqlite3.connect(sql_uri, uri=True) as con:\n",
    "            cursor = con.cursor()\n",
    "            r = cursor.execute(SQL_QUERY_SIM_INFO).fetchone()\n",
    "            if r:\n",
    "                simulation_info = r[0]\n",
    "                m = VERSION_REGEX.search(simulation_info)\n",
    "                if m:\n",
    "                    gpdict = m.groupdict()\n",
    "                    version_with_sha = \"{}.{}.{}-{}\".format(gpdict['Major'],\n",
    "                                                                 gpdict['Minor'],\n",
    "                                                                 gpdict['Patch'],\n",
    "                                                                 gpdict['SHA'])\n",
    "            else:\n",
    "                msg = (\"Cannot find the EnergyPlusVersion in the SQL file. \"\n",
    "                       \"For:\\n{}\".format(sql_path))\n",
    "                #raise ValueError(msg)\n",
    "                print(msg)\n",
    "\n",
    "            # Get Site kBTU\n",
    "            r = cursor.execute(SQL_QUERY_TOTAL_SITE_KBTU).fetchone()\n",
    "            if r:\n",
    "                site_gj = float(r[0])\n",
    "                site_kbtu = site_gj * GJ_TO_KBTU\n",
    "                msg = (\"Cannot find the Total Site Energy in the SQL file. \"\n",
    "                       \"For:\\n{}\".format(sql_path))\n",
    "    return pd.Series([version_with_sha, site_kbtu],\n",
    "                     index=['E+', 'SiteKBTU'],\n",
    "                     name = os.path.split(sql_path)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_sql_version_and_sitekbtu_for_sqlfile('../tmp/os321.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_sql_version_and_sitekbtu_for_sqlfile('../tmp/transition.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_sql_version_and_sitekbtu_for_sqlfile('../tmp/os330.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run period year changed from 2009 (Thursday) to 2006 (Sunday)?\n",
    "Sizing:System: 100% Outdoor Air in Cooling/Heating was changed from 'Yes' to 'No'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_sql_version_and_sitekbtu_for_sqlfile('../tmp/os330_sizingsystem/eplusout.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_sql_version_and_sitekbtu_for_sqlfile('../tmp/os330_runperiod/eplusout.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_sql_version_and_sitekbtu_for_sqlfile('../tmp/os330_sizingsystem_runperiod/eplusout.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{4109270.60329/4122900.21348 - 1:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "847.867px",
    "left": "43px",
    "right": "1643px",
    "top": "111.483px",
    "width": "416px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
